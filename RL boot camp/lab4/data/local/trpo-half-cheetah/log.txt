[2018-07-02 16:20:27.434398 UTC] Starting env pool
[2018-07-02 16:20:27.492088 UTC] Starting iteration 0
[2018-07-02 16:20:27.493115 UTC] Start collecting samples
[2018-07-02 16:20:33.558504 UTC] Computing input variables for policy optimization
[2018-07-02 16:20:33.732230 UTC] Performing policy update
[2018-07-02 16:20:33.733864 UTC] Computing gradient in Euclidean space
[2018-07-02 16:20:33.820978 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:20:34.516058 UTC] Performing line search
[2018-07-02 16:20:34.625478 UTC] Updating baseline
[2018-07-02 16:20:35.453869 UTC] Computing logging information
-------------------------------------
| Iteration            | 0          |
| ExpectedImprovement  | 0.035451   |
| ActualImprovement    | 0.034284   |
| ImprovementRatio     | 0.9671     |
| MeanKL               | 0.0065813  |
| Entropy              | 8.5136     |
| Perplexity           | 4982.2     |
| AveragePolicyStd     | 1          |
| AveragePolicyStd[0]  | 1          |
| AveragePolicyStd[1]  | 1          |
| AveragePolicyStd[2]  | 1          |
| AveragePolicyStd[3]  | 1          |
| AveragePolicyStd[4]  | 1          |
| AveragePolicyStd[5]  | 1          |
| AverageReturn        | -11.697    |
| MinReturn            | -56.6      |
| MaxReturn            | -2.1933    |
| StdReturn            | 6.8409     |
| AverageEpisodeLength | 18         |
| MinEpisodeLength     | 12         |
| MaxEpisodeLength     | 44         |
| StdEpisodeLength     | 5.4314     |
| TotalNEpisodes       | 267        |
| TotalNSamples        | 4867       |
| ExplainedVariance    | -0.0098364 |
-------------------------------------
[2018-07-02 16:20:36.326590 UTC] Saving snapshot
[2018-07-02 16:20:36.337610 UTC] Starting iteration 1
[2018-07-02 16:20:36.338546 UTC] Start collecting samples
[2018-07-02 16:20:43.483888 UTC] Computing input variables for policy optimization
[2018-07-02 16:20:44.084887 UTC] Performing policy update
[2018-07-02 16:20:44.088046 UTC] Computing gradient in Euclidean space
[2018-07-02 16:20:44.229108 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:20:46.135693 UTC] Performing line search
[2018-07-02 16:20:46.543444 UTC] Updating baseline
[2018-07-02 16:20:49.679583 UTC] Computing logging information
------------------------------------
| Iteration            | 1         |
| ExpectedImprovement  | 0.035914  |
| ActualImprovement    | 0.034777  |
| ImprovementRatio     | 0.96833   |
| MeanKL               | 0.0065257 |
| Entropy              | 8.4827    |
| Perplexity           | 4830.5    |
| AveragePolicyStd     | 0.99487   |
| AveragePolicyStd[0]  | 1.0003    |
| AveragePolicyStd[1]  | 0.99555   |
| AveragePolicyStd[2]  | 0.99276   |
| AveragePolicyStd[3]  | 0.99548   |
| AveragePolicyStd[4]  | 0.99536   |
| AveragePolicyStd[5]  | 0.98975   |
| AverageReturn        | -10.436   |
| MinReturn            | -29.434   |
| MaxReturn            | 2.7889    |
| StdReturn            | 4.8499    |
| AverageEpisodeLength | 17.5      |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 31        |
| StdEpisodeLength     | 3.2481    |
| TotalNEpisodes       | 560       |
| TotalNSamples        | 9896      |
| ExplainedVariance    | 0.23823   |
------------------------------------
[2018-07-02 16:20:51.106139 UTC] Saving snapshot
[2018-07-02 16:20:51.106986 UTC] Starting iteration 2
[2018-07-02 16:20:51.107737 UTC] Start collecting samples
[2018-07-02 16:21:01.626502 UTC] Computing input variables for policy optimization
[2018-07-02 16:21:02.202947 UTC] Performing policy update
[2018-07-02 16:21:02.206094 UTC] Computing gradient in Euclidean space
[2018-07-02 16:21:02.328959 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:21:03.870358 UTC] Performing line search
[2018-07-02 16:21:04.054348 UTC] Updating baseline
[2018-07-02 16:21:06.148124 UTC] Computing logging information
------------------------------------
| Iteration            | 2         |
| ExpectedImprovement  | 0.037985  |
| ActualImprovement    | 0.037281  |
| ImprovementRatio     | 0.98145   |
| MeanKL               | 0.0065807 |
| Entropy              | 8.409     |
| Perplexity           | 4487.1    |
| AveragePolicyStd     | 0.98273   |
| AveragePolicyStd[0]  | 0.99491   |
| AveragePolicyStd[1]  | 0.98721   |
| AveragePolicyStd[2]  | 0.98112   |
| AveragePolicyStd[3]  | 0.97703   |
| AveragePolicyStd[4]  | 0.98313   |
| AveragePolicyStd[5]  | 0.97298   |
| AverageReturn        | -9.4565   |
| MinReturn            | -30.555   |
| MaxReturn            | 2.142     |
| StdReturn            | 5.4989    |
| AverageEpisodeLength | 16.7      |
| MinEpisodeLength     | 12        |
| MaxEpisodeLength     | 29        |
| StdEpisodeLength     | 2.3601    |
| TotalNEpisodes       | 851       |
| TotalNSamples        | 14899     |
| ExplainedVariance    | 0.31636   |
------------------------------------
[2018-07-02 16:21:07.236864 UTC] Saving snapshot
[2018-07-02 16:21:07.237447 UTC] Starting iteration 3
[2018-07-02 16:21:07.237890 UTC] Start collecting samples
[2018-07-02 16:21:17.303841 UTC] Computing input variables for policy optimization
[2018-07-02 16:21:17.837145 UTC] Performing policy update
[2018-07-02 16:21:17.858567 UTC] Computing gradient in Euclidean space
[2018-07-02 16:21:17.948482 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:21:19.047045 UTC] Performing line search
[2018-07-02 16:21:19.174188 UTC] Updating baseline
[2018-07-02 16:21:20.633774 UTC] Computing logging information
------------------------------------
| Iteration            | 3         |
| ExpectedImprovement  | 0.036098  |
| ActualImprovement    | 0.035107  |
| ImprovementRatio     | 0.97254   |
| MeanKL               | 0.0065295 |
| Entropy              | 8.3307    |
| Perplexity           | 4149.1    |
| AveragePolicyStd     | 0.97002   |
| AveragePolicyStd[0]  | 0.98838   |
| AveragePolicyStd[1]  | 0.97306   |
| AveragePolicyStd[2]  | 0.96732   |
| AveragePolicyStd[3]  | 0.96153   |
| AveragePolicyStd[4]  | 0.97386   |
| AveragePolicyStd[5]  | 0.95598   |
| AverageReturn        | -8.3507   |
| MinReturn            | -30.348   |
| MaxReturn            | 0.38646   |
| StdReturn            | 4.5791    |
| AverageEpisodeLength | 17.64     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 36        |
| StdEpisodeLength     | 3.1544    |
| TotalNEpisodes       | 1137      |
| TotalNSamples        | 19920     |
| ExplainedVariance    | 0.28656   |
------------------------------------
[2018-07-02 16:21:21.729305 UTC] Saving snapshot
[2018-07-02 16:21:21.730116 UTC] Starting iteration 4
[2018-07-02 16:21:21.730862 UTC] Start collecting samples
[2018-07-02 16:21:31.831468 UTC] Computing input variables for policy optimization
[2018-07-02 16:21:32.486058 UTC] Performing policy update
[2018-07-02 16:21:32.488043 UTC] Computing gradient in Euclidean space
[2018-07-02 16:21:32.608538 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:21:33.877115 UTC] Performing line search
[2018-07-02 16:21:34.023866 UTC] Updating baseline
[2018-07-02 16:21:35.711922 UTC] Computing logging information
------------------------------------
| Iteration            | 4         |
| ExpectedImprovement  | 0.038239  |
| ActualImprovement    | 0.037554  |
| ImprovementRatio     | 0.98208   |
| MeanKL               | 0.0066055 |
| Entropy              | 8.2601    |
| Perplexity           | 3866.6    |
| AveragePolicyStd     | 0.95874   |
| AveragePolicyStd[0]  | 0.98371   |
| AveragePolicyStd[1]  | 0.96383   |
| AveragePolicyStd[2]  | 0.95345   |
| AveragePolicyStd[3]  | 0.94891   |
| AveragePolicyStd[4]  | 0.96462   |
| AveragePolicyStd[5]  | 0.93791   |
| AverageReturn        | -7.8841   |
| MinReturn            | -22.807   |
| MaxReturn            | 2.2319    |
| StdReturn            | 4.3378    |
| AverageEpisodeLength | 17.81     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 48        |
| StdEpisodeLength     | 4.0563    |
| TotalNEpisodes       | 1418      |
| TotalNSamples        | 24861     |
| ExplainedVariance    | 0.31227   |
------------------------------------
[2018-07-02 16:21:36.880799 UTC] Saving snapshot
[2018-07-02 16:21:36.881979 UTC] Starting iteration 5
[2018-07-02 16:21:36.882765 UTC] Start collecting samples
[2018-07-02 16:21:46.143352 UTC] Computing input variables for policy optimization
[2018-07-02 16:21:46.355167 UTC] Performing policy update
[2018-07-02 16:21:46.356280 UTC] Computing gradient in Euclidean space
[2018-07-02 16:21:46.417438 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:21:47.098529 UTC] Performing line search
[2018-07-02 16:21:47.207899 UTC] Updating baseline
[2018-07-02 16:21:48.109838 UTC] Computing logging information
------------------------------------
| Iteration            | 5         |
| ExpectedImprovement  | 0.034345  |
| ActualImprovement    | 0.033228  |
| ImprovementRatio     | 0.96747   |
| MeanKL               | 0.0065557 |
| Entropy              | 8.1701    |
| Perplexity           | 3533.6    |
| AveragePolicyStd     | 0.9445    |
| AveragePolicyStd[0]  | 0.97132   |
| AveragePolicyStd[1]  | 0.95746   |
| AveragePolicyStd[2]  | 0.93924   |
| AveragePolicyStd[3]  | 0.93068   |
| AveragePolicyStd[4]  | 0.94817   |
| AveragePolicyStd[5]  | 0.92013   |
| AverageReturn        | -7.0973   |
| MinReturn            | -16.216   |
| MaxReturn            | 0.6893    |
| StdReturn            | 3.5993    |
| AverageEpisodeLength | 17.58     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 30        |
| StdEpisodeLength     | 2.442     |
| TotalNEpisodes       | 1702      |
| TotalNSamples        | 29937     |
| ExplainedVariance    | 0.29297   |
------------------------------------
[2018-07-02 16:21:49.104047 UTC] Saving snapshot
[2018-07-02 16:21:49.104628 UTC] Starting iteration 6
[2018-07-02 16:21:49.105209 UTC] Start collecting samples
[2018-07-02 16:21:54.676627 UTC] Computing input variables for policy optimization
[2018-07-02 16:21:54.855080 UTC] Performing policy update
[2018-07-02 16:21:54.856404 UTC] Computing gradient in Euclidean space
[2018-07-02 16:21:54.919720 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:21:55.607654 UTC] Performing line search
[2018-07-02 16:21:55.702585 UTC] Updating baseline
[2018-07-02 16:21:56.660093 UTC] Computing logging information
------------------------------------
| Iteration            | 6         |
| ExpectedImprovement  | 0.036676  |
| ActualImprovement    | 0.035895  |
| ImprovementRatio     | 0.97871   |
| MeanKL               | 0.0066728 |
| Entropy              | 8.0642    |
| Perplexity           | 3178.5    |
| AveragePolicyStd     | 0.92795   |
| AveragePolicyStd[0]  | 0.94694   |
| AveragePolicyStd[1]  | 0.94326   |
| AveragePolicyStd[2]  | 0.92198   |
| AveragePolicyStd[3]  | 0.91195   |
| AveragePolicyStd[4]  | 0.93609   |
| AveragePolicyStd[5]  | 0.90747   |
| AverageReturn        | -6.0199   |
| MinReturn            | -16.262   |
| MaxReturn            | 3.7236    |
| StdReturn            | 3.867     |
| AverageEpisodeLength | 17.61     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 37        |
| StdEpisodeLength     | 3.2121    |
| TotalNEpisodes       | 1988      |
| TotalNSamples        | 34944     |
| ExplainedVariance    | 0.25232   |
------------------------------------
[2018-07-02 16:21:57.452383 UTC] Saving snapshot
[2018-07-02 16:21:57.452999 UTC] Starting iteration 7
[2018-07-02 16:21:57.453824 UTC] Start collecting samples
[2018-07-02 16:22:02.925505 UTC] Computing input variables for policy optimization
[2018-07-02 16:22:03.103123 UTC] Performing policy update
[2018-07-02 16:22:03.104275 UTC] Computing gradient in Euclidean space
[2018-07-02 16:22:03.168676 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:22:03.837956 UTC] Performing line search
[2018-07-02 16:22:03.934291 UTC] Updating baseline
[2018-07-02 16:22:04.901021 UTC] Computing logging information
------------------------------------
| Iteration            | 7         |
| ExpectedImprovement  | 0.03207   |
| ActualImprovement    | 0.031895  |
| ImprovementRatio     | 0.99454   |
| MeanKL               | 0.0066529 |
| Entropy              | 7.9534    |
| Perplexity           | 2845.2    |
| AveragePolicyStd     | 0.911     |
| AveragePolicyStd[0]  | 0.92624   |
| AveragePolicyStd[1]  | 0.93316   |
| AveragePolicyStd[2]  | 0.90161   |
| AveragePolicyStd[3]  | 0.89241   |
| AveragePolicyStd[4]  | 0.92116   |
| AveragePolicyStd[5]  | 0.89145   |
| AverageReturn        | -4.7967   |
| MinReturn            | -12.987   |
| MaxReturn            | 1.6311    |
| StdReturn            | 3.2826    |
| AverageEpisodeLength | 17.93     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 37        |
| StdEpisodeLength     | 3.0307    |
| TotalNEpisodes       | 2268      |
| TotalNSamples        | 39929     |
| ExplainedVariance    | 0.17968   |
------------------------------------
[2018-07-02 16:22:05.966637 UTC] Saving snapshot
[2018-07-02 16:22:05.967403 UTC] Starting iteration 8
[2018-07-02 16:22:05.968049 UTC] Start collecting samples
[2018-07-02 16:22:12.272970 UTC] Computing input variables for policy optimization
[2018-07-02 16:22:12.452307 UTC] Performing policy update
[2018-07-02 16:22:12.453592 UTC] Computing gradient in Euclidean space
[2018-07-02 16:22:12.518347 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:22:13.210533 UTC] Performing line search
[2018-07-02 16:22:13.309880 UTC] Updating baseline
[2018-07-02 16:22:14.181616 UTC] Computing logging information
------------------------------------
| Iteration            | 8         |
| ExpectedImprovement  | 0.03116   |
| ActualImprovement    | 0.030839  |
| ImprovementRatio     | 0.98969   |
| MeanKL               | 0.0067481 |
| Entropy              | 7.8443    |
| Perplexity           | 2551.2    |
| AveragePolicyStd     | 0.89466   |
| AveragePolicyStd[0]  | 0.90764   |
| AveragePolicyStd[1]  | 0.91422   |
| AveragePolicyStd[2]  | 0.87955   |
| AveragePolicyStd[3]  | 0.88133   |
| AveragePolicyStd[4]  | 0.91788   |
| AveragePolicyStd[5]  | 0.86732   |
| AverageReturn        | -3.9501   |
| MinReturn            | -11.907   |
| MaxReturn            | 2.3302    |
| StdReturn            | 2.9559    |
| AverageEpisodeLength | 17.93     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 32        |
| StdEpisodeLength     | 2.3801    |
| TotalNEpisodes       | 2547      |
| TotalNSamples        | 44964     |
| ExplainedVariance    | 0.21191   |
------------------------------------
[2018-07-02 16:22:15.293489 UTC] Saving snapshot
[2018-07-02 16:22:15.294509 UTC] Starting iteration 9
[2018-07-02 16:22:15.296020 UTC] Start collecting samples
[2018-07-02 16:22:21.013172 UTC] Computing input variables for policy optimization
[2018-07-02 16:22:21.198833 UTC] Performing policy update
[2018-07-02 16:22:21.200134 UTC] Computing gradient in Euclidean space
[2018-07-02 16:22:21.268405 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:22:21.952116 UTC] Performing line search
[2018-07-02 16:22:22.047003 UTC] Updating baseline
[2018-07-02 16:22:23.068923 UTC] Computing logging information
-----------------------------------
| Iteration            | 9        |
| ExpectedImprovement  | 0.03133  |
| ActualImprovement    | 0.030609 |
| ImprovementRatio     | 0.97697  |
| MeanKL               | 0.006694 |
| Entropy              | 7.7378   |
| Perplexity           | 2293.4   |
| AveragePolicyStd     | 0.87891  |
| AveragePolicyStd[0]  | 0.89519  |
| AveragePolicyStd[1]  | 0.89548  |
| AveragePolicyStd[2]  | 0.86293  |
| AveragePolicyStd[3]  | 0.86456  |
| AveragePolicyStd[4]  | 0.90135  |
| AveragePolicyStd[5]  | 0.85394  |
| AverageReturn        | -3.2785  |
| MinReturn            | -13.829  |
| MaxReturn            | 4.7284   |
| StdReturn            | 3.01     |
| AverageEpisodeLength | 18.09    |
| MinEpisodeLength     | 13       |
| MaxEpisodeLength     | 33       |
| StdEpisodeLength     | 2.9702   |
| TotalNEpisodes       | 2823     |
| TotalNSamples        | 49925    |
| ExplainedVariance    | 0.23007  |
-----------------------------------
[2018-07-02 16:22:23.863723 UTC] Saving snapshot
[2018-07-02 16:22:23.864352 UTC] Starting iteration 10
[2018-07-02 16:22:23.864955 UTC] Start collecting samples
[2018-07-02 16:22:30.657273 UTC] Computing input variables for policy optimization
[2018-07-02 16:22:30.836465 UTC] Performing policy update
[2018-07-02 16:22:30.837536 UTC] Computing gradient in Euclidean space
[2018-07-02 16:22:30.901136 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:22:31.579590 UTC] Performing line search
[2018-07-02 16:22:31.674296 UTC] Updating baseline
[2018-07-02 16:22:32.748306 UTC] Computing logging information
------------------------------------
| Iteration            | 10        |
| ExpectedImprovement  | 0.030309  |
| ActualImprovement    | 0.0297    |
| ImprovementRatio     | 0.97992   |
| MeanKL               | 0.0066584 |
| Entropy              | 7.6104    |
| Perplexity           | 2019.1    |
| AveragePolicyStd     | 0.86054   |
| AveragePolicyStd[0]  | 0.88118   |
| AveragePolicyStd[1]  | 0.88424   |
| AveragePolicyStd[2]  | 0.84072   |
| AveragePolicyStd[3]  | 0.83895   |
| AveragePolicyStd[4]  | 0.88403   |
| AveragePolicyStd[5]  | 0.83413   |
| AverageReturn        | -2.7906   |
| MinReturn            | -14.013   |
| MaxReturn            | 7.1412    |
| StdReturn            | 3.5015    |
| AverageEpisodeLength | 18.16     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 35        |
| StdEpisodeLength     | 3.0748    |
| TotalNEpisodes       | 3100      |
| TotalNSamples        | 54916     |
| ExplainedVariance    | 0.1732    |
------------------------------------
[2018-07-02 16:22:33.580561 UTC] Saving snapshot
[2018-07-02 16:22:33.591366 UTC] Starting iteration 11
[2018-07-02 16:22:33.592081 UTC] Start collecting samples
[2018-07-02 16:22:40.227455 UTC] Computing input variables for policy optimization
[2018-07-02 16:22:40.410242 UTC] Performing policy update
[2018-07-02 16:22:40.411370 UTC] Computing gradient in Euclidean space
[2018-07-02 16:22:40.485177 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:22:41.170109 UTC] Performing line search
[2018-07-02 16:22:41.270704 UTC] Updating baseline
[2018-07-02 16:22:42.240669 UTC] Computing logging information
------------------------------------
| Iteration            | 11        |
| ExpectedImprovement  | 0.030858  |
| ActualImprovement    | 0.030401  |
| ImprovementRatio     | 0.9852    |
| MeanKL               | 0.0066809 |
| Entropy              | 7.4997    |
| Perplexity           | 1807.6    |
| AveragePolicyStd     | 0.84498   |
| AveragePolicyStd[0]  | 0.86542   |
| AveragePolicyStd[1]  | 0.8781    |
| AveragePolicyStd[2]  | 0.82986   |
| AveragePolicyStd[3]  | 0.8124    |
| AveragePolicyStd[4]  | 0.87145   |
| AveragePolicyStd[5]  | 0.81262   |
| AverageReturn        | -2.3782   |
| MinReturn            | -11.362   |
| MaxReturn            | 4.8878    |
| StdReturn            | 3.1453    |
| AverageEpisodeLength | 17.59     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 24        |
| StdEpisodeLength     | 1.7498    |
| TotalNEpisodes       | 3379      |
| TotalNSamples        | 59976     |
| ExplainedVariance    | 0.15812   |
------------------------------------
[2018-07-02 16:22:43.269287 UTC] Saving snapshot
[2018-07-02 16:22:43.269957 UTC] Starting iteration 12
[2018-07-02 16:22:43.270821 UTC] Start collecting samples
[2018-07-02 16:22:49.597504 UTC] Computing input variables for policy optimization
[2018-07-02 16:22:49.766644 UTC] Performing policy update
[2018-07-02 16:22:49.767721 UTC] Computing gradient in Euclidean space
[2018-07-02 16:22:49.831446 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:22:50.484427 UTC] Performing line search
[2018-07-02 16:22:50.580205 UTC] Updating baseline
[2018-07-02 16:22:51.483668 UTC] Computing logging information
------------------------------------
| Iteration            | 12        |
| ExpectedImprovement  | 0.030766  |
| ActualImprovement    | 0.030595  |
| ImprovementRatio     | 0.99446   |
| MeanKL               | 0.0066613 |
| Entropy              | 7.382     |
| Perplexity           | 1606.8    |
| AveragePolicyStd     | 0.82848   |
| AveragePolicyStd[0]  | 0.85552   |
| AveragePolicyStd[1]  | 0.86022   |
| AveragePolicyStd[2]  | 0.81197   |
| AveragePolicyStd[3]  | 0.80235   |
| AveragePolicyStd[4]  | 0.84142   |
| AveragePolicyStd[5]  | 0.79943   |
| AverageReturn        | -0.97257  |
| MinReturn            | -15.151   |
| MaxReturn            | 15.741    |
| StdReturn            | 3.7059    |
| AverageEpisodeLength | 18.73     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 41        |
| StdEpisodeLength     | 4.3792    |
| TotalNEpisodes       | 3649      |
| TotalNSamples        | 64953     |
| ExplainedVariance    | 0.16462   |
------------------------------------
[2018-07-02 16:22:52.364414 UTC] Saving snapshot
[2018-07-02 16:22:52.365079 UTC] Starting iteration 13
[2018-07-02 16:22:52.365816 UTC] Start collecting samples
[2018-07-02 16:22:58.031792 UTC] Computing input variables for policy optimization
[2018-07-02 16:22:58.201627 UTC] Performing policy update
[2018-07-02 16:22:58.202882 UTC] Computing gradient in Euclidean space
[2018-07-02 16:22:58.262420 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:22:58.920302 UTC] Performing line search
[2018-07-02 16:22:59.017182 UTC] Updating baseline
[2018-07-02 16:22:59.913836 UTC] Computing logging information
------------------------------------
| Iteration            | 13        |
| ExpectedImprovement  | 0.028741  |
| ActualImprovement    | 0.028286  |
| ImprovementRatio     | 0.98419   |
| MeanKL               | 0.0066627 |
| Entropy              | 7.2524    |
| Perplexity           | 1411.5    |
| AveragePolicyStd     | 0.81075   |
| AveragePolicyStd[0]  | 0.83266   |
| AveragePolicyStd[1]  | 0.83972   |
| AveragePolicyStd[2]  | 0.80209   |
| AveragePolicyStd[3]  | 0.78359   |
| AveragePolicyStd[4]  | 0.82476   |
| AveragePolicyStd[5]  | 0.78168   |
| AverageReturn        | -0.3493   |
| MinReturn            | -7.7865   |
| MaxReturn            | 6.2986    |
| StdReturn            | 2.7863    |
| AverageEpisodeLength | 18.78     |
| MinEpisodeLength     | 15        |
| MaxEpisodeLength     | 34        |
| StdEpisodeLength     | 3.2267    |
| TotalNEpisodes       | 3922      |
| TotalNSamples        | 69959     |
| ExplainedVariance    | 0.18513   |
------------------------------------
[2018-07-02 16:23:00.984942 UTC] Saving snapshot
[2018-07-02 16:23:00.985715 UTC] Starting iteration 14
[2018-07-02 16:23:00.986495 UTC] Start collecting samples
[2018-07-02 16:23:07.098114 UTC] Computing input variables for policy optimization
[2018-07-02 16:23:07.280196 UTC] Performing policy update
[2018-07-02 16:23:07.281575 UTC] Computing gradient in Euclidean space
[2018-07-02 16:23:07.343110 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:23:08.024703 UTC] Performing line search
[2018-07-02 16:23:08.126506 UTC] Updating baseline
[2018-07-02 16:23:09.060434 UTC] Computing logging information
------------------------------------
| Iteration            | 14        |
| ExpectedImprovement  | 0.028758  |
| ActualImprovement    | 0.028144  |
| ImprovementRatio     | 0.97866   |
| MeanKL               | 0.0064914 |
| Entropy              | 7.1226    |
| Perplexity           | 1239.6    |
| AveragePolicyStd     | 0.79335   |
| AveragePolicyStd[0]  | 0.81738   |
| AveragePolicyStd[1]  | 0.81862   |
| AveragePolicyStd[2]  | 0.78701   |
| AveragePolicyStd[3]  | 0.76634   |
| AveragePolicyStd[4]  | 0.801     |
| AveragePolicyStd[5]  | 0.76972   |
| AverageReturn        | 0.25571   |
| MinReturn            | -6.3282   |
| MaxReturn            | 12.22     |
| StdReturn            | 2.6908    |
| AverageEpisodeLength | 18.29     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 31        |
| StdEpisodeLength     | 2.6657    |
| TotalNEpisodes       | 4196      |
| TotalNSamples        | 74992     |
| ExplainedVariance    | 0.238     |
------------------------------------
[2018-07-02 16:23:10.040760 UTC] Saving snapshot
[2018-07-02 16:23:10.041607 UTC] Starting iteration 15
[2018-07-02 16:23:10.042279 UTC] Start collecting samples
[2018-07-02 16:23:15.462747 UTC] Computing input variables for policy optimization
[2018-07-02 16:23:15.639810 UTC] Performing policy update
[2018-07-02 16:23:15.640997 UTC] Computing gradient in Euclidean space
[2018-07-02 16:23:15.701946 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:23:16.357560 UTC] Performing line search
[2018-07-02 16:23:16.452015 UTC] Updating baseline
[2018-07-02 16:23:17.324696 UTC] Computing logging information
------------------------------------
| Iteration            | 15        |
| ExpectedImprovement  | 0.027745  |
| ActualImprovement    | 0.027583  |
| ImprovementRatio     | 0.99415   |
| MeanKL               | 0.0067477 |
| Entropy              | 6.9969    |
| Perplexity           | 1093.3    |
| AveragePolicyStd     | 0.77697   |
| AveragePolicyStd[0]  | 0.80392   |
| AveragePolicyStd[1]  | 0.80563   |
| AveragePolicyStd[2]  | 0.77355   |
| AveragePolicyStd[3]  | 0.7528    |
| AveragePolicyStd[4]  | 0.78      |
| AveragePolicyStd[5]  | 0.74594   |
| AverageReturn        | 0.92049   |
| MinReturn            | -5.1776   |
| MaxReturn            | 10.747    |
| StdReturn            | 2.9901    |
| AverageEpisodeLength | 18.67     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 40        |
| StdEpisodeLength     | 3.4265    |
| TotalNEpisodes       | 4468      |
| TotalNSamples        | 79989     |
| ExplainedVariance    | 0.25527   |
------------------------------------
[2018-07-02 16:23:18.328022 UTC] Saving snapshot
[2018-07-02 16:23:18.328826 UTC] Starting iteration 16
[2018-07-02 16:23:18.329579 UTC] Start collecting samples
[2018-07-02 16:23:23.795360 UTC] Computing input variables for policy optimization
[2018-07-02 16:23:23.993359 UTC] Performing policy update
[2018-07-02 16:23:23.994754 UTC] Computing gradient in Euclidean space
[2018-07-02 16:23:24.061468 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:23:24.786756 UTC] Performing line search
[2018-07-02 16:23:24.890146 UTC] Updating baseline
[2018-07-02 16:23:25.936318 UTC] Computing logging information
------------------------------------
| Iteration            | 16        |
| ExpectedImprovement  | 0.027948  |
| ActualImprovement    | 0.027505  |
| ImprovementRatio     | 0.98415   |
| MeanKL               | 0.0066281 |
| Entropy              | 6.8728    |
| Perplexity           | 965.61    |
| AveragePolicyStd     | 0.76112   |
| AveragePolicyStd[0]  | 0.78647   |
| AveragePolicyStd[1]  | 0.79521   |
| AveragePolicyStd[2]  | 0.75915   |
| AveragePolicyStd[3]  | 0.7326    |
| AveragePolicyStd[4]  | 0.7629    |
| AveragePolicyStd[5]  | 0.73038   |
| AverageReturn        | 1.2181    |
| MinReturn            | -7.72     |
| MaxReturn            | 10.697    |
| StdReturn            | 3.227     |
| AverageEpisodeLength | 18.79     |
| MinEpisodeLength     | 16        |
| MaxEpisodeLength     | 32        |
| StdEpisodeLength     | 3.0636    |
| TotalNEpisodes       | 4742      |
| TotalNSamples        | 85011     |
| ExplainedVariance    | 0.21731   |
------------------------------------
[2018-07-02 16:23:26.904429 UTC] Saving snapshot
[2018-07-02 16:23:26.905342 UTC] Starting iteration 17
[2018-07-02 16:23:26.906357 UTC] Start collecting samples
[2018-07-02 16:23:33.053156 UTC] Computing input variables for policy optimization
[2018-07-02 16:23:33.248147 UTC] Performing policy update
[2018-07-02 16:23:33.249271 UTC] Computing gradient in Euclidean space
[2018-07-02 16:23:33.316201 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:23:34.097013 UTC] Performing line search
[2018-07-02 16:23:34.198714 UTC] Updating baseline
[2018-07-02 16:23:35.124668 UTC] Computing logging information
------------------------------------
| Iteration            | 17        |
| ExpectedImprovement  | 0.025641  |
| ActualImprovement    | 0.025292  |
| ImprovementRatio     | 0.98636   |
| MeanKL               | 0.0067379 |
| Entropy              | 6.741     |
| Perplexity           | 846.44    |
| AveragePolicyStd     | 0.7447    |
| AveragePolicyStd[0]  | 0.77476   |
| AveragePolicyStd[1]  | 0.78242   |
| AveragePolicyStd[2]  | 0.747     |
| AveragePolicyStd[3]  | 0.71041   |
| AveragePolicyStd[4]  | 0.73804   |
| AveragePolicyStd[5]  | 0.71557   |
| AverageReturn        | 1.7714    |
| MinReturn            | -4.5636   |
| MaxReturn            | 6.6042    |
| StdReturn            | 2.4989    |
| AverageEpisodeLength | 18.25     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 34        |
| StdEpisodeLength     | 2.2776    |
| TotalNEpisodes       | 5012      |
| TotalNSamples        | 90034     |
| ExplainedVariance    | 0.2906    |
------------------------------------
[2018-07-02 16:23:36.239495 UTC] Saving snapshot
[2018-07-02 16:23:36.240521 UTC] Starting iteration 18
[2018-07-02 16:23:36.241326 UTC] Start collecting samples
[2018-07-02 16:23:42.336639 UTC] Computing input variables for policy optimization
[2018-07-02 16:23:42.510637 UTC] Performing policy update
[2018-07-02 16:23:42.511825 UTC] Computing gradient in Euclidean space
[2018-07-02 16:23:42.574492 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:23:43.257593 UTC] Performing line search
[2018-07-02 16:23:43.355219 UTC] Updating baseline
[2018-07-02 16:23:44.268351 UTC] Computing logging information
------------------------------------
| Iteration            | 18        |
| ExpectedImprovement  | 0.027384  |
| ActualImprovement    | 0.026741  |
| ImprovementRatio     | 0.97651   |
| MeanKL               | 0.0065791 |
| Entropy              | 6.616     |
| Perplexity           | 746.92    |
| AveragePolicyStd     | 0.72936   |
| AveragePolicyStd[0]  | 0.75413   |
| AveragePolicyStd[1]  | 0.77014   |
| AveragePolicyStd[2]  | 0.73615   |
| AveragePolicyStd[3]  | 0.6937    |
| AveragePolicyStd[4]  | 0.7197    |
| AveragePolicyStd[5]  | 0.70234   |
| AverageReturn        | 2.0124    |
| MinReturn            | -8.4913   |
| MaxReturn            | 12.406    |
| StdReturn            | 3.0107    |
| AverageEpisodeLength | 18.82     |
| MinEpisodeLength     | 15        |
| MaxEpisodeLength     | 31        |
| StdEpisodeLength     | 2.7906    |
| TotalNEpisodes       | 5278      |
| TotalNSamples        | 95010     |
| ExplainedVariance    | 0.29221   |
------------------------------------
[2018-07-02 16:23:45.338089 UTC] Saving snapshot
[2018-07-02 16:23:45.338983 UTC] Starting iteration 19
[2018-07-02 16:23:45.339591 UTC] Start collecting samples
[2018-07-02 16:23:52.006407 UTC] Computing input variables for policy optimization
[2018-07-02 16:23:52.208263 UTC] Performing policy update
[2018-07-02 16:23:52.210211 UTC] Computing gradient in Euclidean space
[2018-07-02 16:23:52.280391 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:23:53.018249 UTC] Performing line search
[2018-07-02 16:23:53.137392 UTC] Updating baseline
[2018-07-02 16:23:54.071506 UTC] Computing logging information
------------------------------------
| Iteration            | 19        |
| ExpectedImprovement  | 0.026909  |
| ActualImprovement    | 0.026503  |
| ImprovementRatio     | 0.98493   |
| MeanKL               | 0.0066986 |
| Entropy              | 6.4819    |
| Perplexity           | 653.23    |
| AveragePolicyStd     | 0.71331   |
| AveragePolicyStd[0]  | 0.75089   |
| AveragePolicyStd[1]  | 0.74986   |
| AveragePolicyStd[2]  | 0.71027   |
| AveragePolicyStd[3]  | 0.68167   |
| AveragePolicyStd[4]  | 0.70443   |
| AveragePolicyStd[5]  | 0.68274   |
| AverageReturn        | 2.6456    |
| MinReturn            | -2.7242   |
| MaxReturn            | 11.382    |
| StdReturn            | 2.7208    |
| AverageEpisodeLength | 18.29     |
| MinEpisodeLength     | 14        |
| MaxEpisodeLength     | 29        |
| StdEpisodeLength     | 2.1601    |
| TotalNEpisodes       | 5550      |
| TotalNSamples        | 99996     |
| ExplainedVariance    | 0.28191   |
------------------------------------
[2018-07-02 16:23:55.206623 UTC] Saving snapshot
[2018-07-02 16:23:55.207276 UTC] Starting iteration 20
[2018-07-02 16:23:55.208056 UTC] Start collecting samples
[2018-07-02 16:24:01.369355 UTC] Computing input variables for policy optimization
[2018-07-02 16:24:01.540385 UTC] Performing policy update
[2018-07-02 16:24:01.541637 UTC] Computing gradient in Euclidean space
[2018-07-02 16:24:01.602679 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:24:02.331855 UTC] Performing line search
[2018-07-02 16:24:02.433461 UTC] Updating baseline
[2018-07-02 16:24:03.448012 UTC] Computing logging information
-------------------------------------
| Iteration            | 20         |
| ExpectedImprovement  | 0.025006   |
| ActualImprovement    | 0.024588   |
| ImprovementRatio     | 0.98326    |
| MeanKL               | 0.0066232  |
| Entropy              | 6.346      |
| Perplexity           | 570.19     |
| AveragePolicyStd     | 0.6974     |
| AveragePolicyStd[0]  | 0.73048    |
| AveragePolicyStd[1]  | 0.74052    |
| AveragePolicyStd[2]  | 0.69575    |
| AveragePolicyStd[3]  | 0.66614    |
| AveragePolicyStd[4]  | 0.68786    |
| AveragePolicyStd[5]  | 0.66363    |
| AverageReturn        | 3.1416     |
| MinReturn            | -3.5299    |
| MaxReturn            | 17.593     |
| StdReturn            | 3.1309     |
| AverageEpisodeLength | 18.55      |
| MinEpisodeLength     | 14         |
| MaxEpisodeLength     | 40         |
| StdEpisodeLength     | 3.2538     |
| TotalNEpisodes       | 5821       |
| TotalNSamples        | 1.0505e+05 |
| ExplainedVariance    | 0.33998    |
-------------------------------------
[2018-07-02 16:24:04.732075 UTC] Saving snapshot
[2018-07-02 16:24:04.743396 UTC] Starting iteration 21
[2018-07-02 16:24:04.744099 UTC] Start collecting samples
[2018-07-02 16:24:16.833645 UTC] Computing input variables for policy optimization
[2018-07-02 16:24:17.418923 UTC] Performing policy update
[2018-07-02 16:24:17.423863 UTC] Computing gradient in Euclidean space
[2018-07-02 16:24:17.554446 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:24:19.154640 UTC] Performing line search
[2018-07-02 16:24:19.374727 UTC] Updating baseline
[2018-07-02 16:24:21.605456 UTC] Computing logging information
-------------------------------------
| Iteration            | 21         |
| ExpectedImprovement  | 0.023617   |
| ActualImprovement    | 0.023697   |
| ImprovementRatio     | 1.0034     |
| MeanKL               | 0.0067353  |
| Entropy              | 6.2114     |
| Perplexity           | 498.39     |
| AveragePolicyStd     | 0.68194    |
| AveragePolicyStd[0]  | 0.70888    |
| AveragePolicyStd[1]  | 0.7266     |
| AveragePolicyStd[2]  | 0.6815     |
| AveragePolicyStd[3]  | 0.65033    |
| AveragePolicyStd[4]  | 0.67793    |
| AveragePolicyStd[5]  | 0.64638    |
| AverageReturn        | 4.0407     |
| MinReturn            | -2.9604    |
| MaxReturn            | 13.394     |
| StdReturn            | 2.8226     |
| AverageEpisodeLength | 18.75      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 34         |
| StdEpisodeLength     | 2.9441     |
| TotalNEpisodes       | 6086       |
| TotalNSamples        | 1.1002e+05 |
| ExplainedVariance    | 0.33571    |
-------------------------------------
[2018-07-02 16:24:22.841014 UTC] Saving snapshot
[2018-07-02 16:24:22.841820 UTC] Starting iteration 22
[2018-07-02 16:24:22.842690 UTC] Start collecting samples
[2018-07-02 16:24:33.648524 UTC] Computing input variables for policy optimization
[2018-07-02 16:24:34.202059 UTC] Performing policy update
[2018-07-02 16:24:34.205155 UTC] Computing gradient in Euclidean space
[2018-07-02 16:24:34.343194 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:24:35.752367 UTC] Performing line search
[2018-07-02 16:24:35.939838 UTC] Updating baseline
[2018-07-02 16:24:37.584880 UTC] Computing logging information
-------------------------------------
| Iteration            | 22         |
| ExpectedImprovement  | 0.026668   |
| ActualImprovement    | 0.026505   |
| ImprovementRatio     | 0.99387    |
| MeanKL               | 0.0068404  |
| Entropy              | 6.0749     |
| Perplexity           | 434.8      |
| AveragePolicyStd     | 0.66662    |
| AveragePolicyStd[0]  | 0.69027    |
| AveragePolicyStd[1]  | 0.7111     |
| AveragePolicyStd[2]  | 0.66737    |
| AveragePolicyStd[3]  | 0.63145    |
| AveragePolicyStd[4]  | 0.66717    |
| AveragePolicyStd[5]  | 0.63237    |
| AverageReturn        | 4.2818     |
| MinReturn            | -4.4384    |
| MaxReturn            | 20.59      |
| StdReturn            | 3.2437     |
| AverageEpisodeLength | 18.65      |
| MinEpisodeLength     | 14         |
| MaxEpisodeLength     | 38         |
| StdEpisodeLength     | 2.9373     |
| TotalNEpisodes       | 6355       |
| TotalNSamples        | 1.1506e+05 |
| ExplainedVariance    | 0.42022    |
-------------------------------------
[2018-07-02 16:24:38.827045 UTC] Saving snapshot
[2018-07-02 16:24:38.828240 UTC] Starting iteration 23
[2018-07-02 16:24:38.833902 UTC] Start collecting samples
[2018-07-02 16:24:49.774859 UTC] Computing input variables for policy optimization
[2018-07-02 16:24:50.237409 UTC] Performing policy update
[2018-07-02 16:24:50.240272 UTC] Computing gradient in Euclidean space
[2018-07-02 16:24:50.323743 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:24:51.709574 UTC] Performing line search
[2018-07-02 16:24:51.946525 UTC] Updating baseline
[2018-07-02 16:24:54.017935 UTC] Computing logging information
-------------------------------------
| Iteration            | 23         |
| ExpectedImprovement  | 0.023342   |
| ActualImprovement    | 0.022907   |
| ImprovementRatio     | 0.98134    |
| MeanKL               | 0.0067273  |
| Entropy              | 5.9629     |
| Perplexity           | 388.74     |
| AveragePolicyStd     | 0.65431    |
| AveragePolicyStd[0]  | 0.6731     |
| AveragePolicyStd[1]  | 0.69934    |
| AveragePolicyStd[2]  | 0.65895    |
| AveragePolicyStd[3]  | 0.61498    |
| AveragePolicyStd[4]  | 0.6553     |
| AveragePolicyStd[5]  | 0.62418    |
| AverageReturn        | 4.708      |
| MinReturn            | -6.4566    |
| MaxReturn            | 14.827     |
| StdReturn            | 2.866      |
| AverageEpisodeLength | 18.77      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 35         |
| StdEpisodeLength     | 2.4934     |
| TotalNEpisodes       | 6611       |
| TotalNSamples        | 1.2001e+05 |
| ExplainedVariance    | 0.39839    |
-------------------------------------
[2018-07-02 16:24:55.158866 UTC] Saving snapshot
[2018-07-02 16:24:55.159445 UTC] Starting iteration 24
[2018-07-02 16:24:55.160016 UTC] Start collecting samples
[2018-07-02 16:25:06.515852 UTC] Computing input variables for policy optimization
[2018-07-02 16:25:07.228034 UTC] Performing policy update
[2018-07-02 16:25:07.236659 UTC] Computing gradient in Euclidean space
[2018-07-02 16:25:07.420215 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:25:08.990036 UTC] Performing line search
[2018-07-02 16:25:09.135756 UTC] Updating baseline
[2018-07-02 16:25:10.455598 UTC] Computing logging information
-------------------------------------
| Iteration            | 24         |
| ExpectedImprovement  | 0.026256   |
| ActualImprovement    | 0.026048   |
| ImprovementRatio     | 0.99208    |
| MeanKL               | 0.0067194  |
| Entropy              | 5.8178     |
| Perplexity           | 336.24     |
| AveragePolicyStd     | 0.63881    |
| AveragePolicyStd[0]  | 0.66138    |
| AveragePolicyStd[1]  | 0.68867    |
| AveragePolicyStd[2]  | 0.6418     |
| AveragePolicyStd[3]  | 0.60032    |
| AveragePolicyStd[4]  | 0.63618    |
| AveragePolicyStd[5]  | 0.60449    |
| AverageReturn        | 5.2707     |
| MinReturn            | -2.6077    |
| MaxReturn            | 13.792     |
| StdReturn            | 2.5162     |
| AverageEpisodeLength | 18.9       |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 34         |
| StdEpisodeLength     | 2.9        |
| TotalNEpisodes       | 6874       |
| TotalNSamples        | 1.2501e+05 |
| ExplainedVariance    | 0.51051    |
-------------------------------------
[2018-07-02 16:25:11.631180 UTC] Saving snapshot
[2018-07-02 16:25:11.632337 UTC] Starting iteration 25
[2018-07-02 16:25:11.633286 UTC] Start collecting samples
[2018-07-02 16:25:21.973092 UTC] Computing input variables for policy optimization
[2018-07-02 16:25:22.388549 UTC] Performing policy update
[2018-07-02 16:25:22.391402 UTC] Computing gradient in Euclidean space
[2018-07-02 16:25:22.501590 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:25:23.888750 UTC] Performing line search
[2018-07-02 16:25:24.086320 UTC] Updating baseline
[2018-07-02 16:25:25.987091 UTC] Computing logging information
-------------------------------------
| Iteration            | 25         |
| ExpectedImprovement  | 0.02339    |
| ActualImprovement    | 0.02316    |
| ImprovementRatio     | 0.99016    |
| MeanKL               | 0.0066148  |
| Entropy              | 5.694      |
| Perplexity           | 297.07     |
| AveragePolicyStd     | 0.62582    |
| AveragePolicyStd[0]  | 0.64686    |
| AveragePolicyStd[1]  | 0.67715    |
| AveragePolicyStd[2]  | 0.6296     |
| AveragePolicyStd[3]  | 0.58944    |
| AveragePolicyStd[4]  | 0.62462    |
| AveragePolicyStd[5]  | 0.58724    |
| AverageReturn        | 6.0587     |
| MinReturn            | 0.52199    |
| MaxReturn            | 16.07      |
| StdReturn            | 2.7391     |
| AverageEpisodeLength | 19.21      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 32         |
| StdEpisodeLength     | 2.8117     |
| TotalNEpisodes       | 7136       |
| TotalNSamples        | 1.3008e+05 |
| ExplainedVariance    | 0.42898    |
-------------------------------------
[2018-07-02 16:25:27.089263 UTC] Saving snapshot
[2018-07-02 16:25:27.089942 UTC] Starting iteration 26
[2018-07-02 16:25:27.090603 UTC] Start collecting samples
[2018-07-02 16:25:37.595489 UTC] Computing input variables for policy optimization
[2018-07-02 16:25:37.965698 UTC] Performing policy update
[2018-07-02 16:25:37.968462 UTC] Computing gradient in Euclidean space
[2018-07-02 16:25:38.067809 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:25:39.030203 UTC] Performing line search
[2018-07-02 16:25:39.214635 UTC] Updating baseline
[2018-07-02 16:25:40.485198 UTC] Computing logging information
-------------------------------------
| Iteration            | 26         |
| ExpectedImprovement  | 0.024035   |
| ActualImprovement    | 0.023879   |
| ImprovementRatio     | 0.99351    |
| MeanKL               | 0.0066512  |
| Entropy              | 5.5652     |
| Perplexity           | 261.17     |
| AveragePolicyStd     | 0.61263    |
| AveragePolicyStd[0]  | 0.63658    |
| AveragePolicyStd[1]  | 0.66117    |
| AveragePolicyStd[2]  | 0.61943    |
| AveragePolicyStd[3]  | 0.571      |
| AveragePolicyStd[4]  | 0.61532    |
| AveragePolicyStd[5]  | 0.57228    |
| AverageReturn        | 6.2672     |
| MinReturn            | -1.2205    |
| MaxReturn            | 16.06      |
| StdReturn            | 2.884      |
| AverageEpisodeLength | 19.57      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 38         |
| StdEpisodeLength     | 3.7263     |
| TotalNEpisodes       | 7394       |
| TotalNSamples        | 1.3509e+05 |
| ExplainedVariance    | 0.52775    |
-------------------------------------
[2018-07-02 16:25:41.803655 UTC] Saving snapshot
[2018-07-02 16:25:41.804310 UTC] Starting iteration 27
[2018-07-02 16:25:41.804997 UTC] Start collecting samples
[2018-07-02 16:25:51.876555 UTC] Computing input variables for policy optimization
[2018-07-02 16:25:52.294297 UTC] Performing policy update
[2018-07-02 16:25:52.297429 UTC] Computing gradient in Euclidean space
[2018-07-02 16:25:52.416622 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:25:53.927205 UTC] Performing line search
[2018-07-02 16:25:54.110039 UTC] Updating baseline
[2018-07-02 16:25:55.958162 UTC] Computing logging information
-------------------------------------
| Iteration            | 27         |
| ExpectedImprovement  | 0.022998   |
| ActualImprovement    | 0.023006   |
| ImprovementRatio     | 1.0004     |
| MeanKL               | 0.0068115  |
| Entropy              | 5.4567     |
| Perplexity           | 234.33     |
| AveragePolicyStd     | 0.60167    |
| AveragePolicyStd[0]  | 0.62656    |
| AveragePolicyStd[1]  | 0.64664    |
| AveragePolicyStd[2]  | 0.61604    |
| AveragePolicyStd[3]  | 0.5621     |
| AveragePolicyStd[4]  | 0.59876    |
| AveragePolicyStd[5]  | 0.5599     |
| AverageReturn        | 7.1841     |
| MinReturn            | 1.6107     |
| MaxReturn            | 16.328     |
| StdReturn            | 2.8159     |
| AverageEpisodeLength | 19.35      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 36         |
| StdEpisodeLength     | 2.9304     |
| TotalNEpisodes       | 7653       |
| TotalNSamples        | 1.4007e+05 |
| ExplainedVariance    | 0.53635    |
-------------------------------------
[2018-07-02 16:25:57.086757 UTC] Saving snapshot
[2018-07-02 16:25:57.087526 UTC] Starting iteration 28
[2018-07-02 16:25:57.088240 UTC] Start collecting samples
[2018-07-02 16:26:06.378464 UTC] Computing input variables for policy optimization
[2018-07-02 16:26:06.693764 UTC] Performing policy update
[2018-07-02 16:26:06.697644 UTC] Computing gradient in Euclidean space
[2018-07-02 16:26:06.803761 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:26:07.773978 UTC] Performing line search
[2018-07-02 16:26:07.907911 UTC] Updating baseline
[2018-07-02 16:26:09.208430 UTC] Computing logging information
-------------------------------------
| Iteration            | 28         |
| ExpectedImprovement  | 0.022835   |
| ActualImprovement    | 0.022666   |
| ImprovementRatio     | 0.9926     |
| MeanKL               | 0.006567   |
| Entropy              | 5.3372     |
| Perplexity           | 207.92     |
| AveragePolicyStd     | 0.58976    |
| AveragePolicyStd[0]  | 0.61059    |
| AveragePolicyStd[1]  | 0.63386    |
| AveragePolicyStd[2]  | 0.60336    |
| AveragePolicyStd[3]  | 0.55303    |
| AveragePolicyStd[4]  | 0.59036    |
| AveragePolicyStd[5]  | 0.54738    |
| AverageReturn        | 6.5053     |
| MinReturn            | -1.3131    |
| MaxReturn            | 17.471     |
| StdReturn            | 2.7937     |
| AverageEpisodeLength | 18.91      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 33         |
| StdEpisodeLength     | 2.7206     |
| TotalNEpisodes       | 7910       |
| TotalNSamples        | 1.4511e+05 |
| ExplainedVariance    | 0.50546    |
-------------------------------------
[2018-07-02 16:26:10.233039 UTC] Saving snapshot
[2018-07-02 16:26:10.233764 UTC] Starting iteration 29
[2018-07-02 16:26:10.234369 UTC] Start collecting samples
[2018-07-02 16:26:19.129254 UTC] Computing input variables for policy optimization
[2018-07-02 16:26:19.559014 UTC] Performing policy update
[2018-07-02 16:26:19.562122 UTC] Computing gradient in Euclidean space
[2018-07-02 16:26:19.699652 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:26:21.101191 UTC] Performing line search
[2018-07-02 16:26:21.290806 UTC] Updating baseline
[2018-07-02 16:26:23.272516 UTC] Computing logging information
-------------------------------------
| Iteration            | 29         |
| ExpectedImprovement  | 0.022423   |
| ActualImprovement    | 0.021982   |
| ImprovementRatio     | 0.98037    |
| MeanKL               | 0.0068386  |
| Entropy              | 5.243      |
| Perplexity           | 189.24     |
| AveragePolicyStd     | 0.58063    |
| AveragePolicyStd[0]  | 0.5981     |
| AveragePolicyStd[1]  | 0.62723    |
| AveragePolicyStd[2]  | 0.59087    |
| AveragePolicyStd[3]  | 0.54016    |
| AveragePolicyStd[4]  | 0.58714    |
| AveragePolicyStd[5]  | 0.54028    |
| AverageReturn        | 7.1715     |
| MinReturn            | -1.1049    |
| MaxReturn            | 22.62      |
| StdReturn            | 3.4159     |
| AverageEpisodeLength | 19.39      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 31         |
| StdEpisodeLength     | 2.8068     |
| TotalNEpisodes       | 8169       |
| TotalNSamples        | 1.5013e+05 |
| ExplainedVariance    | 0.51082    |
-------------------------------------
[2018-07-02 16:26:24.450378 UTC] Saving snapshot
[2018-07-02 16:26:24.451661 UTC] Starting iteration 30
[2018-07-02 16:26:24.452157 UTC] Start collecting samples
[2018-07-02 16:26:34.199375 UTC] Computing input variables for policy optimization
[2018-07-02 16:26:34.552531 UTC] Performing policy update
[2018-07-02 16:26:34.558633 UTC] Computing gradient in Euclidean space
[2018-07-02 16:26:34.690279 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:26:36.117598 UTC] Performing line search
[2018-07-02 16:26:36.382235 UTC] Updating baseline
[2018-07-02 16:26:38.315345 UTC] Computing logging information
-------------------------------------
| Iteration            | 30         |
| ExpectedImprovement  | 0.023364   |
| ActualImprovement    | 0.022824   |
| ImprovementRatio     | 0.97691    |
| MeanKL               | 0.0065936  |
| Entropy              | 5.1303     |
| Perplexity           | 169.07     |
| AveragePolicyStd     | 0.56985    |
| AveragePolicyStd[0]  | 0.58856    |
| AveragePolicyStd[1]  | 0.61643    |
| AveragePolicyStd[2]  | 0.58161    |
| AveragePolicyStd[3]  | 0.53182    |
| AveragePolicyStd[4]  | 0.57258    |
| AveragePolicyStd[5]  | 0.52812    |
| AverageReturn        | 7.3194     |
| MinReturn            | 0.75534    |
| MaxReturn            | 17.964     |
| StdReturn            | 2.6202     |
| AverageEpisodeLength | 19.2       |
| MinEpisodeLength     | 14         |
| MaxEpisodeLength     | 39         |
| StdEpisodeLength     | 3.0984     |
| TotalNEpisodes       | 8424       |
| TotalNSamples        | 1.5509e+05 |
| ExplainedVariance    | 0.59692    |
-------------------------------------
[2018-07-02 16:26:39.571720 UTC] Saving snapshot
[2018-07-02 16:26:39.582315 UTC] Starting iteration 31
[2018-07-02 16:26:39.583072 UTC] Start collecting samples
[2018-07-02 16:26:50.091496 UTC] Computing input variables for policy optimization
[2018-07-02 16:26:50.675435 UTC] Performing policy update
[2018-07-02 16:26:50.679482 UTC] Computing gradient in Euclidean space
[2018-07-02 16:26:50.810391 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:26:52.337227 UTC] Performing line search
[2018-07-02 16:26:52.524698 UTC] Updating baseline
[2018-07-02 16:26:54.687394 UTC] Computing logging information
-------------------------------------
| Iteration            | 31         |
| ExpectedImprovement  | 0.024281   |
| ActualImprovement    | 0.024089   |
| ImprovementRatio     | 0.99212    |
| MeanKL               | 0.0067242  |
| Entropy              | 5.0397     |
| Perplexity           | 154.42     |
| AveragePolicyStd     | 0.56136    |
| AveragePolicyStd[0]  | 0.57488    |
| AveragePolicyStd[1]  | 0.61195    |
| AveragePolicyStd[2]  | 0.57092    |
| AveragePolicyStd[3]  | 0.52752    |
| AveragePolicyStd[4]  | 0.56661    |
| AveragePolicyStd[5]  | 0.5163     |
| AverageReturn        | 7.9726     |
| MinReturn            | 2.248      |
| MaxReturn            | 14.639     |
| StdReturn            | 2.518      |
| AverageEpisodeLength | 19.29      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 34         |
| StdEpisodeLength     | 2.8224     |
| TotalNEpisodes       | 8683       |
| TotalNSamples        | 1.6012e+05 |
| ExplainedVariance    | 0.63119    |
-------------------------------------
[2018-07-02 16:26:55.901302 UTC] Saving snapshot
[2018-07-02 16:26:55.901926 UTC] Starting iteration 32
[2018-07-02 16:26:55.902724 UTC] Start collecting samples
[2018-07-02 16:27:07.359260 UTC] Computing input variables for policy optimization
[2018-07-02 16:27:07.821948 UTC] Performing policy update
[2018-07-02 16:27:07.825548 UTC] Computing gradient in Euclidean space
[2018-07-02 16:27:07.946088 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:27:09.530710 UTC] Performing line search
[2018-07-02 16:27:09.742304 UTC] Updating baseline
[2018-07-02 16:27:11.678752 UTC] Computing logging information
-------------------------------------
| Iteration            | 32         |
| ExpectedImprovement  | 0.024356   |
| ActualImprovement    | 0.024554   |
| ImprovementRatio     | 1.0081     |
| MeanKL               | 0.0066517  |
| Entropy              | 4.9384     |
| Perplexity           | 139.54     |
| AveragePolicyStd     | 0.55189    |
| AveragePolicyStd[0]  | 0.56712    |
| AveragePolicyStd[1]  | 0.59587    |
| AveragePolicyStd[2]  | 0.56649    |
| AveragePolicyStd[3]  | 0.51743    |
| AveragePolicyStd[4]  | 0.55443    |
| AveragePolicyStd[5]  | 0.51001    |
| AverageReturn        | 8.383      |
| MinReturn            | 2.0116     |
| MaxReturn            | 19.734     |
| StdReturn            | 3.1232     |
| AverageEpisodeLength | 19.76      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 36         |
| StdEpisodeLength     | 3.592      |
| TotalNEpisodes       | 8936       |
| TotalNSamples        | 1.6511e+05 |
| ExplainedVariance    | 0.58898    |
-------------------------------------
[2018-07-02 16:27:12.847019 UTC] Saving snapshot
[2018-07-02 16:27:12.847667 UTC] Starting iteration 33
[2018-07-02 16:27:12.848279 UTC] Start collecting samples
[2018-07-02 16:27:22.676418 UTC] Computing input variables for policy optimization
[2018-07-02 16:27:22.938084 UTC] Performing policy update
[2018-07-02 16:27:22.942202 UTC] Computing gradient in Euclidean space
[2018-07-02 16:27:23.036289 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:27:24.094263 UTC] Performing line search
[2018-07-02 16:27:24.230466 UTC] Updating baseline
[2018-07-02 16:27:26.017356 UTC] Computing logging information
-------------------------------------
| Iteration            | 33         |
| ExpectedImprovement  | 0.024176   |
| ActualImprovement    | 0.023874   |
| ImprovementRatio     | 0.98749    |
| MeanKL               | 0.0066589  |
| Entropy              | 4.8443     |
| Perplexity           | 127.01     |
| AveragePolicyStd     | 0.54341    |
| AveragePolicyStd[0]  | 0.5612     |
| AveragePolicyStd[1]  | 0.58921    |
| AveragePolicyStd[2]  | 0.55805    |
| AveragePolicyStd[3]  | 0.51015    |
| AveragePolicyStd[4]  | 0.54413    |
| AveragePolicyStd[5]  | 0.49769    |
| AverageReturn        | 8.4962     |
| MinReturn            | 1.1715     |
| MaxReturn            | 18.246     |
| StdReturn            | 2.936      |
| AverageEpisodeLength | 19.5       |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 38         |
| StdEpisodeLength     | 3.1257     |
| TotalNEpisodes       | 9192       |
| TotalNSamples        | 1.7009e+05 |
| ExplainedVariance    | 0.63101    |
-------------------------------------
[2018-07-02 16:27:27.548386 UTC] Saving snapshot
[2018-07-02 16:27:27.549181 UTC] Starting iteration 34
[2018-07-02 16:27:27.549964 UTC] Start collecting samples
[2018-07-02 16:27:37.214227 UTC] Computing input variables for policy optimization
[2018-07-02 16:27:37.779722 UTC] Performing policy update
[2018-07-02 16:27:37.785285 UTC] Computing gradient in Euclidean space
[2018-07-02 16:27:37.961713 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:27:39.269164 UTC] Performing line search
[2018-07-02 16:27:39.469283 UTC] Updating baseline
[2018-07-02 16:27:41.505650 UTC] Computing logging information
-------------------------------------
| Iteration            | 34         |
| ExpectedImprovement  | 0.022564   |
| ActualImprovement    | 0.022127   |
| ImprovementRatio     | 0.9806     |
| MeanKL               | 0.0067015  |
| Entropy              | 4.7372     |
| Perplexity           | 114.11     |
| AveragePolicyStd     | 0.53387    |
| AveragePolicyStd[0]  | 0.54425    |
| AveragePolicyStd[1]  | 0.5822     |
| AveragePolicyStd[2]  | 0.55106    |
| AveragePolicyStd[3]  | 0.49853    |
| AveragePolicyStd[4]  | 0.53903    |
| AveragePolicyStd[5]  | 0.48814    |
| AverageReturn        | 9.2139     |
| MinReturn            | 0.9666     |
| MaxReturn            | 20.953     |
| StdReturn            | 3.7194     |
| AverageEpisodeLength | 20.05      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 37         |
| StdEpisodeLength     | 4.0679     |
| TotalNEpisodes       | 9444       |
| TotalNSamples        | 1.7512e+05 |
| ExplainedVariance    | 0.58031    |
-------------------------------------
[2018-07-02 16:27:42.848288 UTC] Saving snapshot
[2018-07-02 16:27:42.849139 UTC] Starting iteration 35
[2018-07-02 16:27:42.849823 UTC] Start collecting samples
[2018-07-02 16:27:53.170575 UTC] Computing input variables for policy optimization
[2018-07-02 16:27:53.674136 UTC] Performing policy update
[2018-07-02 16:27:53.678298 UTC] Computing gradient in Euclidean space
[2018-07-02 16:27:53.798266 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:27:55.361543 UTC] Performing line search
[2018-07-02 16:27:55.545488 UTC] Updating baseline
[2018-07-02 16:27:57.804500 UTC] Computing logging information
-------------------------------------
| Iteration            | 35         |
| ExpectedImprovement  | 0.026903   |
| ActualImprovement    | 0.026171   |
| ImprovementRatio     | 0.97279    |
| MeanKL               | 0.006731   |
| Entropy              | 4.647      |
| Perplexity           | 104.27     |
| AveragePolicyStd     | 0.52609    |
| AveragePolicyStd[0]  | 0.54328    |
| AveragePolicyStd[1]  | 0.57642    |
| AveragePolicyStd[2]  | 0.54557    |
| AveragePolicyStd[3]  | 0.48116    |
| AveragePolicyStd[4]  | 0.52763    |
| AveragePolicyStd[5]  | 0.4825     |
| AverageReturn        | 9.5004     |
| MinReturn            | 2.4828     |
| MaxReturn            | 26.635     |
| StdReturn            | 3.2817     |
| AverageEpisodeLength | 20.05      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 39         |
| StdEpisodeLength     | 4.0752     |
| TotalNEpisodes       | 9696       |
| TotalNSamples        | 1.8015e+05 |
| ExplainedVariance    | 0.64382    |
-------------------------------------
[2018-07-02 16:27:59.159203 UTC] Saving snapshot
[2018-07-02 16:27:59.160259 UTC] Starting iteration 36
[2018-07-02 16:27:59.161271 UTC] Start collecting samples
[2018-07-02 16:28:08.108207 UTC] Computing input variables for policy optimization
[2018-07-02 16:28:08.601248 UTC] Performing policy update
[2018-07-02 16:28:08.607268 UTC] Computing gradient in Euclidean space
[2018-07-02 16:28:08.762930 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:28:10.382239 UTC] Performing line search
[2018-07-02 16:28:10.581591 UTC] Updating baseline
[2018-07-02 16:28:12.738795 UTC] Computing logging information
-------------------------------------
| Iteration            | 36         |
| ExpectedImprovement  | 0.023811   |
| ActualImprovement    | 0.023825   |
| ImprovementRatio     | 1.0006     |
| MeanKL               | 0.0066255  |
| Entropy              | 4.5433     |
| Perplexity           | 93.997     |
| AveragePolicyStd     | 0.51706    |
| AveragePolicyStd[0]  | 0.53813    |
| AveragePolicyStd[1]  | 0.56894    |
| AveragePolicyStd[2]  | 0.53291    |
| AveragePolicyStd[3]  | 0.47742    |
| AveragePolicyStd[4]  | 0.51074    |
| AveragePolicyStd[5]  | 0.47423    |
| AverageReturn        | 10.364     |
| MinReturn            | 2.676      |
| MaxReturn            | 33.391     |
| StdReturn            | 3.9863     |
| AverageEpisodeLength | 20.75      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 39         |
| StdEpisodeLength     | 5.0584     |
| TotalNEpisodes       | 9939       |
| TotalNSamples        | 1.8512e+05 |
| ExplainedVariance    | 0.60674    |
-------------------------------------
[2018-07-02 16:28:14.094038 UTC] Saving snapshot
[2018-07-02 16:28:14.094818 UTC] Starting iteration 37
[2018-07-02 16:28:14.095395 UTC] Start collecting samples
[2018-07-02 16:28:23.633669 UTC] Computing input variables for policy optimization
[2018-07-02 16:28:24.150642 UTC] Performing policy update
[2018-07-02 16:28:24.155018 UTC] Computing gradient in Euclidean space
[2018-07-02 16:28:24.291527 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:28:26.022692 UTC] Performing line search
[2018-07-02 16:28:26.213232 UTC] Updating baseline
[2018-07-02 16:28:28.340073 UTC] Computing logging information
-------------------------------------
| Iteration            | 37         |
| ExpectedImprovement  | 0.022509   |
| ActualImprovement    | 0.022361   |
| ImprovementRatio     | 0.99342    |
| MeanKL               | 0.0066057  |
| Entropy              | 4.4616     |
| Perplexity           | 86.623     |
| AveragePolicyStd     | 0.51014    |
| AveragePolicyStd[0]  | 0.52889    |
| AveragePolicyStd[1]  | 0.56481    |
| AveragePolicyStd[2]  | 0.52484    |
| AveragePolicyStd[3]  | 0.47207    |
| AveragePolicyStd[4]  | 0.50595    |
| AveragePolicyStd[5]  | 0.46431    |
| AverageReturn        | 11.127     |
| MinReturn            | 5.3828     |
| MaxReturn            | 24.753     |
| StdReturn            | 3.5819     |
| AverageEpisodeLength | 20.88      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 39         |
| StdEpisodeLength     | 4.928      |
| TotalNEpisodes       | 10179      |
| TotalNSamples        | 1.9016e+05 |
| ExplainedVariance    | 0.54548    |
-------------------------------------
[2018-07-02 16:28:29.476775 UTC] Saving snapshot
[2018-07-02 16:28:29.477396 UTC] Starting iteration 38
[2018-07-02 16:28:29.477937 UTC] Start collecting samples
[2018-07-02 16:28:38.343255 UTC] Computing input variables for policy optimization
[2018-07-02 16:28:38.747627 UTC] Performing policy update
[2018-07-02 16:28:38.751102 UTC] Computing gradient in Euclidean space
[2018-07-02 16:28:38.867414 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:28:40.294344 UTC] Performing line search
[2018-07-02 16:28:40.476692 UTC] Updating baseline
[2018-07-02 16:28:42.360952 UTC] Computing logging information
-------------------------------------
| Iteration            | 38         |
| ExpectedImprovement  | 0.024705   |
| ActualImprovement    | 0.024541   |
| ImprovementRatio     | 0.99335    |
| MeanKL               | 0.0066291  |
| Entropy              | 4.3911     |
| Perplexity           | 80.731     |
| AveragePolicyStd     | 0.5042     |
| AveragePolicyStd[0]  | 0.51949    |
| AveragePolicyStd[1]  | 0.56198    |
| AveragePolicyStd[2]  | 0.5158     |
| AveragePolicyStd[3]  | 0.47189    |
| AveragePolicyStd[4]  | 0.49968    |
| AveragePolicyStd[5]  | 0.45636    |
| AverageReturn        | 11.681     |
| MinReturn            | 1.3857     |
| MaxReturn            | 29.31      |
| StdReturn            | 4.3609     |
| AverageEpisodeLength | 21.32      |
| MinEpisodeLength     | 14         |
| MaxEpisodeLength     | 36         |
| StdEpisodeLength     | 4.4292     |
| TotalNEpisodes       | 10414      |
| TotalNSamples        | 1.9509e+05 |
| ExplainedVariance    | 0.6823     |
-------------------------------------
[2018-07-02 16:28:43.621743 UTC] Saving snapshot
[2018-07-02 16:28:43.622521 UTC] Starting iteration 39
[2018-07-02 16:28:43.623180 UTC] Start collecting samples
[2018-07-02 16:28:52.512694 UTC] Computing input variables for policy optimization
[2018-07-02 16:28:52.818388 UTC] Performing policy update
[2018-07-02 16:28:52.826078 UTC] Computing gradient in Euclidean space
[2018-07-02 16:28:52.983758 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:28:54.166534 UTC] Performing line search
[2018-07-02 16:28:54.294987 UTC] Updating baseline
[2018-07-02 16:28:55.900456 UTC] Computing logging information
-------------------------------------
| Iteration            | 39         |
| ExpectedImprovement  | 0.02332    |
| ActualImprovement    | 0.023214   |
| ImprovementRatio     | 0.99545    |
| MeanKL               | 0.0066279  |
| Entropy              | 4.315      |
| Perplexity           | 74.813     |
| AveragePolicyStd     | 0.49804    |
| AveragePolicyStd[0]  | 0.51516    |
| AveragePolicyStd[1]  | 0.55982    |
| AveragePolicyStd[2]  | 0.5115     |
| AveragePolicyStd[3]  | 0.46429    |
| AveragePolicyStd[4]  | 0.49068    |
| AveragePolicyStd[5]  | 0.44682    |
| AverageReturn        | 11.75      |
| MinReturn            | 4.2794     |
| MaxReturn            | 24.723     |
| StdReturn            | 3.8722     |
| AverageEpisodeLength | 21.39      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 43         |
| StdEpisodeLength     | 4.7221     |
| TotalNEpisodes       | 10650      |
| TotalNSamples        | 2.0013e+05 |
| ExplainedVariance    | 0.72062    |
-------------------------------------
[2018-07-02 16:28:57.353473 UTC] Saving snapshot
[2018-07-02 16:28:57.354104 UTC] Starting iteration 40
[2018-07-02 16:28:57.354847 UTC] Start collecting samples
[2018-07-02 16:29:06.850286 UTC] Computing input variables for policy optimization
[2018-07-02 16:29:07.327955 UTC] Performing policy update
[2018-07-02 16:29:07.331303 UTC] Computing gradient in Euclidean space
[2018-07-02 16:29:07.470853 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:29:09.093380 UTC] Performing line search
[2018-07-02 16:29:09.299639 UTC] Updating baseline
[2018-07-02 16:29:11.309112 UTC] Computing logging information
-------------------------------------
| Iteration            | 40         |
| ExpectedImprovement  | 0.022754   |
| ActualImprovement    | 0.022636   |
| ImprovementRatio     | 0.99482    |
| MeanKL               | 0.0066756  |
| Entropy              | 4.2238     |
| Perplexity           | 68.29      |
| AveragePolicyStd     | 0.49078    |
| AveragePolicyStd[0]  | 0.50704    |
| AveragePolicyStd[1]  | 0.55987    |
| AveragePolicyStd[2]  | 0.50365    |
| AveragePolicyStd[3]  | 0.45292    |
| AveragePolicyStd[4]  | 0.48258    |
| AveragePolicyStd[5]  | 0.43861    |
| AverageReturn        | 12.942     |
| MinReturn            | 5.1584     |
| MaxReturn            | 26.609     |
| StdReturn            | 4.1345     |
| AverageEpisodeLength | 23.58      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 41         |
| StdEpisodeLength     | 5.3501     |
| TotalNEpisodes       | 10875      |
| TotalNSamples        | 2.0521e+05 |
| ExplainedVariance    | 0.69817    |
-------------------------------------
[2018-07-02 16:29:12.527316 UTC] Saving snapshot
[2018-07-02 16:29:12.538029 UTC] Starting iteration 41
[2018-07-02 16:29:12.539897 UTC] Start collecting samples
[2018-07-02 16:29:22.173829 UTC] Computing input variables for policy optimization
[2018-07-02 16:29:22.672030 UTC] Performing policy update
[2018-07-02 16:29:22.674988 UTC] Computing gradient in Euclidean space
[2018-07-02 16:29:22.792494 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:29:24.404659 UTC] Performing line search
[2018-07-02 16:29:24.622153 UTC] Updating baseline
[2018-07-02 16:29:26.904382 UTC] Computing logging information
-------------------------------------
| Iteration            | 41         |
| ExpectedImprovement  | 0.023712   |
| ActualImprovement    | 0.023208   |
| ImprovementRatio     | 0.97875    |
| MeanKL               | 0.0067115  |
| Entropy              | 4.1588     |
| Perplexity           | 63.995     |
| AveragePolicyStd     | 0.48554    |
| AveragePolicyStd[0]  | 0.5024     |
| AveragePolicyStd[1]  | 0.55796    |
| AveragePolicyStd[2]  | 0.49648    |
| AveragePolicyStd[3]  | 0.44779    |
| AveragePolicyStd[4]  | 0.47134    |
| AveragePolicyStd[5]  | 0.43728    |
| AverageReturn        | 13.375     |
| MinReturn            | 5.1448     |
| MaxReturn            | 25.963     |
| StdReturn            | 3.983      |
| AverageEpisodeLength | 22.65      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 37         |
| StdEpisodeLength     | 4.394      |
| TotalNEpisodes       | 11095      |
| TotalNSamples        | 2.1010e+05 |
| ExplainedVariance    | 0.70821    |
-------------------------------------
[2018-07-02 16:29:28.231157 UTC] Saving snapshot
[2018-07-02 16:29:28.232596 UTC] Starting iteration 42
[2018-07-02 16:29:28.234827 UTC] Start collecting samples
[2018-07-02 16:29:38.359232 UTC] Computing input variables for policy optimization
[2018-07-02 16:29:38.710338 UTC] Performing policy update
[2018-07-02 16:29:38.711375 UTC] Computing gradient in Euclidean space
[2018-07-02 16:29:38.812617 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:29:39.990774 UTC] Performing line search
[2018-07-02 16:29:40.130271 UTC] Updating baseline
[2018-07-02 16:29:41.424057 UTC] Computing logging information
-------------------------------------
| Iteration            | 42         |
| ExpectedImprovement  | 0.023331   |
| ActualImprovement    | 0.023289   |
| ImprovementRatio     | 0.99818    |
| MeanKL               | 0.0066225  |
| Entropy              | 4.0829     |
| Perplexity           | 59.318     |
| AveragePolicyStd     | 0.47962    |
| AveragePolicyStd[0]  | 0.4939     |
| AveragePolicyStd[1]  | 0.55815    |
| AveragePolicyStd[2]  | 0.49085    |
| AveragePolicyStd[3]  | 0.44496    |
| AveragePolicyStd[4]  | 0.45949    |
| AveragePolicyStd[5]  | 0.43034    |
| AverageReturn        | 13.593     |
| MinReturn            | 5.143      |
| MaxReturn            | 21.116     |
| StdReturn            | 3.71       |
| AverageEpisodeLength | 22.88      |
| MinEpisodeLength     | 18         |
| MaxEpisodeLength     | 37         |
| StdEpisodeLength     | 3.8452     |
| TotalNEpisodes       | 11315      |
| TotalNSamples        | 2.1518e+05 |
| ExplainedVariance    | 0.752      |
-------------------------------------
[2018-07-02 16:29:42.626539 UTC] Saving snapshot
[2018-07-02 16:29:42.627454 UTC] Starting iteration 43
[2018-07-02 16:29:42.628219 UTC] Start collecting samples
[2018-07-02 16:29:52.510697 UTC] Computing input variables for policy optimization
[2018-07-02 16:29:52.989883 UTC] Performing policy update
[2018-07-02 16:29:52.993699 UTC] Computing gradient in Euclidean space
[2018-07-02 16:29:53.090666 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:29:54.575171 UTC] Performing line search
[2018-07-02 16:29:54.751599 UTC] Updating baseline
[2018-07-02 16:29:56.679942 UTC] Computing logging information
-------------------------------------
| Iteration            | 43         |
| ExpectedImprovement  | 0.021908   |
| ActualImprovement    | 0.022081   |
| ImprovementRatio     | 1.0079     |
| MeanKL               | 0.0066852  |
| Entropy              | 4.0358     |
| Perplexity           | 56.586     |
| AveragePolicyStd     | 0.47596    |
| AveragePolicyStd[0]  | 0.49363    |
| AveragePolicyStd[1]  | 0.55501    |
| AveragePolicyStd[2]  | 0.48562    |
| AveragePolicyStd[3]  | 0.43826    |
| AveragePolicyStd[4]  | 0.45747    |
| AveragePolicyStd[5]  | 0.4258     |
| AverageReturn        | 13.719     |
| MinReturn            | 4.2192     |
| MaxReturn            | 23.625     |
| StdReturn            | 3.9205     |
| AverageEpisodeLength | 22.97      |
| MinEpisodeLength     | 16         |
| MaxEpisodeLength     | 37         |
| StdEpisodeLength     | 3.6179     |
| TotalNEpisodes       | 11530      |
| TotalNSamples        | 2.2017e+05 |
| ExplainedVariance    | 0.74777    |
-------------------------------------
[2018-07-02 16:29:57.796928 UTC] Saving snapshot
[2018-07-02 16:29:57.797726 UTC] Starting iteration 44
[2018-07-02 16:29:57.798380 UTC] Start collecting samples
[2018-07-02 16:30:07.571703 UTC] Computing input variables for policy optimization
[2018-07-02 16:30:08.028365 UTC] Performing policy update
[2018-07-02 16:30:08.030739 UTC] Computing gradient in Euclidean space
[2018-07-02 16:30:08.196384 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:30:09.293868 UTC] Performing line search
[2018-07-02 16:30:09.440333 UTC] Updating baseline
[2018-07-02 16:30:10.776605 UTC] Computing logging information
-------------------------------------
| Iteration            | 44         |
| ExpectedImprovement  | 0.023452   |
| ActualImprovement    | 0.022762   |
| ImprovementRatio     | 0.97056    |
| MeanKL               | 0.0065564  |
| Entropy              | 3.9564     |
| Perplexity           | 52.271     |
| AveragePolicyStd     | 0.46979    |
| AveragePolicyStd[0]  | 0.49034    |
| AveragePolicyStd[1]  | 0.5481     |
| AveragePolicyStd[2]  | 0.47844    |
| AveragePolicyStd[3]  | 0.43215    |
| AveragePolicyStd[4]  | 0.45182    |
| AveragePolicyStd[5]  | 0.41789    |
| AverageReturn        | 15.026     |
| MinReturn            | 8.5547     |
| MaxReturn            | 24.184     |
| StdReturn            | 3.6178     |
| AverageEpisodeLength | 23.81      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 35         |
| StdEpisodeLength     | 3.588      |
| TotalNEpisodes       | 11741      |
| TotalNSamples        | 2.2519e+05 |
| ExplainedVariance    | 0.79983    |
-------------------------------------
[2018-07-02 16:30:11.938182 UTC] Saving snapshot
[2018-07-02 16:30:11.939201 UTC] Starting iteration 45
[2018-07-02 16:30:11.939912 UTC] Start collecting samples
[2018-07-02 16:30:20.398075 UTC] Computing input variables for policy optimization
[2018-07-02 16:30:20.787457 UTC] Performing policy update
[2018-07-02 16:30:20.790891 UTC] Computing gradient in Euclidean space
[2018-07-02 16:30:20.961060 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:30:22.368950 UTC] Performing line search
[2018-07-02 16:30:22.602075 UTC] Updating baseline
[2018-07-02 16:30:24.802325 UTC] Computing logging information
-------------------------------------
| Iteration            | 45         |
| ExpectedImprovement  | 0.02427    |
| ActualImprovement    | 0.02392    |
| ImprovementRatio     | 0.98558    |
| MeanKL               | 0.0067445  |
| Entropy              | 3.8661     |
| Perplexity           | 47.755     |
| AveragePolicyStd     | 0.46271    |
| AveragePolicyStd[0]  | 0.48495    |
| AveragePolicyStd[1]  | 0.53896    |
| AveragePolicyStd[2]  | 0.46801    |
| AveragePolicyStd[3]  | 0.42901    |
| AveragePolicyStd[4]  | 0.44367    |
| AveragePolicyStd[5]  | 0.41168    |
| AverageReturn        | 15.478     |
| MinReturn            | 5.2933     |
| MaxReturn            | 26.276     |
| StdReturn            | 3.8705     |
| AverageEpisodeLength | 24.21      |
| MinEpisodeLength     | 15         |
| MaxEpisodeLength     | 38         |
| StdEpisodeLength     | 3.7797     |
| TotalNEpisodes       | 11946      |
| TotalNSamples        | 2.3018e+05 |
| ExplainedVariance    | 0.79755    |
-------------------------------------
[2018-07-02 16:30:25.951262 UTC] Saving snapshot
[2018-07-02 16:30:25.952201 UTC] Starting iteration 46
[2018-07-02 16:30:25.953293 UTC] Start collecting samples
[2018-07-02 16:30:34.353209 UTC] Computing input variables for policy optimization
[2018-07-02 16:30:34.733157 UTC] Performing policy update
[2018-07-02 16:30:34.734623 UTC] Computing gradient in Euclidean space
[2018-07-02 16:30:34.860592 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:30:36.322344 UTC] Performing line search
[2018-07-02 16:30:36.501938 UTC] Updating baseline
[2018-07-02 16:30:38.410986 UTC] Computing logging information
-------------------------------------
| Iteration            | 46         |
| ExpectedImprovement  | 0.022764   |
| ActualImprovement    | 0.022476   |
| ImprovementRatio     | 0.98737    |
| MeanKL               | 0.0066258  |
| Entropy              | 3.7827     |
| Perplexity           | 43.936     |
| AveragePolicyStd     | 0.45642    |
| AveragePolicyStd[0]  | 0.48133    |
| AveragePolicyStd[1]  | 0.53282    |
| AveragePolicyStd[2]  | 0.46207    |
| AveragePolicyStd[3]  | 0.42462    |
| AveragePolicyStd[4]  | 0.43239    |
| AveragePolicyStd[5]  | 0.40531    |
| AverageReturn        | 16.741     |
| MinReturn            | 7.2054     |
| MaxReturn            | 31.228     |
| StdReturn            | 4.0813     |
| AverageEpisodeLength | 25.47      |
| MinEpisodeLength     | 17         |
| MaxEpisodeLength     | 40         |
| StdEpisodeLength     | 4.2625     |
| TotalNEpisodes       | 12147      |
| TotalNSamples        | 2.3517e+05 |
| ExplainedVariance    | 0.80559    |
-------------------------------------
[2018-07-02 16:30:39.593687 UTC] Saving snapshot
[2018-07-02 16:30:39.594492 UTC] Starting iteration 47
[2018-07-02 16:30:39.595272 UTC] Start collecting samples
[2018-07-02 16:30:48.795323 UTC] Computing input variables for policy optimization
[2018-07-02 16:30:49.063901 UTC] Performing policy update
[2018-07-02 16:30:49.068131 UTC] Computing gradient in Euclidean space
[2018-07-02 16:30:49.178219 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:30:50.658159 UTC] Performing line search
[2018-07-02 16:30:50.850775 UTC] Updating baseline
[2018-07-02 16:30:52.942915 UTC] Computing logging information
-------------------------------------
| Iteration            | 47         |
| ExpectedImprovement  | 0.022364   |
| ActualImprovement    | 0.022198   |
| ImprovementRatio     | 0.99256    |
| MeanKL               | 0.0066884  |
| Entropy              | 3.7089     |
| Perplexity           | 40.811     |
| AveragePolicyStd     | 0.45096    |
| AveragePolicyStd[0]  | 0.47379    |
| AveragePolicyStd[1]  | 0.52874    |
| AveragePolicyStd[2]  | 0.46022    |
| AveragePolicyStd[3]  | 0.41704    |
| AveragePolicyStd[4]  | 0.42689    |
| AveragePolicyStd[5]  | 0.39908    |
| AverageReturn        | 17.287     |
| MinReturn            | 8.7649     |
| MaxReturn            | 29.297     |
| StdReturn            | 3.367      |
| AverageEpisodeLength | 26.25      |
| MinEpisodeLength     | 18         |
| MaxEpisodeLength     | 45         |
| StdEpisodeLength     | 4.1119     |
| TotalNEpisodes       | 12339      |
| TotalNSamples        | 2.4017e+05 |
| ExplainedVariance    | 0.84985    |
-------------------------------------
[2018-07-02 16:30:54.322086 UTC] Saving snapshot
[2018-07-02 16:30:54.322976 UTC] Starting iteration 48
[2018-07-02 16:30:54.324024 UTC] Start collecting samples
[2018-07-02 16:31:04.215516 UTC] Computing input variables for policy optimization
[2018-07-02 16:31:04.644826 UTC] Performing policy update
[2018-07-02 16:31:04.655754 UTC] Computing gradient in Euclidean space
[2018-07-02 16:31:04.726817 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:31:05.800131 UTC] Performing line search
[2018-07-02 16:31:05.975409 UTC] Updating baseline
[2018-07-02 16:31:07.393528 UTC] Computing logging information
-------------------------------------
| Iteration            | 48         |
| ExpectedImprovement  | 0.028372   |
| ActualImprovement    | 0.028028   |
| ImprovementRatio     | 0.98788    |
| MeanKL               | 0.0066269  |
| Entropy              | 3.6523     |
| Perplexity           | 38.565     |
| AveragePolicyStd     | 0.44686    |
| AveragePolicyStd[0]  | 0.46925    |
| AveragePolicyStd[1]  | 0.52739    |
| AveragePolicyStd[2]  | 0.45553    |
| AveragePolicyStd[3]  | 0.41351    |
| AveragePolicyStd[4]  | 0.42198    |
| AveragePolicyStd[5]  | 0.3935     |
| AverageReturn        | 17.891     |
| MinReturn            | 9.65       |
| MaxReturn            | 25.661     |
| StdReturn            | 2.8991     |
| AverageEpisodeLength | 26.55      |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 35         |
| StdEpisodeLength     | 2.9542     |
| TotalNEpisodes       | 12527      |
| TotalNSamples        | 2.4514e+05 |
| ExplainedVariance    | 0.87209    |
-------------------------------------
[2018-07-02 16:31:08.784731 UTC] Saving snapshot
[2018-07-02 16:31:08.785447 UTC] Starting iteration 49
[2018-07-02 16:31:08.786329 UTC] Start collecting samples
[2018-07-02 16:31:18.125923 UTC] Computing input variables for policy optimization
[2018-07-02 16:31:18.538471 UTC] Performing policy update
[2018-07-02 16:31:18.542178 UTC] Computing gradient in Euclidean space
[2018-07-02 16:31:18.662332 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:31:20.223524 UTC] Performing line search
[2018-07-02 16:31:20.433294 UTC] Updating baseline
[2018-07-02 16:31:22.434437 UTC] Computing logging information
-------------------------------------
| Iteration            | 49         |
| ExpectedImprovement  | 0.024005   |
| ActualImprovement    | 0.023968   |
| ImprovementRatio     | 0.99847    |
| MeanKL               | 0.0065863  |
| Entropy              | 3.583      |
| Perplexity           | 35.981     |
| AveragePolicyStd     | 0.44195    |
| AveragePolicyStd[0]  | 0.46129    |
| AveragePolicyStd[1]  | 0.52902    |
| AveragePolicyStd[2]  | 0.44936    |
| AveragePolicyStd[3]  | 0.41081    |
| AveragePolicyStd[4]  | 0.41335    |
| AveragePolicyStd[5]  | 0.38785    |
| AverageReturn        | 18.554     |
| MinReturn            | 10.201     |
| MaxReturn            | 27.501     |
| StdReturn            | 3.5116     |
| AverageEpisodeLength | 27.13      |
| MinEpisodeLength     | 19         |
| MaxEpisodeLength     | 38         |
| StdEpisodeLength     | 3.8332     |
| TotalNEpisodes       | 12714      |
| TotalNSamples        | 2.5021e+05 |
| ExplainedVariance    | 0.87316    |
-------------------------------------
[2018-07-02 16:31:23.719801 UTC] Saving snapshot
[2018-07-02 16:31:23.720641 UTC] Starting iteration 50
[2018-07-02 16:31:23.721320 UTC] Start collecting samples
[2018-07-02 16:31:32.770864 UTC] Computing input variables for policy optimization
[2018-07-02 16:31:33.163009 UTC] Performing policy update
[2018-07-02 16:31:33.166317 UTC] Computing gradient in Euclidean space
[2018-07-02 16:31:33.293047 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:31:34.755885 UTC] Performing line search
[2018-07-02 16:31:34.935284 UTC] Updating baseline
[2018-07-02 16:31:36.919375 UTC] Computing logging information
-------------------------------------
| Iteration            | 50         |
| ExpectedImprovement  | 0.025663   |
| ActualImprovement    | 0.025471   |
| ImprovementRatio     | 0.99252    |
| MeanKL               | 0.0066931  |
| Entropy              | 3.5076     |
| Perplexity           | 33.369     |
| AveragePolicyStd     | 0.43648    |
| AveragePolicyStd[0]  | 0.45782    |
| AveragePolicyStd[1]  | 0.52329    |
| AveragePolicyStd[2]  | 0.43989    |
| AveragePolicyStd[3]  | 0.40855    |
| AveragePolicyStd[4]  | 0.40903    |
| AveragePolicyStd[5]  | 0.3803     |
| AverageReturn        | 19.087     |
| MinReturn            | 8.4653     |
| MaxReturn            | 30.463     |
| StdReturn            | 4.2956     |
| AverageEpisodeLength | 28.47      |
| MinEpisodeLength     | 18         |
| MaxEpisodeLength     | 41         |
| StdEpisodeLength     | 4.5485     |
| TotalNEpisodes       | 12891      |
| TotalNSamples        | 2.5519e+05 |
| ExplainedVariance    | 0.85668    |
-------------------------------------
[2018-07-02 16:31:38.188681 UTC] Saving snapshot
[2018-07-02 16:31:38.201059 UTC] Starting iteration 51
[2018-07-02 16:31:38.202094 UTC] Start collecting samples
[2018-07-02 16:31:47.597218 UTC] Computing input variables for policy optimization
[2018-07-02 16:31:47.975731 UTC] Performing policy update
[2018-07-02 16:31:47.979789 UTC] Computing gradient in Euclidean space
[2018-07-02 16:31:48.047901 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:31:49.712267 UTC] Performing line search
[2018-07-02 16:31:49.892731 UTC] Updating baseline
[2018-07-02 16:31:51.818958 UTC] Computing logging information
-------------------------------------
| Iteration            | 51         |
| ExpectedImprovement  | 0.027215   |
| ActualImprovement    | 0.027734   |
| ImprovementRatio     | 1.0191     |
| MeanKL               | 0.0066411  |
| Entropy              | 3.4698     |
| Perplexity           | 32.13      |
| AveragePolicyStd     | 0.43388    |
| AveragePolicyStd[0]  | 0.45603    |
| AveragePolicyStd[1]  | 0.52293    |
| AveragePolicyStd[2]  | 0.43704    |
| AveragePolicyStd[3]  | 0.40794    |
| AveragePolicyStd[4]  | 0.40273    |
| AveragePolicyStd[5]  | 0.37664    |
| AverageReturn        | 21.033     |
| MinReturn            | 10.239     |
| MaxReturn            | 31.935     |
| StdReturn            | 4.2456     |
| AverageEpisodeLength | 29.84      |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 42         |
| StdEpisodeLength     | 4.3628     |
| TotalNEpisodes       | 13059      |
| TotalNSamples        | 2.6023e+05 |
| ExplainedVariance    | 0.85872    |
-------------------------------------
[2018-07-02 16:31:53.032343 UTC] Saving snapshot
[2018-07-02 16:31:53.033454 UTC] Starting iteration 52
[2018-07-02 16:31:53.034213 UTC] Start collecting samples
[2018-07-02 16:32:03.157408 UTC] Computing input variables for policy optimization
[2018-07-02 16:32:03.561182 UTC] Performing policy update
[2018-07-02 16:32:03.565449 UTC] Computing gradient in Euclidean space
[2018-07-02 16:32:03.700038 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:32:05.305320 UTC] Performing line search
[2018-07-02 16:32:05.485049 UTC] Updating baseline
[2018-07-02 16:32:07.451889 UTC] Computing logging information
-------------------------------------
| Iteration            | 52         |
| ExpectedImprovement  | 0.028035   |
| ActualImprovement    | 0.028118   |
| ImprovementRatio     | 1.003      |
| MeanKL               | 0.0068763  |
| Entropy              | 3.4138     |
| Perplexity           | 30.381     |
| AveragePolicyStd     | 0.43008    |
| AveragePolicyStd[0]  | 0.45362    |
| AveragePolicyStd[1]  | 0.52116    |
| AveragePolicyStd[2]  | 0.43644    |
| AveragePolicyStd[3]  | 0.40318    |
| AveragePolicyStd[4]  | 0.3943     |
| AveragePolicyStd[5]  | 0.37176    |
| AverageReturn        | 22.31      |
| MinReturn            | 12.394     |
| MaxReturn            | 40.71      |
| StdReturn            | 5.1944     |
| AverageEpisodeLength | 30.43      |
| MinEpisodeLength     | 20         |
| MaxEpisodeLength     | 50         |
| StdEpisodeLength     | 4.4906     |
| TotalNEpisodes       | 13221      |
| TotalNSamples        | 2.6519e+05 |
| ExplainedVariance    | 0.8825     |
-------------------------------------
[2018-07-02 16:32:08.642555 UTC] Saving snapshot
[2018-07-02 16:32:08.643676 UTC] Starting iteration 53
[2018-07-02 16:32:08.644397 UTC] Start collecting samples
[2018-07-02 16:32:17.285028 UTC] Computing input variables for policy optimization
[2018-07-02 16:32:17.641036 UTC] Performing policy update
[2018-07-02 16:32:17.644071 UTC] Computing gradient in Euclidean space
[2018-07-02 16:32:17.791552 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:32:19.582642 UTC] Performing line search
[2018-07-02 16:32:19.842129 UTC] Updating baseline
[2018-07-02 16:32:21.679242 UTC] Computing logging information
-------------------------------------
| Iteration            | 53         |
| ExpectedImprovement  | 0.028399   |
| ActualImprovement    | 0.028347   |
| ImprovementRatio     | 0.99814    |
| MeanKL               | 0.006678   |
| Entropy              | 3.3627     |
| Perplexity           | 28.866     |
| AveragePolicyStd     | 0.42643    |
| AveragePolicyStd[0]  | 0.44984    |
| AveragePolicyStd[1]  | 0.51625    |
| AveragePolicyStd[2]  | 0.43337    |
| AveragePolicyStd[3]  | 0.4014     |
| AveragePolicyStd[4]  | 0.38918    |
| AveragePolicyStd[5]  | 0.3685     |
| AverageReturn        | 23.123     |
| MinReturn            | 12.501     |
| MaxReturn            | 39.36      |
| StdReturn            | 5.1848     |
| AverageEpisodeLength | 31.94      |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 43         |
| StdEpisodeLength     | 4.2138     |
| TotalNEpisodes       | 13377      |
| TotalNSamples        | 2.7019e+05 |
| ExplainedVariance    | 0.84719    |
-------------------------------------
[2018-07-02 16:32:23.185281 UTC] Saving snapshot
[2018-07-02 16:32:23.185969 UTC] Starting iteration 54
[2018-07-02 16:32:23.186557 UTC] Start collecting samples
[2018-07-02 16:32:32.106713 UTC] Computing input variables for policy optimization
[2018-07-02 16:32:32.550662 UTC] Performing policy update
[2018-07-02 16:32:32.556204 UTC] Computing gradient in Euclidean space
[2018-07-02 16:32:32.700111 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:32:34.355271 UTC] Performing line search
[2018-07-02 16:32:34.544176 UTC] Updating baseline
[2018-07-02 16:32:36.623758 UTC] Computing logging information
-------------------------------------
| Iteration            | 54         |
| ExpectedImprovement  | 0.02709    |
| ActualImprovement    | 0.027181   |
| ImprovementRatio     | 1.0034     |
| MeanKL               | 0.0064719  |
| Entropy              | 3.3175     |
| Perplexity           | 27.592     |
| AveragePolicyStd     | 0.42332    |
| AveragePolicyStd[0]  | 0.44641    |
| AveragePolicyStd[1]  | 0.51442    |
| AveragePolicyStd[2]  | 0.43071    |
| AveragePolicyStd[3]  | 0.39781    |
| AveragePolicyStd[4]  | 0.38428    |
| AveragePolicyStd[5]  | 0.36626    |
| AverageReturn        | 25.08      |
| MinReturn            | 13.069     |
| MaxReturn            | 49.014     |
| StdReturn            | 6.7637     |
| AverageEpisodeLength | 33.99      |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 50         |
| StdEpisodeLength     | 5.283      |
| TotalNEpisodes       | 13524      |
| TotalNSamples        | 2.7516e+05 |
| ExplainedVariance    | 0.82774    |
-------------------------------------
[2018-07-02 16:32:37.888163 UTC] Saving snapshot
[2018-07-02 16:32:37.888892 UTC] Starting iteration 55
[2018-07-02 16:32:37.889568 UTC] Start collecting samples
[2018-07-02 16:32:47.453781 UTC] Computing input variables for policy optimization
[2018-07-02 16:32:47.743178 UTC] Performing policy update
[2018-07-02 16:32:47.746306 UTC] Computing gradient in Euclidean space
[2018-07-02 16:32:47.919008 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:32:49.067603 UTC] Performing line search
[2018-07-02 16:32:49.212024 UTC] Updating baseline
[2018-07-02 16:32:50.669614 UTC] Computing logging information
-------------------------------------
| Iteration            | 55         |
| ExpectedImprovement  | 0.027009   |
| ActualImprovement    | 0.027022   |
| ImprovementRatio     | 1.0005     |
| MeanKL               | 0.0065206  |
| Entropy              | 3.2894     |
| Perplexity           | 26.828     |
| AveragePolicyStd     | 0.42133    |
| AveragePolicyStd[0]  | 0.443      |
| AveragePolicyStd[1]  | 0.51165    |
| AveragePolicyStd[2]  | 0.4316     |
| AveragePolicyStd[3]  | 0.39388    |
| AveragePolicyStd[4]  | 0.38217    |
| AveragePolicyStd[5]  | 0.36567    |
| AverageReturn        | 29.575     |
| MinReturn            | 16.408     |
| MaxReturn            | 56.125     |
| StdReturn            | 8.9336     |
| AverageEpisodeLength | 37.58      |
| MinEpisodeLength     | 24         |
| MaxEpisodeLength     | 55         |
| StdEpisodeLength     | 7.0387     |
| TotalNEpisodes       | 13659      |
| TotalNSamples        | 2.8015e+05 |
| ExplainedVariance    | 0.78823    |
-------------------------------------
[2018-07-02 16:32:51.906849 UTC] Saving snapshot
[2018-07-02 16:32:51.907504 UTC] Starting iteration 56
[2018-07-02 16:32:51.908292 UTC] Start collecting samples
[2018-07-02 16:32:58.903530 UTC] Computing input variables for policy optimization
[2018-07-02 16:32:59.017288 UTC] Performing policy update
[2018-07-02 16:32:59.018447 UTC] Computing gradient in Euclidean space
[2018-07-02 16:32:59.080876 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:32:59.757324 UTC] Performing line search
[2018-07-02 16:32:59.851849 UTC] Updating baseline
[2018-07-02 16:33:00.882210 UTC] Computing logging information
-------------------------------------
| Iteration            | 56         |
| ExpectedImprovement  | 0.028544   |
| ActualImprovement    | 0.028534   |
| ImprovementRatio     | 0.99964    |
| MeanKL               | 0.0067514  |
| Entropy              | 3.252      |
| Perplexity           | 25.842     |
| AveragePolicyStd     | 0.41894    |
| AveragePolicyStd[0]  | 0.44039    |
| AveragePolicyStd[1]  | 0.51337    |
| AveragePolicyStd[2]  | 0.42984    |
| AveragePolicyStd[3]  | 0.39033    |
| AveragePolicyStd[4]  | 0.37781    |
| AveragePolicyStd[5]  | 0.36193    |
| AverageReturn        | 31.703     |
| MinReturn            | 11.609     |
| MaxReturn            | 58.104     |
| StdReturn            | 8.8019     |
| AverageEpisodeLength | 39.35      |
| MinEpisodeLength     | 27         |
| MaxEpisodeLength     | 65         |
| StdEpisodeLength     | 7.4597     |
| TotalNEpisodes       | 13787      |
| TotalNSamples        | 2.8512e+05 |
| ExplainedVariance    | 0.83574    |
-------------------------------------
[2018-07-02 16:33:01.885599 UTC] Saving snapshot
[2018-07-02 16:33:01.886346 UTC] Starting iteration 57
[2018-07-02 16:33:01.887140 UTC] Start collecting samples
[2018-07-02 16:33:06.376620 UTC] Computing input variables for policy optimization
[2018-07-02 16:33:06.491847 UTC] Performing policy update
[2018-07-02 16:33:06.493646 UTC] Computing gradient in Euclidean space
[2018-07-02 16:33:06.559620 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:33:07.241802 UTC] Performing line search
[2018-07-02 16:33:07.339483 UTC] Updating baseline
[2018-07-02 16:33:08.259683 UTC] Computing logging information
------------------------------------
| Iteration            | 57        |
| ExpectedImprovement  | 0.027826  |
| ActualImprovement    | 0.027208  |
| ImprovementRatio     | 0.97781   |
| MeanKL               | 0.0066497 |
| Entropy              | 3.2183    |
| Perplexity           | 24.984    |
| AveragePolicyStd     | 0.41678   |
| AveragePolicyStd[0]  | 0.44018   |
| AveragePolicyStd[1]  | 0.51155   |
| AveragePolicyStd[2]  | 0.43103   |
| AveragePolicyStd[3]  | 0.3874    |
| AveragePolicyStd[4]  | 0.37254   |
| AveragePolicyStd[5]  | 0.358     |
| AverageReturn        | 32.181    |
| MinReturn            | 16.792    |
| MaxReturn            | 60.564    |
| StdReturn            | 9.3693    |
| AverageEpisodeLength | 40.34     |
| MinEpisodeLength     | 28        |
| MaxEpisodeLength     | 73        |
| StdEpisodeLength     | 8.0972    |
| TotalNEpisodes       | 13913     |
| TotalNSamples        | 2.902e+05 |
| ExplainedVariance    | 0.82139   |
------------------------------------
[2018-07-02 16:33:09.231374 UTC] Saving snapshot
[2018-07-02 16:33:09.232086 UTC] Starting iteration 58
[2018-07-02 16:33:09.232710 UTC] Start collecting samples
[2018-07-02 16:33:13.460763 UTC] Computing input variables for policy optimization
[2018-07-02 16:33:13.608352 UTC] Performing policy update
[2018-07-02 16:33:13.609596 UTC] Computing gradient in Euclidean space
[2018-07-02 16:33:13.675750 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:33:14.454011 UTC] Performing line search
[2018-07-02 16:33:14.567636 UTC] Updating baseline
[2018-07-02 16:33:15.719060 UTC] Computing logging information
-------------------------------------
| Iteration            | 58         |
| ExpectedImprovement  | 0.024636   |
| ActualImprovement    | 0.024257   |
| ImprovementRatio     | 0.98459    |
| MeanKL               | 0.0066635  |
| Entropy              | 3.186      |
| Perplexity           | 24.192     |
| AveragePolicyStd     | 0.41463    |
| AveragePolicyStd[0]  | 0.43976    |
| AveragePolicyStd[1]  | 0.51039    |
| AveragePolicyStd[2]  | 0.42752    |
| AveragePolicyStd[3]  | 0.38304    |
| AveragePolicyStd[4]  | 0.37088    |
| AveragePolicyStd[5]  | 0.35621    |
| AverageReturn        | 36.336     |
| MinReturn            | 16.153     |
| MaxReturn            | 73.031     |
| StdReturn            | 11.071     |
| AverageEpisodeLength | 46.05      |
| MinEpisodeLength     | 30         |
| MaxEpisodeLength     | 88         |
| StdEpisodeLength     | 11.66      |
| TotalNEpisodes       | 14020      |
| TotalNSamples        | 2.9508e+05 |
| ExplainedVariance    | 0.80214    |
-------------------------------------
[2018-07-02 16:33:17.060079 UTC] Saving snapshot
[2018-07-02 16:33:17.060930 UTC] Starting iteration 59
[2018-07-02 16:33:17.061760 UTC] Start collecting samples
[2018-07-02 16:33:21.904475 UTC] Computing input variables for policy optimization
[2018-07-02 16:33:22.016686 UTC] Performing policy update
[2018-07-02 16:33:22.017966 UTC] Computing gradient in Euclidean space
[2018-07-02 16:33:22.085829 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:33:22.792230 UTC] Performing line search
[2018-07-02 16:33:22.924683 UTC] Updating baseline
[2018-07-02 16:33:24.009420 UTC] Computing logging information
-------------------------------------
| Iteration            | 59         |
| ExpectedImprovement  | 0.023134   |
| ActualImprovement    | 0.023381   |
| ImprovementRatio     | 1.0106     |
| MeanKL               | 0.0067964  |
| Entropy              | 3.1484     |
| Perplexity           | 23.299     |
| AveragePolicyStd     | 0.41211    |
| AveragePolicyStd[0]  | 0.43867    |
| AveragePolicyStd[1]  | 0.50656    |
| AveragePolicyStd[2]  | 0.42729    |
| AveragePolicyStd[3]  | 0.37867    |
| AveragePolicyStd[4]  | 0.36925    |
| AveragePolicyStd[5]  | 0.35224    |
| AverageReturn        | 39.469     |
| MinReturn            | 16.339     |
| MaxReturn            | 62.819     |
| StdReturn            | 10.624     |
| AverageEpisodeLength | 49.12      |
| MinEpisodeLength     | 31         |
| MaxEpisodeLength     | 80         |
| StdEpisodeLength     | 10.592     |
| TotalNEpisodes       | 14122      |
| TotalNSamples        | 3.0009e+05 |
| ExplainedVariance    | 0.82295    |
-------------------------------------
[2018-07-02 16:33:25.306257 UTC] Saving snapshot
[2018-07-02 16:33:25.306927 UTC] Starting iteration 60
[2018-07-02 16:33:25.307623 UTC] Start collecting samples
[2018-07-02 16:33:29.508481 UTC] Computing input variables for policy optimization
[2018-07-02 16:33:29.604193 UTC] Performing policy update
[2018-07-02 16:33:29.605187 UTC] Computing gradient in Euclidean space
[2018-07-02 16:33:29.665417 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:33:30.362326 UTC] Performing line search
[2018-07-02 16:33:30.456205 UTC] Updating baseline
[2018-07-02 16:33:31.481537 UTC] Computing logging information
-------------------------------------
| Iteration            | 60         |
| ExpectedImprovement  | 0.023123   |
| ActualImprovement    | 0.022758   |
| ImprovementRatio     | 0.98418    |
| MeanKL               | 0.0067573  |
| Entropy              | 3.1327     |
| Perplexity           | 22.935     |
| AveragePolicyStd     | 0.41087    |
| AveragePolicyStd[0]  | 0.43663    |
| AveragePolicyStd[1]  | 0.50232    |
| AveragePolicyStd[2]  | 0.42519    |
| AveragePolicyStd[3]  | 0.38124    |
| AveragePolicyStd[4]  | 0.3677     |
| AveragePolicyStd[5]  | 0.35214    |
| AverageReturn        | 41.866     |
| MinReturn            | 10.946     |
| MaxReturn            | 64.821     |
| StdReturn            | 10.381     |
| AverageEpisodeLength | 52.88      |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 96         |
| StdEpisodeLength     | 12.162     |
| TotalNEpisodes       | 14217      |
| TotalNSamples        | 3.0512e+05 |
| ExplainedVariance    | 0.84426    |
-------------------------------------
[2018-07-02 16:33:32.707530 UTC] Saving snapshot
[2018-07-02 16:33:32.720322 UTC] Starting iteration 61
[2018-07-02 16:33:32.721205 UTC] Start collecting samples
[2018-07-02 16:33:37.031522 UTC] Computing input variables for policy optimization
[2018-07-02 16:33:37.134214 UTC] Performing policy update
[2018-07-02 16:33:37.135396 UTC] Computing gradient in Euclidean space
[2018-07-02 16:33:37.200060 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:33:37.897701 UTC] Performing line search
[2018-07-02 16:33:38.046661 UTC] Updating baseline
[2018-07-02 16:33:38.991749 UTC] Computing logging information
-------------------------------------
| Iteration            | 61         |
| ExpectedImprovement  | 0.018358   |
| ActualImprovement    | 0.017923   |
| ImprovementRatio     | 0.97635    |
| MeanKL               | 0.0067369  |
| Entropy              | 3.0912     |
| Perplexity           | 22.004     |
| AveragePolicyStd     | 0.40827    |
| AveragePolicyStd[0]  | 0.43437    |
| AveragePolicyStd[1]  | 0.50246    |
| AveragePolicyStd[2]  | 0.4228     |
| AveragePolicyStd[3]  | 0.38082    |
| AveragePolicyStd[4]  | 0.36201    |
| AveragePolicyStd[5]  | 0.34716    |
| AverageReturn        | 46.336     |
| MinReturn            | 12.584     |
| MaxReturn            | 88.81      |
| StdReturn            | 11.44      |
| AverageEpisodeLength | 57.28      |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 179        |
| StdEpisodeLength     | 17.6       |
| TotalNEpisodes       | 14301      |
| TotalNSamples        | 3.0994e+05 |
| ExplainedVariance    | 0.78671    |
-------------------------------------
[2018-07-02 16:33:40.160217 UTC] Saving snapshot
[2018-07-02 16:33:40.161301 UTC] Starting iteration 62
[2018-07-02 16:33:40.162163 UTC] Start collecting samples
[2018-07-02 16:33:44.195982 UTC] Computing input variables for policy optimization
[2018-07-02 16:33:44.289360 UTC] Performing policy update
[2018-07-02 16:33:44.290552 UTC] Computing gradient in Euclidean space
[2018-07-02 16:33:44.351773 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:33:45.014163 UTC] Performing line search
[2018-07-02 16:33:45.108282 UTC] Updating baseline
[2018-07-02 16:33:45.998758 UTC] Computing logging information
-------------------------------------
| Iteration            | 62         |
| ExpectedImprovement  | 0.021079   |
| ActualImprovement    | 0.020588   |
| ImprovementRatio     | 0.97669    |
| MeanKL               | 0.0065162  |
| Entropy              | 3.035      |
| Perplexity           | 20.802     |
| AveragePolicyStd     | 0.40466    |
| AveragePolicyStd[0]  | 0.42984    |
| AveragePolicyStd[1]  | 0.50096    |
| AveragePolicyStd[2]  | 0.42147    |
| AveragePolicyStd[3]  | 0.37566    |
| AveragePolicyStd[4]  | 0.35712    |
| AveragePolicyStd[5]  | 0.34292    |
| AverageReturn        | 47.764     |
| MinReturn            | 9.5016     |
| MaxReturn            | 75.958     |
| StdReturn            | 11.31      |
| AverageEpisodeLength | 60.8       |
| MinEpisodeLength     | 32         |
| MaxEpisodeLength     | 109        |
| StdEpisodeLength     | 15.268     |
| TotalNEpisodes       | 14385      |
| TotalNSamples        | 3.1501e+05 |
| ExplainedVariance    | 0.8411     |
-------------------------------------
[2018-07-02 16:33:47.136920 UTC] Saving snapshot
[2018-07-02 16:33:47.137557 UTC] Starting iteration 63
[2018-07-02 16:33:47.138297 UTC] Start collecting samples
[2018-07-02 16:33:51.749563 UTC] Computing input variables for policy optimization
[2018-07-02 16:33:51.841088 UTC] Performing policy update
[2018-07-02 16:33:51.842244 UTC] Computing gradient in Euclidean space
[2018-07-02 16:33:51.911583 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:33:52.589567 UTC] Performing line search
[2018-07-02 16:33:52.707115 UTC] Updating baseline
[2018-07-02 16:33:53.638124 UTC] Computing logging information
------------------------------------
| Iteration            | 63        |
| ExpectedImprovement  | 0.019032  |
| ActualImprovement    | 0.018503  |
| ImprovementRatio     | 0.97217   |
| MeanKL               | 0.0067083 |
| Entropy              | 3.0089    |
| Perplexity           | 20.266    |
| AveragePolicyStd     | 0.4031    |
| AveragePolicyStd[0]  | 0.42923   |
| AveragePolicyStd[1]  | 0.50088   |
| AveragePolicyStd[2]  | 0.42026   |
| AveragePolicyStd[3]  | 0.3764    |
| AveragePolicyStd[4]  | 0.35349   |
| AveragePolicyStd[5]  | 0.33836   |
| AverageReturn        | 50.142    |
| MinReturn            | 18.698    |
| MaxReturn            | 80.57     |
| StdReturn            | 10.131    |
| AverageEpisodeLength | 61.47     |
| MinEpisodeLength     | 36        |
| MaxEpisodeLength     | 110       |
| StdEpisodeLength     | 14.825    |
| TotalNEpisodes       | 14468     |
| TotalNSamples        | 3.201e+05 |
| ExplainedVariance    | 0.85915   |
------------------------------------
[2018-07-02 16:33:54.638366 UTC] Saving snapshot
[2018-07-02 16:33:54.639028 UTC] Starting iteration 64
[2018-07-02 16:33:54.639983 UTC] Start collecting samples
[2018-07-02 16:33:58.341691 UTC] Computing input variables for policy optimization
[2018-07-02 16:33:58.430801 UTC] Performing policy update
[2018-07-02 16:33:58.431932 UTC] Computing gradient in Euclidean space
[2018-07-02 16:33:58.495263 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:33:59.164114 UTC] Performing line search
[2018-07-02 16:33:59.256741 UTC] Updating baseline
[2018-07-02 16:34:00.130458 UTC] Computing logging information
-------------------------------------
| Iteration            | 64         |
| ExpectedImprovement  | 0.021086   |
| ActualImprovement    | 0.020726   |
| ImprovementRatio     | 0.98295    |
| MeanKL               | 0.0067465  |
| Entropy              | 2.9753     |
| Perplexity           | 19.596     |
| AveragePolicyStd     | 0.40096    |
| AveragePolicyStd[0]  | 0.42897    |
| AveragePolicyStd[1]  | 0.49802    |
| AveragePolicyStd[2]  | 0.42073    |
| AveragePolicyStd[3]  | 0.37153    |
| AveragePolicyStd[4]  | 0.35002    |
| AveragePolicyStd[5]  | 0.33649    |
| AverageReturn        | 51.051     |
| MinReturn            | 27.831     |
| MaxReturn            | 82.719     |
| StdReturn            | 10.199     |
| AverageEpisodeLength | 63.08      |
| MinEpisodeLength     | 38         |
| MaxEpisodeLength     | 120        |
| StdEpisodeLength     | 17.167     |
| TotalNEpisodes       | 14546      |
| TotalNSamples        | 3.2510e+05 |
| ExplainedVariance    | 0.87518    |
-------------------------------------
[2018-07-02 16:34:01.324331 UTC] Saving snapshot
[2018-07-02 16:34:01.325355 UTC] Starting iteration 65
[2018-07-02 16:34:01.326034 UTC] Start collecting samples
[2018-07-02 16:34:05.325410 UTC] Computing input variables for policy optimization
[2018-07-02 16:34:05.432648 UTC] Performing policy update
[2018-07-02 16:34:05.433916 UTC] Computing gradient in Euclidean space
[2018-07-02 16:34:05.502298 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:34:06.176915 UTC] Performing line search
[2018-07-02 16:34:06.266900 UTC] Updating baseline
[2018-07-02 16:34:07.162464 UTC] Computing logging information
-------------------------------------
| Iteration            | 65         |
| ExpectedImprovement  | 0.020286   |
| ActualImprovement    | 0.019956   |
| ImprovementRatio     | 0.98377    |
| MeanKL               | 0.0065007  |
| Entropy              | 2.9554     |
| Perplexity           | 19.209     |
| AveragePolicyStd     | 0.39972    |
| AveragePolicyStd[0]  | 0.42752    |
| AveragePolicyStd[1]  | 0.49924    |
| AveragePolicyStd[2]  | 0.41874    |
| AveragePolicyStd[3]  | 0.36741    |
| AveragePolicyStd[4]  | 0.34832    |
| AveragePolicyStd[5]  | 0.33709    |
| AverageReturn        | 53.032     |
| MinReturn            | 24.993     |
| MaxReturn            | 100.31     |
| StdReturn            | 11.336     |
| AverageEpisodeLength | 65.83      |
| MinEpisodeLength     | 40         |
| MaxEpisodeLength     | 162        |
| StdEpisodeLength     | 19.884     |
| TotalNEpisodes       | 14620      |
| TotalNSamples        | 3.2999e+05 |
| ExplainedVariance    | 0.8755     |
-------------------------------------
[2018-07-02 16:34:08.292525 UTC] Saving snapshot
[2018-07-02 16:34:08.293289 UTC] Starting iteration 66
[2018-07-02 16:34:08.294012 UTC] Start collecting samples
[2018-07-02 16:34:12.426811 UTC] Computing input variables for policy optimization
[2018-07-02 16:34:12.524274 UTC] Performing policy update
[2018-07-02 16:34:12.525370 UTC] Computing gradient in Euclidean space
[2018-07-02 16:34:12.600863 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:34:13.413077 UTC] Performing line search
[2018-07-02 16:34:13.524226 UTC] Updating baseline
[2018-07-02 16:34:14.694257 UTC] Computing logging information
-------------------------------------
| Iteration            | 66         |
| ExpectedImprovement  | 0.020805   |
| ActualImprovement    | 0.020138   |
| ImprovementRatio     | 0.96793    |
| MeanKL               | 0.0068609  |
| Entropy              | 2.9189     |
| Perplexity           | 18.522     |
| AveragePolicyStd     | 0.39739    |
| AveragePolicyStd[0]  | 0.42189    |
| AveragePolicyStd[1]  | 0.49918    |
| AveragePolicyStd[2]  | 0.41721    |
| AveragePolicyStd[3]  | 0.36595    |
| AveragePolicyStd[4]  | 0.34367    |
| AveragePolicyStd[5]  | 0.33642    |
| AverageReturn        | 54.11      |
| MinReturn            | 24.993     |
| MaxReturn            | 82.516     |
| StdReturn            | 10.478     |
| AverageEpisodeLength | 68.12      |
| MinEpisodeLength     | 40         |
| MaxEpisodeLength     | 140        |
| StdEpisodeLength     | 17.927     |
| TotalNEpisodes       | 14693      |
| TotalNSamples        | 3.3502e+05 |
| ExplainedVariance    | 0.90494    |
-------------------------------------
[2018-07-02 16:34:15.901941 UTC] Saving snapshot
[2018-07-02 16:34:15.903168 UTC] Starting iteration 67
[2018-07-02 16:34:15.905028 UTC] Start collecting samples
[2018-07-02 16:34:20.466497 UTC] Computing input variables for policy optimization
[2018-07-02 16:34:20.546439 UTC] Performing policy update
[2018-07-02 16:34:20.547392 UTC] Computing gradient in Euclidean space
[2018-07-02 16:34:20.607142 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:34:21.364910 UTC] Performing line search
[2018-07-02 16:34:21.477935 UTC] Updating baseline
[2018-07-02 16:34:22.654871 UTC] Computing logging information
-------------------------------------
| Iteration            | 67         |
| ExpectedImprovement  | 0.020466   |
| ActualImprovement    | 0.019528   |
| ImprovementRatio     | 0.95417    |
| MeanKL               | 0.0066208  |
| Entropy              | 2.8738     |
| Perplexity           | 17.704     |
| AveragePolicyStd     | 0.39451    |
| AveragePolicyStd[0]  | 0.41867    |
| AveragePolicyStd[1]  | 0.49708    |
| AveragePolicyStd[2]  | 0.41432    |
| AveragePolicyStd[3]  | 0.36372    |
| AveragePolicyStd[4]  | 0.34012    |
| AveragePolicyStd[5]  | 0.33314    |
| AverageReturn        | 56.115     |
| MinReturn            | 19.356     |
| MaxReturn            | 98.689     |
| StdReturn            | 12.486     |
| AverageEpisodeLength | 72.41      |
| MinEpisodeLength     | 40         |
| MaxEpisodeLength     | 144        |
| StdEpisodeLength     | 21.727     |
| TotalNEpisodes       | 14757      |
| TotalNSamples        | 3.3977e+05 |
| ExplainedVariance    | 0.88387    |
-------------------------------------
[2018-07-02 16:34:23.646987 UTC] Saving snapshot
[2018-07-02 16:34:23.647701 UTC] Starting iteration 68
[2018-07-02 16:34:23.648657 UTC] Start collecting samples
[2018-07-02 16:34:27.151416 UTC] Computing input variables for policy optimization
[2018-07-02 16:34:27.236312 UTC] Performing policy update
[2018-07-02 16:34:27.237431 UTC] Computing gradient in Euclidean space
[2018-07-02 16:34:27.297562 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:34:27.944785 UTC] Performing line search
[2018-07-02 16:34:28.035473 UTC] Updating baseline
[2018-07-02 16:34:28.927373 UTC] Computing logging information
-------------------------------------
| Iteration            | 68         |
| ExpectedImprovement  | 0.019601   |
| ActualImprovement    | 0.019675   |
| ImprovementRatio     | 1.0038     |
| MeanKL               | 0.0066951  |
| Entropy              | 2.8281     |
| Perplexity           | 16.913     |
| AveragePolicyStd     | 0.39178    |
| AveragePolicyStd[0]  | 0.41727    |
| AveragePolicyStd[1]  | 0.49846    |
| AveragePolicyStd[2]  | 0.41052    |
| AveragePolicyStd[3]  | 0.3559     |
| AveragePolicyStd[4]  | 0.33911    |
| AveragePolicyStd[5]  | 0.32943    |
| AverageReturn        | 57.265     |
| MinReturn            | 19.356     |
| MaxReturn            | 92.915     |
| StdReturn            | 12.055     |
| AverageEpisodeLength | 72.03      |
| MinEpisodeLength     | 40         |
| MaxEpisodeLength     | 159        |
| StdEpisodeLength     | 21.816     |
| TotalNEpisodes       | 14823      |
| TotalNSamples        | 3.4468e+05 |
| ExplainedVariance    | 0.92733    |
-------------------------------------
[2018-07-02 16:34:30.104839 UTC] Saving snapshot
[2018-07-02 16:34:30.105289 UTC] Starting iteration 69
[2018-07-02 16:34:30.105890 UTC] Start collecting samples
[2018-07-02 16:34:33.934929 UTC] Computing input variables for policy optimization
[2018-07-02 16:34:34.024350 UTC] Performing policy update
[2018-07-02 16:34:34.025406 UTC] Computing gradient in Euclidean space
[2018-07-02 16:34:34.085213 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:34:34.740405 UTC] Performing line search
[2018-07-02 16:34:34.834094 UTC] Updating baseline
[2018-07-02 16:34:35.728156 UTC] Computing logging information
-------------------------------------
| Iteration            | 69         |
| ExpectedImprovement  | 0.023342   |
| ActualImprovement    | 0.019728   |
| ImprovementRatio     | 0.84517    |
| MeanKL               | 0.0065549  |
| Entropy              | 2.8343     |
| Perplexity           | 17.019     |
| AveragePolicyStd     | 0.39223    |
| AveragePolicyStd[0]  | 0.41522    |
| AveragePolicyStd[1]  | 0.50136    |
| AveragePolicyStd[2]  | 0.41157    |
| AveragePolicyStd[3]  | 0.35461    |
| AveragePolicyStd[4]  | 0.33829    |
| AveragePolicyStd[5]  | 0.33234    |
| AverageReturn        | 59.99      |
| MinReturn            | 17.672     |
| MaxReturn            | 114.18     |
| StdReturn            | 14.734     |
| AverageEpisodeLength | 76.97      |
| MinEpisodeLength     | 42         |
| MaxEpisodeLength     | 208        |
| StdEpisodeLength     | 28.763     |
| TotalNEpisodes       | 14892      |
| TotalNSamples        | 3.4996e+05 |
| ExplainedVariance    | 0.83126    |
-------------------------------------
[2018-07-02 16:34:36.698711 UTC] Saving snapshot
[2018-07-02 16:34:36.699457 UTC] Starting iteration 70
[2018-07-02 16:34:36.700327 UTC] Start collecting samples
[2018-07-02 16:34:40.317903 UTC] Computing input variables for policy optimization
[2018-07-02 16:34:40.406545 UTC] Performing policy update
[2018-07-02 16:34:40.407560 UTC] Computing gradient in Euclidean space
[2018-07-02 16:34:40.466077 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:34:41.128471 UTC] Performing line search
[2018-07-02 16:34:41.224542 UTC] Updating baseline
[2018-07-02 16:34:42.177842 UTC] Computing logging information
-------------------------------------
| Iteration            | 70         |
| ExpectedImprovement  | 0.023983   |
| ActualImprovement    | 0.022995   |
| ImprovementRatio     | 0.95882    |
| MeanKL               | 0.0064833  |
| Entropy              | 2.8143     |
| Perplexity           | 16.681     |
| AveragePolicyStd     | 0.39099    |
| AveragePolicyStd[0]  | 0.41648    |
| AveragePolicyStd[1]  | 0.49982    |
| AveragePolicyStd[2]  | 0.40912    |
| AveragePolicyStd[3]  | 0.35319    |
| AveragePolicyStd[4]  | 0.33807    |
| AveragePolicyStd[5]  | 0.32926    |
| AverageReturn        | 59.995     |
| MinReturn            | 8.4492     |
| MaxReturn            | 107.5      |
| StdReturn            | 16.036     |
| AverageEpisodeLength | 80.36      |
| MinEpisodeLength     | 39         |
| MaxEpisodeLength     | 167        |
| StdEpisodeLength     | 26.325     |
| TotalNEpisodes       | 14949      |
| TotalNSamples        | 3.5495e+05 |
| ExplainedVariance    | 0.7117     |
-------------------------------------
[2018-07-02 16:34:43.366582 UTC] Saving snapshot
[2018-07-02 16:34:43.378118 UTC] Starting iteration 71
[2018-07-02 16:34:43.378955 UTC] Start collecting samples
[2018-07-02 16:34:47.024124 UTC] Computing input variables for policy optimization
[2018-07-02 16:34:47.102234 UTC] Performing policy update
[2018-07-02 16:34:47.103357 UTC] Computing gradient in Euclidean space
[2018-07-02 16:34:47.162628 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:34:47.844662 UTC] Performing line search
[2018-07-02 16:34:47.934173 UTC] Updating baseline
[2018-07-02 16:34:48.887311 UTC] Computing logging information
------------------------------------
| Iteration            | 71        |
| ExpectedImprovement  | 0.019654  |
| ActualImprovement    | 0.019227  |
| ImprovementRatio     | 0.97831   |
| MeanKL               | 0.0066813 |
| Entropy              | 2.798     |
| Perplexity           | 16.413    |
| AveragePolicyStd     | 0.38978   |
| AveragePolicyStd[0]  | 0.4143    |
| AveragePolicyStd[1]  | 0.4973    |
| AveragePolicyStd[2]  | 0.40558   |
| AveragePolicyStd[3]  | 0.35515   |
| AveragePolicyStd[4]  | 0.33663   |
| AveragePolicyStd[5]  | 0.32974   |
| AverageReturn        | 61.304    |
| MinReturn            | 8.4492    |
| MaxReturn            | 119.45    |
| StdReturn            | 17.254    |
| AverageEpisodeLength | 84.3      |
| MinEpisodeLength     | 39        |
| MaxEpisodeLength     | 202       |
| StdEpisodeLength     | 30.236    |
| TotalNEpisodes       | 15008     |
| TotalNSamples        | 3.598e+05 |
| ExplainedVariance    | 0.84946   |
------------------------------------
[2018-07-02 16:34:50.097712 UTC] Saving snapshot
[2018-07-02 16:34:50.098656 UTC] Starting iteration 72
[2018-07-02 16:34:50.099764 UTC] Start collecting samples
[2018-07-02 16:34:53.726144 UTC] Computing input variables for policy optimization
[2018-07-02 16:34:53.803134 UTC] Performing policy update
[2018-07-02 16:34:53.804213 UTC] Computing gradient in Euclidean space
[2018-07-02 16:34:53.861084 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:34:54.508473 UTC] Performing line search
[2018-07-02 16:34:54.596862 UTC] Updating baseline
[2018-07-02 16:34:55.485223 UTC] Computing logging information
-------------------------------------
| Iteration            | 72         |
| ExpectedImprovement  | 0.020014   |
| ActualImprovement    | 0.019606   |
| ImprovementRatio     | 0.97962    |
| MeanKL               | 0.0066539  |
| Entropy              | 2.7631     |
| Perplexity           | 15.848     |
| AveragePolicyStd     | 0.38774    |
| AveragePolicyStd[0]  | 0.41275    |
| AveragePolicyStd[1]  | 0.49896    |
| AveragePolicyStd[2]  | 0.40195    |
| AveragePolicyStd[3]  | 0.35121    |
| AveragePolicyStd[4]  | 0.33338    |
| AveragePolicyStd[5]  | 0.32819    |
| AverageReturn        | 64.134     |
| MinReturn            | 26.082     |
| MaxReturn            | 120.61     |
| StdReturn            | 16.235     |
| AverageEpisodeLength | 83.04      |
| MinEpisodeLength     | 42         |
| MaxEpisodeLength     | 232        |
| StdEpisodeLength     | 35.196     |
| TotalNEpisodes       | 15068      |
| TotalNSamples        | 3.6478e+05 |
| ExplainedVariance    | 0.90423    |
-------------------------------------
[2018-07-02 16:34:56.682409 UTC] Saving snapshot
[2018-07-02 16:34:56.683003 UTC] Starting iteration 73
[2018-07-02 16:34:56.683695 UTC] Start collecting samples
[2018-07-02 16:35:00.314442 UTC] Computing input variables for policy optimization
[2018-07-02 16:35:00.387048 UTC] Performing policy update
[2018-07-02 16:35:00.388068 UTC] Computing gradient in Euclidean space
[2018-07-02 16:35:00.446895 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:35:01.127025 UTC] Performing line search
[2018-07-02 16:35:01.215263 UTC] Updating baseline
[2018-07-02 16:35:02.064194 UTC] Computing logging information
-------------------------------------
| Iteration            | 73         |
| ExpectedImprovement  | 0.019432   |
| ActualImprovement    | 0.019471   |
| ImprovementRatio     | 1.002      |
| MeanKL               | 0.00678    |
| Entropy              | 2.7533     |
| Perplexity           | 15.694     |
| AveragePolicyStd     | 0.38702    |
| AveragePolicyStd[0]  | 0.41247    |
| AveragePolicyStd[1]  | 0.49719    |
| AveragePolicyStd[2]  | 0.39995    |
| AveragePolicyStd[3]  | 0.34969    |
| AveragePolicyStd[4]  | 0.33453    |
| AveragePolicyStd[5]  | 0.3283     |
| AverageReturn        | 66.049     |
| MinReturn            | 11.881     |
| MaxReturn            | 150.04     |
| StdReturn            | 19.138     |
| AverageEpisodeLength | 91.71      |
| MinEpisodeLength     | 38         |
| MaxEpisodeLength     | 326        |
| StdEpisodeLength     | 45.184     |
| TotalNEpisodes       | 15116      |
| TotalNSamples        | 3.6964e+05 |
| ExplainedVariance    | 0.73949    |
-------------------------------------
[2018-07-02 16:35:03.293863 UTC] Saving snapshot
[2018-07-02 16:35:03.295270 UTC] Starting iteration 74
[2018-07-02 16:35:03.296184 UTC] Start collecting samples
[2018-07-02 16:35:07.099602 UTC] Computing input variables for policy optimization
[2018-07-02 16:35:07.185368 UTC] Performing policy update
[2018-07-02 16:35:07.186374 UTC] Computing gradient in Euclidean space
[2018-07-02 16:35:07.245249 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:35:07.999326 UTC] Performing line search
[2018-07-02 16:35:08.091468 UTC] Updating baseline
[2018-07-02 16:35:09.095088 UTC] Computing logging information
-------------------------------------
| Iteration            | 74         |
| ExpectedImprovement  | 0.019092   |
| ActualImprovement    | 0.0193     |
| ImprovementRatio     | 1.0109     |
| MeanKL               | 0.0065193  |
| Entropy              | 2.7562     |
| Perplexity           | 15.739     |
| AveragePolicyStd     | 0.38717    |
| AveragePolicyStd[0]  | 0.41048    |
| AveragePolicyStd[1]  | 0.49766    |
| AveragePolicyStd[2]  | 0.40067    |
| AveragePolicyStd[3]  | 0.3504     |
| AveragePolicyStd[4]  | 0.33459    |
| AveragePolicyStd[5]  | 0.32921    |
| AverageReturn        | 68.625     |
| MinReturn            | 11.881     |
| MaxReturn            | 150.04     |
| StdReturn            | 19.632     |
| AverageEpisodeLength | 98.36      |
| MinEpisodeLength     | 38         |
| MaxEpisodeLength     | 326        |
| StdEpisodeLength     | 45.1       |
| TotalNEpisodes       | 15171      |
| TotalNSamples        | 3.7486e+05 |
| ExplainedVariance    | 0.86742    |
-------------------------------------
[2018-07-02 16:35:10.211008 UTC] Saving snapshot
[2018-07-02 16:35:10.211478 UTC] Starting iteration 75
[2018-07-02 16:35:10.212082 UTC] Start collecting samples
[2018-07-02 16:35:13.718500 UTC] Computing input variables for policy optimization
[2018-07-02 16:35:13.795301 UTC] Performing policy update
[2018-07-02 16:35:13.796568 UTC] Computing gradient in Euclidean space
[2018-07-02 16:35:13.855996 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:35:14.537111 UTC] Performing line search
[2018-07-02 16:35:14.628561 UTC] Updating baseline
[2018-07-02 16:35:15.562074 UTC] Computing logging information
------------------------------------
| Iteration            | 75        |
| ExpectedImprovement  | 0.020571  |
| ActualImprovement    | 0.020481  |
| ImprovementRatio     | 0.99559   |
| MeanKL               | 0.0065881 |
| Entropy              | 2.7596    |
| Perplexity           | 15.793    |
| AveragePolicyStd     | 0.38741   |
| AveragePolicyStd[0]  | 0.40859   |
| AveragePolicyStd[1]  | 0.49993   |
| AveragePolicyStd[2]  | 0.40017   |
| AveragePolicyStd[3]  | 0.35098   |
| AveragePolicyStd[4]  | 0.33215   |
| AveragePolicyStd[5]  | 0.33265   |
| AverageReturn        | 67.852    |
| MinReturn            | 43.441    |
| MaxReturn            | 126.5     |
| StdReturn            | 13.731    |
| AverageEpisodeLength | 89.51     |
| MinEpisodeLength     | 50        |
| MaxEpisodeLength     | 207       |
| StdEpisodeLength     | 30.077    |
| TotalNEpisodes       | 15229     |
| TotalNSamples        | 3.799e+05 |
| ExplainedVariance    | 0.94608   |
------------------------------------
[2018-07-02 16:35:16.644579 UTC] Saving snapshot
[2018-07-02 16:35:16.645449 UTC] Starting iteration 76
[2018-07-02 16:35:16.645987 UTC] Start collecting samples
[2018-07-02 16:35:19.898598 UTC] Computing input variables for policy optimization
[2018-07-02 16:35:19.967963 UTC] Performing policy update
[2018-07-02 16:35:19.969330 UTC] Computing gradient in Euclidean space
[2018-07-02 16:35:20.030890 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:35:20.725341 UTC] Performing line search
[2018-07-02 16:35:20.821579 UTC] Updating baseline
[2018-07-02 16:35:21.768738 UTC] Computing logging information
-------------------------------------
| Iteration            | 76         |
| ExpectedImprovement  | 0.017745   |
| ActualImprovement    | 0.017252   |
| ImprovementRatio     | 0.97222    |
| MeanKL               | 0.0066814  |
| Entropy              | 2.7509     |
| Perplexity           | 15.657     |
| AveragePolicyStd     | 0.38684    |
| AveragePolicyStd[0]  | 0.40836    |
| AveragePolicyStd[1]  | 0.49939    |
| AveragePolicyStd[2]  | 0.39857    |
| AveragePolicyStd[3]  | 0.34925    |
| AveragePolicyStd[4]  | 0.33055    |
| AveragePolicyStd[5]  | 0.33491    |
| AverageReturn        | 67.36      |
| MinReturn            | 46.811     |
| MaxReturn            | 136.7      |
| StdReturn            | 13.408     |
| AverageEpisodeLength | 89.98      |
| MinEpisodeLength     | 52         |
| MaxEpisodeLength     | 281        |
| StdEpisodeLength     | 36.908     |
| TotalNEpisodes       | 15271      |
| TotalNSamples        | 3.8385e+05 |
| ExplainedVariance    | 0.79648    |
-------------------------------------
[2018-07-02 16:35:22.860541 UTC] Saving snapshot
[2018-07-02 16:35:22.861156 UTC] Starting iteration 77
[2018-07-02 16:35:22.861846 UTC] Start collecting samples
[2018-07-02 16:35:26.250306 UTC] Computing input variables for policy optimization
[2018-07-02 16:35:26.323653 UTC] Performing policy update
[2018-07-02 16:35:26.324820 UTC] Computing gradient in Euclidean space
[2018-07-02 16:35:26.384483 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:35:27.076694 UTC] Performing line search
[2018-07-02 16:35:27.166370 UTC] Updating baseline
[2018-07-02 16:35:28.255270 UTC] Computing logging information
------------------------------------
| Iteration            | 77        |
| ExpectedImprovement  | 0.016556  |
| ActualImprovement    | 0.016251  |
| ImprovementRatio     | 0.98161   |
| MeanKL               | 0.0066968 |
| Entropy              | 2.7187    |
| Perplexity           | 15.16     |
| AveragePolicyStd     | 0.38469   |
| AveragePolicyStd[0]  | 0.40825   |
| AveragePolicyStd[1]  | 0.49427   |
| AveragePolicyStd[2]  | 0.39735   |
| AveragePolicyStd[3]  | 0.34289   |
| AveragePolicyStd[4]  | 0.33065   |
| AveragePolicyStd[5]  | 0.33473   |
| AverageReturn        | 73.116    |
| MinReturn            | 21.838    |
| MaxReturn            | 183.31    |
| StdReturn            | 24.25     |
| AverageEpisodeLength | 105.78    |
| MinEpisodeLength     | 56        |
| MaxEpisodeLength     | 428       |
| StdEpisodeLength     | 62.836    |
| TotalNEpisodes       | 15321     |
| TotalNSamples        | 3.898e+05 |
| ExplainedVariance    | 0.89024   |
------------------------------------
[2018-07-02 16:35:29.324715 UTC] Saving snapshot
[2018-07-02 16:35:29.325397 UTC] Starting iteration 78
[2018-07-02 16:35:29.326025 UTC] Start collecting samples
[2018-07-02 16:35:33.063770 UTC] Computing input variables for policy optimization
[2018-07-02 16:35:33.144504 UTC] Performing policy update
[2018-07-02 16:35:33.145754 UTC] Computing gradient in Euclidean space
[2018-07-02 16:35:33.205590 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:35:33.917939 UTC] Performing line search
[2018-07-02 16:35:34.005271 UTC] Updating baseline
[2018-07-02 16:35:34.947326 UTC] Computing logging information
------------------------------------
| Iteration            | 78        |
| ExpectedImprovement  | 0.017689  |
| ActualImprovement    | 0.017537  |
| ImprovementRatio     | 0.99144   |
| MeanKL               | 0.0066254 |
| Entropy              | 2.7046    |
| Perplexity           | 14.948    |
| AveragePolicyStd     | 0.38397   |
| AveragePolicyStd[0]  | 0.40768   |
| AveragePolicyStd[1]  | 0.49588   |
| AveragePolicyStd[2]  | 0.39742   |
| AveragePolicyStd[3]  | 0.33919   |
| AveragePolicyStd[4]  | 0.32864   |
| AveragePolicyStd[5]  | 0.33501   |
| AverageReturn        | 72.06     |
| MinReturn            | 41.606    |
| MaxReturn            | 154.09    |
| StdReturn            | 21.067    |
| AverageEpisodeLength | 96.31     |
| MinEpisodeLength     | 51        |
| MaxEpisodeLength     | 297       |
| StdEpisodeLength     | 49.61     |
| TotalNEpisodes       | 15380     |
| TotalNSamples        | 3.948e+05 |
| ExplainedVariance    | 0.90141   |
------------------------------------
[2018-07-02 16:35:36.031462 UTC] Saving snapshot
[2018-07-02 16:35:36.032397 UTC] Starting iteration 79
[2018-07-02 16:35:36.033036 UTC] Start collecting samples
[2018-07-02 16:35:39.121763 UTC] Computing input variables for policy optimization
[2018-07-02 16:35:39.190899 UTC] Performing policy update
[2018-07-02 16:35:39.191948 UTC] Computing gradient in Euclidean space
[2018-07-02 16:35:39.251874 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:35:39.917318 UTC] Performing line search
[2018-07-02 16:35:40.004552 UTC] Updating baseline
[2018-07-02 16:35:41.150919 UTC] Computing logging information
-------------------------------------
| Iteration            | 79         |
| ExpectedImprovement  | 0.016975   |
| ActualImprovement    | 0.017326   |
| ImprovementRatio     | 1.0207     |
| MeanKL               | 0.0068417  |
| Entropy              | 2.7012     |
| Perplexity           | 14.898     |
| AveragePolicyStd     | 0.3839     |
| AveragePolicyStd[0]  | 0.40969    |
| AveragePolicyStd[1]  | 0.4977     |
| AveragePolicyStd[2]  | 0.3957     |
| AveragePolicyStd[3]  | 0.33968    |
| AveragePolicyStd[4]  | 0.32693    |
| AveragePolicyStd[5]  | 0.33372    |
| AverageReturn        | 70.036     |
| MinReturn            | 40.826     |
| MaxReturn            | 153.59     |
| StdReturn            | 18.208     |
| AverageEpisodeLength | 93.43      |
| MinEpisodeLength     | 46         |
| MaxEpisodeLength     | 345        |
| StdEpisodeLength     | 48.043     |
| TotalNEpisodes       | 15423      |
| TotalNSamples        | 3.9927e+05 |
| ExplainedVariance    | 0.86343    |
-------------------------------------
[2018-07-02 16:35:42.597787 UTC] Saving snapshot
[2018-07-02 16:35:42.598625 UTC] Starting iteration 80
[2018-07-02 16:35:42.599417 UTC] Start collecting samples
[2018-07-02 16:35:45.980622 UTC] Computing input variables for policy optimization
[2018-07-02 16:35:46.050912 UTC] Performing policy update
[2018-07-02 16:35:46.051901 UTC] Computing gradient in Euclidean space
[2018-07-02 16:35:46.107577 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:35:46.745291 UTC] Performing line search
[2018-07-02 16:35:46.830774 UTC] Updating baseline
[2018-07-02 16:35:47.819094 UTC] Computing logging information
-------------------------------------
| Iteration            | 80         |
| ExpectedImprovement  | 0.019461   |
| ActualImprovement    | 0.018216   |
| ImprovementRatio     | 0.93605    |
| MeanKL               | 0.0066416  |
| Entropy              | 2.6815     |
| Perplexity           | 14.607     |
| AveragePolicyStd     | 0.38271    |
| AveragePolicyStd[0]  | 0.40732    |
| AveragePolicyStd[1]  | 0.49838    |
| AveragePolicyStd[2]  | 0.39324    |
| AveragePolicyStd[3]  | 0.33813    |
| AveragePolicyStd[4]  | 0.32593    |
| AveragePolicyStd[5]  | 0.33324    |
| AverageReturn        | 74.511     |
| MinReturn            | 21.51      |
| MaxReturn            | 171.27     |
| StdReturn            | 23.145     |
| AverageEpisodeLength | 104.92     |
| MinEpisodeLength     | 46         |
| MaxEpisodeLength     | 345        |
| StdEpisodeLength     | 55.941     |
| TotalNEpisodes       | 15470      |
| TotalNSamples        | 4.0449e+05 |
| ExplainedVariance    | 0.89127    |
-------------------------------------
[2018-07-02 16:35:48.787624 UTC] Saving snapshot
[2018-07-02 16:35:48.797551 UTC] Starting iteration 81
[2018-07-02 16:35:48.800150 UTC] Start collecting samples
[2018-07-02 16:35:52.133279 UTC] Computing input variables for policy optimization
[2018-07-02 16:35:52.214877 UTC] Performing policy update
[2018-07-02 16:35:52.216014 UTC] Computing gradient in Euclidean space
[2018-07-02 16:35:52.281867 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:35:52.980512 UTC] Performing line search
[2018-07-02 16:35:53.088572 UTC] Updating baseline
[2018-07-02 16:35:54.239558 UTC] Computing logging information
-------------------------------------
| Iteration            | 81         |
| ExpectedImprovement  | 0.018303   |
| ActualImprovement    | 0.017527   |
| ImprovementRatio     | 0.9576     |
| MeanKL               | 0.0067644  |
| Entropy              | 2.667      |
| Perplexity           | 14.396     |
| AveragePolicyStd     | 0.38197    |
| AveragePolicyStd[0]  | 0.40596    |
| AveragePolicyStd[1]  | 0.50095    |
| AveragePolicyStd[2]  | 0.39157    |
| AveragePolicyStd[3]  | 0.33671    |
| AveragePolicyStd[4]  | 0.32411    |
| AveragePolicyStd[5]  | 0.33249    |
| AverageReturn        | 77.557     |
| MinReturn            | 21.51      |
| MaxReturn            | 209.64     |
| StdReturn            | 28.197     |
| AverageEpisodeLength | 109.47     |
| MinEpisodeLength     | 47         |
| MaxEpisodeLength     | 404        |
| StdEpisodeLength     | 63.638     |
| TotalNEpisodes       | 15518      |
| TotalNSamples        | 4.0956e+05 |
| ExplainedVariance    | 0.86104    |
-------------------------------------
[2018-07-02 16:35:55.360316 UTC] Saving snapshot
[2018-07-02 16:35:55.361130 UTC] Starting iteration 82
[2018-07-02 16:35:55.361900 UTC] Start collecting samples
[2018-07-02 16:35:58.608208 UTC] Computing input variables for policy optimization
[2018-07-02 16:35:58.679633 UTC] Performing policy update
[2018-07-02 16:35:58.680660 UTC] Computing gradient in Euclidean space
[2018-07-02 16:35:58.739306 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:35:59.420482 UTC] Performing line search
[2018-07-02 16:35:59.508902 UTC] Updating baseline
[2018-07-02 16:36:00.577782 UTC] Computing logging information
-------------------------------------
| Iteration            | 82         |
| ExpectedImprovement  | 0.018716   |
| ActualImprovement    | 0.017776   |
| ImprovementRatio     | 0.9498     |
| MeanKL               | 0.0065898  |
| Entropy              | 2.6487     |
| Perplexity           | 14.136     |
| AveragePolicyStd     | 0.38087    |
| AveragePolicyStd[0]  | 0.40561    |
| AveragePolicyStd[1]  | 0.50122    |
| AveragePolicyStd[2]  | 0.38826    |
| AveragePolicyStd[3]  | 0.33499    |
| AveragePolicyStd[4]  | 0.32465    |
| AveragePolicyStd[5]  | 0.33052    |
| AverageReturn        | 76.441     |
| MinReturn            | 36.682     |
| MaxReturn            | 209.64     |
| StdReturn            | 24.608     |
| AverageEpisodeLength | 102.75     |
| MinEpisodeLength     | 48         |
| MaxEpisodeLength     | 404        |
| StdEpisodeLength     | 54.722     |
| TotalNEpisodes       | 15565      |
| TotalNSamples        | 4.1438e+05 |
| ExplainedVariance    | 0.92864    |
-------------------------------------
[2018-07-02 16:36:01.628508 UTC] Saving snapshot
[2018-07-02 16:36:01.629057 UTC] Starting iteration 83
[2018-07-02 16:36:01.629598 UTC] Start collecting samples
[2018-07-02 16:36:05.419570 UTC] Computing input variables for policy optimization
[2018-07-02 16:36:05.498104 UTC] Performing policy update
[2018-07-02 16:36:05.499343 UTC] Computing gradient in Euclidean space
[2018-07-02 16:36:05.557500 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:36:06.291316 UTC] Performing line search
[2018-07-02 16:36:06.383519 UTC] Updating baseline
[2018-07-02 16:36:07.372590 UTC] Computing logging information
-------------------------------------
| Iteration            | 83         |
| ExpectedImprovement  | 0.017667   |
| ActualImprovement    | 0.017632   |
| ImprovementRatio     | 0.99804    |
| MeanKL               | 0.0067794  |
| Entropy              | 2.6343     |
| Perplexity           | 13.933     |
| AveragePolicyStd     | 0.37988    |
| AveragePolicyStd[0]  | 0.40314    |
| AveragePolicyStd[1]  | 0.49976    |
| AveragePolicyStd[2]  | 0.38687    |
| AveragePolicyStd[3]  | 0.33274    |
| AveragePolicyStd[4]  | 0.32578    |
| AveragePolicyStd[5]  | 0.33098    |
| AverageReturn        | 78.465     |
| MinReturn            | 44.795     |
| MaxReturn            | 224.22     |
| StdReturn            | 25.569     |
| AverageEpisodeLength | 106.8      |
| MinEpisodeLength     | 52         |
| MaxEpisodeLength     | 481        |
| StdEpisodeLength     | 60.14      |
| TotalNEpisodes       | 15610      |
| TotalNSamples        | 4.1953e+05 |
| ExplainedVariance    | 0.92866    |
-------------------------------------
[2018-07-02 16:36:08.589985 UTC] Saving snapshot
[2018-07-02 16:36:08.609730 UTC] Starting iteration 84
[2018-07-02 16:36:08.612235 UTC] Start collecting samples
[2018-07-02 16:36:12.411928 UTC] Computing input variables for policy optimization
[2018-07-02 16:36:12.500010 UTC] Performing policy update
[2018-07-02 16:36:12.502765 UTC] Computing gradient in Euclidean space
[2018-07-02 16:36:12.576628 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:36:13.283264 UTC] Performing line search
[2018-07-02 16:36:13.388018 UTC] Updating baseline
[2018-07-02 16:36:14.458557 UTC] Computing logging information
-------------------------------------
| Iteration            | 84         |
| ExpectedImprovement  | 0.019575   |
| ActualImprovement    | 0.019286   |
| ImprovementRatio     | 0.98527    |
| MeanKL               | 0.0066991  |
| Entropy              | 2.6114     |
| Perplexity           | 13.618     |
| AveragePolicyStd     | 0.37826    |
| AveragePolicyStd[0]  | 0.40148    |
| AveragePolicyStd[1]  | 0.49561    |
| AveragePolicyStd[2]  | 0.38376    |
| AveragePolicyStd[3]  | 0.3322     |
| AveragePolicyStd[4]  | 0.32744    |
| AveragePolicyStd[5]  | 0.32909    |
| AverageReturn        | 83.86      |
| MinReturn            | 31.162     |
| MaxReturn            | 224.22     |
| StdReturn            | 26.867     |
| AverageEpisodeLength | 117.76     |
| MinEpisodeLength     | 56         |
| MaxEpisodeLength     | 481        |
| StdEpisodeLength     | 62.113     |
| TotalNEpisodes       | 15651      |
| TotalNSamples        | 4.2461e+05 |
| ExplainedVariance    | 0.88376    |
-------------------------------------
[2018-07-02 16:36:15.688190 UTC] Saving snapshot
[2018-07-02 16:36:15.688895 UTC] Starting iteration 85
[2018-07-02 16:36:15.689520 UTC] Start collecting samples
[2018-07-02 16:36:19.403654 UTC] Computing input variables for policy optimization
[2018-07-02 16:36:19.478807 UTC] Performing policy update
[2018-07-02 16:36:19.480840 UTC] Computing gradient in Euclidean space
[2018-07-02 16:36:19.543056 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:36:20.279076 UTC] Performing line search
[2018-07-02 16:36:20.387768 UTC] Updating baseline
[2018-07-02 16:36:21.342330 UTC] Computing logging information
-------------------------------------
| Iteration            | 85         |
| ExpectedImprovement  | 0.019065   |
| ActualImprovement    | 0.018847   |
| ImprovementRatio     | 0.98858    |
| MeanKL               | 0.0067162  |
| Entropy              | 2.5891     |
| Perplexity           | 13.318     |
| AveragePolicyStd     | 0.3768     |
| AveragePolicyStd[0]  | 0.39865    |
| AveragePolicyStd[1]  | 0.49322    |
| AveragePolicyStd[2]  | 0.38272    |
| AveragePolicyStd[3]  | 0.33057    |
| AveragePolicyStd[4]  | 0.32849    |
| AveragePolicyStd[5]  | 0.32713    |
| AverageReturn        | 85.56      |
| MinReturn            | 31.162     |
| MaxReturn            | 224.22     |
| StdReturn            | 27.1       |
| AverageEpisodeLength | 122.55     |
| MinEpisodeLength     | 56         |
| MaxEpisodeLength     | 481        |
| StdEpisodeLength     | 60.948     |
| TotalNEpisodes       | 15690      |
| TotalNSamples        | 4.2933e+05 |
| ExplainedVariance    | 0.82556    |
-------------------------------------
[2018-07-02 16:36:22.382564 UTC] Saving snapshot
[2018-07-02 16:36:22.384271 UTC] Starting iteration 86
[2018-07-02 16:36:22.385164 UTC] Start collecting samples
[2018-07-02 16:36:25.808583 UTC] Computing input variables for policy optimization
[2018-07-02 16:36:25.877791 UTC] Performing policy update
[2018-07-02 16:36:25.878712 UTC] Computing gradient in Euclidean space
[2018-07-02 16:36:25.941397 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:36:26.644571 UTC] Performing line search
[2018-07-02 16:36:26.731719 UTC] Updating baseline
[2018-07-02 16:36:27.736633 UTC] Computing logging information
-------------------------------------
| Iteration            | 86         |
| ExpectedImprovement  | 0.019323   |
| ActualImprovement    | 0.019091   |
| ImprovementRatio     | 0.98798    |
| MeanKL               | 0.0067246  |
| Entropy              | 2.5791     |
| Perplexity           | 13.185     |
| AveragePolicyStd     | 0.37598    |
| AveragePolicyStd[0]  | 0.39678    |
| AveragePolicyStd[1]  | 0.49004    |
| AveragePolicyStd[2]  | 0.38094    |
| AveragePolicyStd[3]  | 0.33151    |
| AveragePolicyStd[4]  | 0.32999    |
| AveragePolicyStd[5]  | 0.32659    |
| AverageReturn        | 85.209     |
| MinReturn            | 31.162     |
| MaxReturn            | 263.6      |
| StdReturn            | 28.786     |
| AverageEpisodeLength | 122.99     |
| MinEpisodeLength     | 60         |
| MaxEpisodeLength     | 561        |
| StdEpisodeLength     | 64.172     |
| TotalNEpisodes       | 15730      |
| TotalNSamples        | 4.3418e+05 |
| ExplainedVariance    | 0.88827    |
-------------------------------------
[2018-07-02 16:36:28.982579 UTC] Saving snapshot
[2018-07-02 16:36:28.983355 UTC] Starting iteration 87
[2018-07-02 16:36:28.984045 UTC] Start collecting samples
[2018-07-02 16:36:32.345108 UTC] Computing input variables for policy optimization
[2018-07-02 16:36:32.420142 UTC] Performing policy update
[2018-07-02 16:36:32.421111 UTC] Computing gradient in Euclidean space
[2018-07-02 16:36:32.481722 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:36:33.158786 UTC] Performing line search
[2018-07-02 16:36:33.245794 UTC] Updating baseline
[2018-07-02 16:36:34.180438 UTC] Computing logging information
-------------------------------------
| Iteration            | 87         |
| ExpectedImprovement  | 0.01856    |
| ActualImprovement    | 0.018218   |
| ImprovementRatio     | 0.98158    |
| MeanKL               | 0.0069558  |
| Entropy              | 2.5444     |
| Perplexity           | 12.736     |
| AveragePolicyStd     | 0.37369    |
| AveragePolicyStd[0]  | 0.39262    |
| AveragePolicyStd[1]  | 0.48435    |
| AveragePolicyStd[2]  | 0.38234    |
| AveragePolicyStd[3]  | 0.32923    |
| AveragePolicyStd[4]  | 0.32931    |
| AveragePolicyStd[5]  | 0.3243     |
| AverageReturn        | 88.496     |
| MinReturn            | 41.357     |
| MaxReturn            | 272.2      |
| StdReturn            | 32.246     |
| AverageEpisodeLength | 123.93     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 561        |
| StdEpisodeLength     | 72.282     |
| TotalNEpisodes       | 15773      |
| TotalNSamples        | 4.3978e+05 |
| ExplainedVariance    | 0.94807    |
-------------------------------------
[2018-07-02 16:36:35.212987 UTC] Saving snapshot
[2018-07-02 16:36:35.213547 UTC] Starting iteration 88
[2018-07-02 16:36:35.214060 UTC] Start collecting samples
[2018-07-02 16:36:38.533251 UTC] Computing input variables for policy optimization
[2018-07-02 16:36:38.610275 UTC] Performing policy update
[2018-07-02 16:36:38.611599 UTC] Computing gradient in Euclidean space
[2018-07-02 16:36:38.670537 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:36:39.333763 UTC] Performing line search
[2018-07-02 16:36:39.426050 UTC] Updating baseline
[2018-07-02 16:36:40.295887 UTC] Computing logging information
-------------------------------------
| Iteration            | 88         |
| ExpectedImprovement  | 0.019456   |
| ActualImprovement    | 0.019001   |
| ImprovementRatio     | 0.97661    |
| MeanKL               | 0.0069002  |
| Entropy              | 2.5109     |
| Perplexity           | 12.315     |
| AveragePolicyStd     | 0.37151    |
| AveragePolicyStd[0]  | 0.38992    |
| AveragePolicyStd[1]  | 0.4806     |
| AveragePolicyStd[2]  | 0.37898    |
| AveragePolicyStd[3]  | 0.33025    |
| AveragePolicyStd[4]  | 0.32512    |
| AveragePolicyStd[5]  | 0.32416    |
| AverageReturn        | 91.572     |
| MinReturn            | 62.439     |
| MaxReturn            | 272.2      |
| StdReturn            | 28.109     |
| AverageEpisodeLength | 126.25     |
| MinEpisodeLength     | 65         |
| MaxEpisodeLength     | 532        |
| StdEpisodeLength     | 62.541     |
| TotalNEpisodes       | 15813      |
| TotalNSamples        | 4.4477e+05 |
| ExplainedVariance    | 0.96572    |
-------------------------------------
[2018-07-02 16:36:41.302567 UTC] Saving snapshot
[2018-07-02 16:36:41.303116 UTC] Starting iteration 89
[2018-07-02 16:36:41.303765 UTC] Start collecting samples
[2018-07-02 16:36:44.385800 UTC] Computing input variables for policy optimization
[2018-07-02 16:36:44.456708 UTC] Performing policy update
[2018-07-02 16:36:44.457762 UTC] Computing gradient in Euclidean space
[2018-07-02 16:36:44.520304 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:36:45.204015 UTC] Performing line search
[2018-07-02 16:36:45.294677 UTC] Updating baseline
[2018-07-02 16:36:46.198740 UTC] Computing logging information
-------------------------------------
| Iteration            | 89         |
| ExpectedImprovement  | 0.018172   |
| ActualImprovement    | 0.0171     |
| ImprovementRatio     | 0.94098    |
| MeanKL               | 0.0064724  |
| Entropy              | 2.5088     |
| Perplexity           | 12.29      |
| AveragePolicyStd     | 0.37139    |
| AveragePolicyStd[0]  | 0.38921    |
| AveragePolicyStd[1]  | 0.48354    |
| AveragePolicyStd[2]  | 0.37252    |
| AveragePolicyStd[3]  | 0.33094    |
| AveragePolicyStd[4]  | 0.32677    |
| AveragePolicyStd[5]  | 0.32536    |
| AverageReturn        | 95.837     |
| MinReturn            | 59.314     |
| MaxReturn            | 272.2      |
| StdReturn            | 29.222     |
| AverageEpisodeLength | 134.52     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 532        |
| StdEpisodeLength     | 64.799     |
| TotalNEpisodes       | 15848      |
| TotalNSamples        | 4.4958e+05 |
| ExplainedVariance    | 0.90916    |
-------------------------------------
[2018-07-02 16:36:47.495183 UTC] Saving snapshot
[2018-07-02 16:36:47.495913 UTC] Starting iteration 90
[2018-07-02 16:36:47.496837 UTC] Start collecting samples
[2018-07-02 16:36:50.636583 UTC] Computing input variables for policy optimization
[2018-07-02 16:36:50.700407 UTC] Performing policy update
[2018-07-02 16:36:50.701509 UTC] Computing gradient in Euclidean space
[2018-07-02 16:36:50.757179 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:36:51.439449 UTC] Performing line search
[2018-07-02 16:36:51.530605 UTC] Updating baseline
[2018-07-02 16:36:52.515663 UTC] Computing logging information
------------------------------------
| Iteration            | 90        |
| ExpectedImprovement  | 0.018373  |
| ActualImprovement    | 0.017842  |
| ImprovementRatio     | 0.97109   |
| MeanKL               | 0.0065598 |
| Entropy              | 2.4984    |
| Perplexity           | 12.163    |
| AveragePolicyStd     | 0.37052   |
| AveragePolicyStd[0]  | 0.38744   |
| AveragePolicyStd[1]  | 0.47833   |
| AveragePolicyStd[2]  | 0.37348   |
| AveragePolicyStd[3]  | 0.33124   |
| AveragePolicyStd[4]  | 0.3252    |
| AveragePolicyStd[5]  | 0.32745   |
| AverageReturn        | 97.753    |
| MinReturn            | 58.825    |
| MaxReturn            | 191.94    |
| StdReturn            | 27.812    |
| AverageEpisodeLength | 138.16    |
| MinEpisodeLength     | 62        |
| MaxEpisodeLength     | 347       |
| StdEpisodeLength     | 61.18     |
| TotalNEpisodes       | 15880     |
| TotalNSamples        | 4.543e+05 |
| ExplainedVariance    | 0.8593    |
------------------------------------
[2018-07-02 16:36:53.553006 UTC] Saving snapshot
[2018-07-02 16:36:53.597000 UTC] Starting iteration 91
[2018-07-02 16:36:53.597861 UTC] Start collecting samples
[2018-07-02 16:36:56.708106 UTC] Computing input variables for policy optimization
[2018-07-02 16:36:56.774396 UTC] Performing policy update
[2018-07-02 16:36:56.775601 UTC] Computing gradient in Euclidean space
[2018-07-02 16:36:56.834194 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:36:57.527988 UTC] Performing line search
[2018-07-02 16:36:57.618485 UTC] Updating baseline
[2018-07-02 16:36:58.532395 UTC] Computing logging information
-------------------------------------
| Iteration            | 91         |
| ExpectedImprovement  | 0.02166    |
| ActualImprovement    | 0.021455   |
| ImprovementRatio     | 0.99054    |
| MeanKL               | 0.0067402  |
| Entropy              | 2.4753     |
| Perplexity           | 11.885     |
| AveragePolicyStd     | 0.3692     |
| AveragePolicyStd[0]  | 0.38625    |
| AveragePolicyStd[1]  | 0.47892    |
| AveragePolicyStd[2]  | 0.37038    |
| AveragePolicyStd[3]  | 0.33062    |
| AveragePolicyStd[4]  | 0.32523    |
| AveragePolicyStd[5]  | 0.3238     |
| AverageReturn        | 101.02     |
| MinReturn            | 36.107     |
| MaxReturn            | 191.94     |
| StdReturn            | 30.246     |
| AverageEpisodeLength | 144.92     |
| MinEpisodeLength     | 62         |
| MaxEpisodeLength     | 347        |
| StdEpisodeLength     | 62.146     |
| TotalNEpisodes       | 15915      |
| TotalNSamples        | 4.5945e+05 |
| ExplainedVariance    | 0.88305    |
-------------------------------------
[2018-07-02 16:36:59.613598 UTC] Saving snapshot
[2018-07-02 16:36:59.614565 UTC] Starting iteration 92
[2018-07-02 16:36:59.615115 UTC] Start collecting samples
[2018-07-02 16:37:02.776000 UTC] Computing input variables for policy optimization
[2018-07-02 16:37:02.842348 UTC] Performing policy update
[2018-07-02 16:37:02.843380 UTC] Computing gradient in Euclidean space
[2018-07-02 16:37:02.901735 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:37:03.575253 UTC] Performing line search
[2018-07-02 16:37:03.662913 UTC] Updating baseline
[2018-07-02 16:37:04.672143 UTC] Computing logging information
-------------------------------------
| Iteration            | 92         |
| ExpectedImprovement  | 0.020657   |
| ActualImprovement    | 0.020076   |
| ImprovementRatio     | 0.97185    |
| MeanKL               | 0.0065264  |
| Entropy              | 2.4779     |
| Perplexity           | 11.916     |
| AveragePolicyStd     | 0.3694     |
| AveragePolicyStd[0]  | 0.3872     |
| AveragePolicyStd[1]  | 0.47885    |
| AveragePolicyStd[2]  | 0.3723     |
| AveragePolicyStd[3]  | 0.33025    |
| AveragePolicyStd[4]  | 0.32427    |
| AveragePolicyStd[5]  | 0.32354    |
| AverageReturn        | 104.61     |
| MinReturn            | 36.107     |
| MaxReturn            | 213.57     |
| StdReturn            | 32.557     |
| AverageEpisodeLength | 151.56     |
| MinEpisodeLength     | 64         |
| MaxEpisodeLength     | 378        |
| StdEpisodeLength     | 67.067     |
| TotalNEpisodes       | 15946      |
| TotalNSamples        | 4.6442e+05 |
| ExplainedVariance    | 0.92724    |
-------------------------------------
[2018-07-02 16:37:05.742111 UTC] Saving snapshot
[2018-07-02 16:37:05.742852 UTC] Starting iteration 93
[2018-07-02 16:37:05.743635 UTC] Start collecting samples
[2018-07-02 16:37:08.686072 UTC] Computing input variables for policy optimization
[2018-07-02 16:37:08.752692 UTC] Performing policy update
[2018-07-02 16:37:08.753622 UTC] Computing gradient in Euclidean space
[2018-07-02 16:37:08.812526 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:37:09.553179 UTC] Performing line search
[2018-07-02 16:37:09.654462 UTC] Updating baseline
[2018-07-02 16:37:10.779967 UTC] Computing logging information
-------------------------------------
| Iteration            | 93         |
| ExpectedImprovement  | 0.022944   |
| ActualImprovement    | 0.020635   |
| ImprovementRatio     | 0.89934    |
| MeanKL               | 0.0065757  |
| Entropy              | 2.4876     |
| Perplexity           | 12.033     |
| AveragePolicyStd     | 0.36992    |
| AveragePolicyStd[0]  | 0.38668    |
| AveragePolicyStd[1]  | 0.47897    |
| AveragePolicyStd[2]  | 0.37205    |
| AveragePolicyStd[3]  | 0.33254    |
| AveragePolicyStd[4]  | 0.32455    |
| AveragePolicyStd[5]  | 0.32475    |
| AverageReturn        | 103.94     |
| MinReturn            | 31.507     |
| MaxReturn            | 213.57     |
| StdReturn            | 32.322     |
| AverageEpisodeLength | 147.19     |
| MinEpisodeLength     | 64         |
| MaxEpisodeLength     | 378        |
| StdEpisodeLength     | 63.126     |
| TotalNEpisodes       | 15983      |
| TotalNSamples        | 4.6949e+05 |
| ExplainedVariance    | 0.94871    |
-------------------------------------
[2018-07-02 16:37:11.802097 UTC] Saving snapshot
[2018-07-02 16:37:11.802708 UTC] Starting iteration 94
[2018-07-02 16:37:11.803405 UTC] Start collecting samples
[2018-07-02 16:37:14.977468 UTC] Computing input variables for policy optimization
[2018-07-02 16:37:15.051451 UTC] Performing policy update
[2018-07-02 16:37:15.052668 UTC] Computing gradient in Euclidean space
[2018-07-02 16:37:15.110457 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:37:15.797135 UTC] Performing line search
[2018-07-02 16:37:15.891527 UTC] Updating baseline
[2018-07-02 16:37:16.754004 UTC] Computing logging information
-------------------------------------
| Iteration            | 94         |
| ExpectedImprovement  | 0.023344   |
| ActualImprovement    | 0.020876   |
| ImprovementRatio     | 0.89427    |
| MeanKL               | 0.0066731  |
| Entropy              | 2.4631     |
| Perplexity           | 11.741     |
| AveragePolicyStd     | 0.36833    |
| AveragePolicyStd[0]  | 0.38265    |
| AveragePolicyStd[1]  | 0.47735    |
| AveragePolicyStd[2]  | 0.36896    |
| AveragePolicyStd[3]  | 0.33079    |
| AveragePolicyStd[4]  | 0.325      |
| AveragePolicyStd[5]  | 0.32526    |
| AverageReturn        | 99.677     |
| MinReturn            | 11.528     |
| MaxReturn            | 213.57     |
| StdReturn            | 31.388     |
| AverageEpisodeLength | 137.51     |
| MinEpisodeLength     | 28         |
| MaxEpisodeLength     | 378        |
| StdEpisodeLength     | 60.25      |
| TotalNEpisodes       | 16024      |
| TotalNSamples        | 4.7448e+05 |
| ExplainedVariance    | 0.92194    |
-------------------------------------
[2018-07-02 16:37:17.857484 UTC] Saving snapshot
[2018-07-02 16:37:17.858274 UTC] Starting iteration 95
[2018-07-02 16:37:17.859099 UTC] Start collecting samples
[2018-07-02 16:37:20.950565 UTC] Computing input variables for policy optimization
[2018-07-02 16:37:21.015144 UTC] Performing policy update
[2018-07-02 16:37:21.016185 UTC] Computing gradient in Euclidean space
[2018-07-02 16:37:21.073524 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:37:21.774370 UTC] Performing line search
[2018-07-02 16:37:21.863610 UTC] Updating baseline
[2018-07-02 16:37:22.927275 UTC] Computing logging information
-------------------------------------
| Iteration            | 95         |
| ExpectedImprovement  | 0.020204   |
| ActualImprovement    | 0.019919   |
| ImprovementRatio     | 0.98594    |
| MeanKL               | 0.0067079  |
| Entropy              | 2.4243     |
| Perplexity           | 11.295     |
| AveragePolicyStd     | 0.36597    |
| AveragePolicyStd[0]  | 0.37884    |
| AveragePolicyStd[1]  | 0.47492    |
| AveragePolicyStd[2]  | 0.36699    |
| AveragePolicyStd[3]  | 0.32831    |
| AveragePolicyStd[4]  | 0.32379    |
| AveragePolicyStd[5]  | 0.32299    |
| AverageReturn        | 97.665     |
| MinReturn            | 11.528     |
| MaxReturn            | 209.87     |
| StdReturn            | 31.323     |
| AverageEpisodeLength | 132.28     |
| MinEpisodeLength     | 28         |
| MaxEpisodeLength     | 349        |
| StdEpisodeLength     | 57.973     |
| TotalNEpisodes       | 16054      |
| TotalNSamples        | 4.7868e+05 |
| ExplainedVariance    | 0.88039    |
-------------------------------------
[2018-07-02 16:37:23.964149 UTC] Saving snapshot
[2018-07-02 16:37:23.964860 UTC] Starting iteration 96
[2018-07-02 16:37:23.965366 UTC] Start collecting samples
[2018-07-02 16:37:27.126863 UTC] Computing input variables for policy optimization
[2018-07-02 16:37:27.193792 UTC] Performing policy update
[2018-07-02 16:37:27.194886 UTC] Computing gradient in Euclidean space
[2018-07-02 16:37:27.252969 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:37:27.973438 UTC] Performing line search
[2018-07-02 16:37:28.063956 UTC] Updating baseline
[2018-07-02 16:37:29.059820 UTC] Computing logging information
-------------------------------------
| Iteration            | 96         |
| ExpectedImprovement  | 0.01934    |
| ActualImprovement    | 0.019472   |
| ImprovementRatio     | 1.0068     |
| MeanKL               | 0.0069275  |
| Entropy              | 2.4008     |
| Perplexity           | 11.032     |
| AveragePolicyStd     | 0.36449    |
| AveragePolicyStd[0]  | 0.37531    |
| AveragePolicyStd[1]  | 0.47289    |
| AveragePolicyStd[2]  | 0.36625    |
| AveragePolicyStd[3]  | 0.32674    |
| AveragePolicyStd[4]  | 0.32142    |
| AveragePolicyStd[5]  | 0.32436    |
| AverageReturn        | 103.69     |
| MinReturn            | 11.528     |
| MaxReturn            | 260.04     |
| StdReturn            | 38.039     |
| AverageEpisodeLength | 143.37     |
| MinEpisodeLength     | 28         |
| MaxEpisodeLength     | 460        |
| StdEpisodeLength     | 73.639     |
| TotalNEpisodes       | 16092      |
| TotalNSamples        | 4.8492e+05 |
| ExplainedVariance    | 0.96107    |
-------------------------------------
[2018-07-02 16:37:30.145683 UTC] Saving snapshot
[2018-07-02 16:37:30.146413 UTC] Starting iteration 97
[2018-07-02 16:37:30.147103 UTC] Start collecting samples
[2018-07-02 16:37:33.330571 UTC] Computing input variables for policy optimization
[2018-07-02 16:37:33.403186 UTC] Performing policy update
[2018-07-02 16:37:33.404352 UTC] Computing gradient in Euclidean space
[2018-07-02 16:37:33.468900 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:37:34.141182 UTC] Performing line search
[2018-07-02 16:37:34.233549 UTC] Updating baseline
[2018-07-02 16:37:35.090964 UTC] Computing logging information
-------------------------------------
| Iteration            | 97         |
| ExpectedImprovement  | 0.020863   |
| ActualImprovement    | 0.020819   |
| ImprovementRatio     | 0.99789    |
| MeanKL               | 0.0065489  |
| Entropy              | 2.3656     |
| Perplexity           | 10.65      |
| AveragePolicyStd     | 0.36245    |
| AveragePolicyStd[0]  | 0.37303    |
| AveragePolicyStd[1]  | 0.47246    |
| AveragePolicyStd[2]  | 0.36171    |
| AveragePolicyStd[3]  | 0.32709    |
| AveragePolicyStd[4]  | 0.31865    |
| AveragePolicyStd[5]  | 0.32173    |
| AverageReturn        | 105.48     |
| MinReturn            | 24.292     |
| MaxReturn            | 260.04     |
| StdReturn            | 36.459     |
| AverageEpisodeLength | 144.89     |
| MinEpisodeLength     | 43         |
| MaxEpisodeLength     | 460        |
| StdEpisodeLength     | 72.948     |
| TotalNEpisodes       | 16129      |
| TotalNSamples        | 4.8952e+05 |
| ExplainedVariance    | 0.97536    |
-------------------------------------
[2018-07-02 16:37:36.197985 UTC] Saving snapshot
[2018-07-02 16:37:36.198837 UTC] Starting iteration 98
[2018-07-02 16:37:36.199666 UTC] Start collecting samples
[2018-07-02 16:37:39.134622 UTC] Computing input variables for policy optimization
[2018-07-02 16:37:39.230580 UTC] Performing policy update
[2018-07-02 16:37:39.231677 UTC] Computing gradient in Euclidean space
[2018-07-02 16:37:39.295497 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:37:40.015272 UTC] Performing line search
[2018-07-02 16:37:40.119216 UTC] Updating baseline
[2018-07-02 16:37:41.091214 UTC] Computing logging information
-------------------------------------
| Iteration            | 98         |
| ExpectedImprovement  | 0.019937   |
| ActualImprovement    | 0.019391   |
| ImprovementRatio     | 0.97266    |
| MeanKL               | 0.0067451  |
| Entropy              | 2.3428     |
| Perplexity           | 10.41      |
| AveragePolicyStd     | 0.3609     |
| AveragePolicyStd[0]  | 0.37157    |
| AveragePolicyStd[1]  | 0.46734    |
| AveragePolicyStd[2]  | 0.36098    |
| AveragePolicyStd[3]  | 0.32515    |
| AveragePolicyStd[4]  | 0.3172     |
| AveragePolicyStd[5]  | 0.32319    |
| AverageReturn        | 108.54     |
| MinReturn            | 68.087     |
| MaxReturn            | 244.32     |
| StdReturn            | 31.238     |
| AverageEpisodeLength | 149.1      |
| MinEpisodeLength     | 73         |
| MaxEpisodeLength     | 455        |
| StdEpisodeLength     | 62.624     |
| TotalNEpisodes       | 16159      |
| TotalNSamples        | 4.9441e+05 |
| ExplainedVariance    | 0.89392    |
-------------------------------------
[2018-07-02 16:37:42.187605 UTC] Saving snapshot
[2018-07-02 16:37:42.188489 UTC] Starting iteration 99
[2018-07-02 16:37:42.189335 UTC] Start collecting samples
[2018-07-02 16:37:47.037603 UTC] Computing input variables for policy optimization
[2018-07-02 16:37:47.285555 UTC] Performing policy update
[2018-07-02 16:37:47.288479 UTC] Computing gradient in Euclidean space
[2018-07-02 16:37:47.511048 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:37:49.212167 UTC] Performing line search
[2018-07-02 16:37:49.400956 UTC] Updating baseline
[2018-07-02 16:37:51.619430 UTC] Computing logging information
-------------------------------------
| Iteration            | 99         |
| ExpectedImprovement  | 0.019092   |
| ActualImprovement    | 0.0183     |
| ImprovementRatio     | 0.95853    |
| MeanKL               | 0.0067665  |
| Entropy              | 2.3169     |
| Perplexity           | 10.144     |
| AveragePolicyStd     | 0.35935    |
| AveragePolicyStd[0]  | 0.36778    |
| AveragePolicyStd[1]  | 0.46568    |
| AveragePolicyStd[2]  | 0.36036    |
| AveragePolicyStd[3]  | 0.32567    |
| AveragePolicyStd[4]  | 0.31492    |
| AveragePolicyStd[5]  | 0.32165    |
| AverageReturn        | 107.05     |
| MinReturn            | 41.639     |
| MaxReturn            | 195.03     |
| StdReturn            | 26.808     |
| AverageEpisodeLength | 146        |
| MinEpisodeLength     | 52         |
| MaxEpisodeLength     | 316        |
| StdEpisodeLength     | 52.247     |
| TotalNEpisodes       | 16188      |
| TotalNSamples        | 4.9894e+05 |
| ExplainedVariance    | 0.92868    |
-------------------------------------
[2018-07-02 16:37:52.951760 UTC] Saving snapshot
[2018-07-02 16:37:52.952367 UTC] Starting iteration 100
[2018-07-02 16:37:52.952905 UTC] Start collecting samples
[2018-07-02 16:37:59.378597 UTC] Computing input variables for policy optimization
[2018-07-02 16:37:59.557878 UTC] Performing policy update
[2018-07-02 16:37:59.559127 UTC] Computing gradient in Euclidean space
[2018-07-02 16:37:59.689419 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:38:01.293675 UTC] Performing line search
[2018-07-02 16:38:01.483640 UTC] Updating baseline
[2018-07-02 16:38:03.117930 UTC] Computing logging information
-------------------------------------
| Iteration            | 100        |
| ExpectedImprovement  | 0.019125   |
| ActualImprovement    | 0.017419   |
| ImprovementRatio     | 0.91078    |
| MeanKL               | 0.0068458  |
| Entropy              | 2.2897     |
| Perplexity           | 9.8724     |
| AveragePolicyStd     | 0.3577     |
| AveragePolicyStd[0]  | 0.36669    |
| AveragePolicyStd[1]  | 0.46232    |
| AveragePolicyStd[2]  | 0.3603     |
| AveragePolicyStd[3]  | 0.32545    |
| AveragePolicyStd[4]  | 0.31384    |
| AveragePolicyStd[5]  | 0.31762    |
| AverageReturn        | 115.35     |
| MinReturn            | 41.639     |
| MaxReturn            | 247.48     |
| StdReturn            | 32.641     |
| AverageEpisodeLength | 163.16     |
| MinEpisodeLength     | 52         |
| MaxEpisodeLength     | 418        |
| StdEpisodeLength     | 60.885     |
| TotalNEpisodes       | 16215      |
| TotalNSamples        | 5.0395e+05 |
| ExplainedVariance    | 0.8545     |
-------------------------------------
[2018-07-02 16:38:04.605019 UTC] Saving snapshot
[2018-07-02 16:38:04.616341 UTC] Starting iteration 101
[2018-07-02 16:38:04.617311 UTC] Start collecting samples
[2018-07-02 16:38:11.699531 UTC] Computing input variables for policy optimization
[2018-07-02 16:38:11.887179 UTC] Performing policy update
[2018-07-02 16:38:11.889616 UTC] Computing gradient in Euclidean space
[2018-07-02 16:38:12.029896 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:38:13.431108 UTC] Performing line search
[2018-07-02 16:38:13.573042 UTC] Updating baseline
