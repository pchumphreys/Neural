[2018-07-02 16:14:57.776884 UTC] Starting env pool
[2018-07-02 16:14:57.837660 UTC] Starting iteration 0
[2018-07-02 16:14:57.838703 UTC] Start collecting samples
[2018-07-02 16:14:59.486078 UTC] Computing input variables for policy optimization
[2018-07-02 16:14:59.559015 UTC] Performing policy update
[2018-07-02 16:14:59.560556 UTC] Computing gradient in Euclidean space
[2018-07-02 16:14:59.624530 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:00.151895 UTC] Performing line search
[2018-07-02 16:15:00.194702 UTC] Updating baseline
[2018-07-02 16:15:00.983583 UTC] Computing logging information
-------------------------------------
| Iteration            | 0          |
| ExpectedImprovement  | 0.0060282  |
| ActualImprovement    | 0.0047428  |
| ImprovementRatio     | 0.78677    |
| MeanKL               | 0.0083737  |
| Entropy              | 1.4189     |
| Perplexity           | 4.1327     |
| AveragePolicyStd     | 1          |
| AveragePolicyStd[0]  | 1          |
| AverageReturn        | -1125.4    |
| MinReturn            | -1816      |
| MaxReturn            | -843.09    |
| StdReturn            | 189.13     |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 48         |
| TotalNSamples        | 9600       |
| ExplainedVariance    | -0.0010832 |
-------------------------------------
[2018-07-02 16:15:01.864667 UTC] Saving snapshot
[2018-07-02 16:15:01.873748 UTC] Starting iteration 1
[2018-07-02 16:15:01.874615 UTC] Start collecting samples
[2018-07-02 16:15:03.555180 UTC] Computing input variables for policy optimization
[2018-07-02 16:15:03.630914 UTC] Performing policy update
[2018-07-02 16:15:03.632154 UTC] Computing gradient in Euclidean space
[2018-07-02 16:15:03.678951 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:04.218422 UTC] Performing line search
[2018-07-02 16:15:04.261986 UTC] Updating baseline
[2018-07-02 16:15:04.939984 UTC] Computing logging information
------------------------------------
| Iteration            | 1         |
| ExpectedImprovement  | 0.0052478 |
| ActualImprovement    | 0.0071353 |
| ImprovementRatio     | 1.3597    |
| MeanKL               | 0.0094971 |
| Entropy              | 1.4202    |
| Perplexity           | 4.138     |
| AveragePolicyStd     | 1.0013    |
| AveragePolicyStd[0]  | 1.0013    |
| AverageReturn        | -1157.2   |
| MinReturn            | -1816     |
| MaxReturn            | -843.09   |
| StdReturn            | 187.69    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 96        |
| TotalNSamples        | 19200     |
| ExplainedVariance    | 0.097693  |
------------------------------------
[2018-07-02 16:15:05.951592 UTC] Saving snapshot
[2018-07-02 16:15:05.959996 UTC] Starting iteration 2
[2018-07-02 16:15:05.960689 UTC] Start collecting samples
[2018-07-02 16:15:07.601883 UTC] Computing input variables for policy optimization
[2018-07-02 16:15:07.692340 UTC] Performing policy update
[2018-07-02 16:15:07.694540 UTC] Computing gradient in Euclidean space
[2018-07-02 16:15:07.747704 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:08.310143 UTC] Performing line search
[2018-07-02 16:15:08.356079 UTC] Updating baseline
[2018-07-02 16:15:08.985385 UTC] Computing logging information
------------------------------------
| Iteration            | 2         |
| ExpectedImprovement  | 0.0052886 |
| ActualImprovement    | 0.0049214 |
| ImprovementRatio     | 0.93057   |
| MeanKL               | 0.009796  |
| Entropy              | 1.4727    |
| Perplexity           | 4.361     |
| AveragePolicyStd     | 1.0552    |
| AveragePolicyStd[0]  | 1.0552    |
| AverageReturn        | -1157.9   |
| MinReturn            | -1816     |
| MaxReturn            | -845.11   |
| StdReturn            | 184.66    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 144       |
| TotalNSamples        | 28800     |
| ExplainedVariance    | 0.19585   |
------------------------------------
[2018-07-02 16:15:09.817031 UTC] Saving snapshot
[2018-07-02 16:15:09.824958 UTC] Starting iteration 3
[2018-07-02 16:15:09.825554 UTC] Start collecting samples
[2018-07-02 16:15:11.406254 UTC] Computing input variables for policy optimization
[2018-07-02 16:15:11.491178 UTC] Performing policy update
[2018-07-02 16:15:11.492417 UTC] Computing gradient in Euclidean space
[2018-07-02 16:15:11.542166 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:12.101741 UTC] Performing line search
[2018-07-02 16:15:12.183351 UTC] Updating baseline
[2018-07-02 16:15:13.024774 UTC] Computing logging information
------------------------------------
| Iteration            | 3         |
| ExpectedImprovement  | 0.0036888 |
| ActualImprovement    | 0.0036963 |
| ImprovementRatio     | 1.002     |
| MeanKL               | 0.008474  |
| Entropy              | 1.5496    |
| Perplexity           | 4.7096    |
| AveragePolicyStd     | 1.1396    |
| AveragePolicyStd[0]  | 1.1396    |
| AverageReturn        | -1123.3   |
| MinReturn            | -1619     |
| MaxReturn            | -845.11   |
| StdReturn            | 161.97    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 192       |
| TotalNSamples        | 38400     |
| ExplainedVariance    | 0.2728    |
------------------------------------
[2018-07-02 16:15:14.292369 UTC] Saving snapshot
[2018-07-02 16:15:14.303778 UTC] Starting iteration 4
[2018-07-02 16:15:14.304628 UTC] Start collecting samples
[2018-07-02 16:15:16.270146 UTC] Computing input variables for policy optimization
[2018-07-02 16:15:16.347049 UTC] Performing policy update
[2018-07-02 16:15:16.348154 UTC] Computing gradient in Euclidean space
[2018-07-02 16:15:16.396957 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:17.029240 UTC] Performing line search
[2018-07-02 16:15:17.125099 UTC] Updating baseline
[2018-07-02 16:15:17.788174 UTC] Computing logging information
------------------------------------
| Iteration            | 4         |
| ExpectedImprovement  | 0.0044796 |
| ActualImprovement    | 0.0037352 |
| ImprovementRatio     | 0.83383   |
| MeanKL               | 0.0065705 |
| Entropy              | 1.4953    |
| Perplexity           | 4.4608    |
| AveragePolicyStd     | 1.0794    |
| AveragePolicyStd[0]  | 1.0794    |
| AverageReturn        | -1079.6   |
| MinReturn            | -1580     |
| MaxReturn            | -747.17   |
| StdReturn            | 147.61    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 240       |
| TotalNSamples        | 48000     |
| ExplainedVariance    | 0.26664   |
------------------------------------
[2018-07-02 16:15:18.877678 UTC] Saving snapshot
[2018-07-02 16:15:18.887830 UTC] Starting iteration 5
[2018-07-02 16:15:18.888671 UTC] Start collecting samples
[2018-07-02 16:15:20.456474 UTC] Computing input variables for policy optimization
[2018-07-02 16:15:20.529584 UTC] Performing policy update
[2018-07-02 16:15:20.530888 UTC] Computing gradient in Euclidean space
[2018-07-02 16:15:20.580486 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:21.116847 UTC] Performing line search
[2018-07-02 16:15:21.196291 UTC] Updating baseline
[2018-07-02 16:15:21.870589 UTC] Computing logging information
------------------------------------
| Iteration            | 5         |
| ExpectedImprovement  | 0.0056684 |
| ActualImprovement    | 0.0051601 |
| ImprovementRatio     | 0.91033   |
| MeanKL               | 0.0071157 |
| Entropy              | 1.4917    |
| Perplexity           | 4.4446    |
| AveragePolicyStd     | 1.0755    |
| AveragePolicyStd[0]  | 1.0755    |
| AverageReturn        | -1052.4   |
| MinReturn            | -1473.4   |
| MaxReturn            | -747.17   |
| StdReturn            | 137.25    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 288       |
| TotalNSamples        | 57600     |
| ExplainedVariance    | 0.41139   |
------------------------------------
[2018-07-02 16:15:22.592714 UTC] Saving snapshot
[2018-07-02 16:15:22.601225 UTC] Starting iteration 6
[2018-07-02 16:15:22.602390 UTC] Start collecting samples
[2018-07-02 16:15:24.148078 UTC] Computing input variables for policy optimization
[2018-07-02 16:15:24.238266 UTC] Performing policy update
[2018-07-02 16:15:24.239458 UTC] Computing gradient in Euclidean space
[2018-07-02 16:15:24.292604 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:24.975100 UTC] Performing line search
[2018-07-02 16:15:25.016860 UTC] Updating baseline
[2018-07-02 16:15:25.690882 UTC] Computing logging information
------------------------------------
| Iteration            | 6         |
| ExpectedImprovement  | 0.0080416 |
| ActualImprovement    | 0.0062871 |
| ImprovementRatio     | 0.78182   |
| MeanKL               | 0.0085671 |
| Entropy              | 1.4797    |
| Perplexity           | 4.3915    |
| AveragePolicyStd     | 1.0626    |
| AveragePolicyStd[0]  | 1.0626    |
| AverageReturn        | -1056.4   |
| MinReturn            | -1473.4   |
| MaxReturn            | -872.36   |
| StdReturn            | 129.69    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 336       |
| TotalNSamples        | 67200     |
| ExplainedVariance    | 0.49669   |
------------------------------------
[2018-07-02 16:15:26.530469 UTC] Saving snapshot
[2018-07-02 16:15:26.541217 UTC] Starting iteration 7
[2018-07-02 16:15:26.541959 UTC] Start collecting samples
[2018-07-02 16:15:28.227107 UTC] Computing input variables for policy optimization
[2018-07-02 16:15:28.344381 UTC] Performing policy update
[2018-07-02 16:15:28.345626 UTC] Computing gradient in Euclidean space
[2018-07-02 16:15:28.398914 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:28.971714 UTC] Performing line search
[2018-07-02 16:15:29.064507 UTC] Updating baseline
[2018-07-02 16:15:29.780306 UTC] Computing logging information
------------------------------------
| Iteration            | 7         |
| ExpectedImprovement  | 0.005769  |
| ActualImprovement    | 0.0058095 |
| ImprovementRatio     | 1.007     |
| MeanKL               | 0.0067757 |
| Entropy              | 1.4804    |
| Perplexity           | 4.3945    |
| AveragePolicyStd     | 1.0633    |
| AveragePolicyStd[0]  | 1.0633    |
| AverageReturn        | -1022.7   |
| MinReturn            | -1372.3   |
| MaxReturn            | -752.66   |
| StdReturn            | 125.75    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 400       |
| TotalNSamples        | 80000     |
| ExplainedVariance    | 0.6254    |
------------------------------------
[2018-07-02 16:15:30.623635 UTC] Saving snapshot
[2018-07-02 16:15:30.631279 UTC] Starting iteration 8
[2018-07-02 16:15:30.632263 UTC] Start collecting samples
[2018-07-02 16:15:32.304050 UTC] Computing input variables for policy optimization
[2018-07-02 16:15:32.396112 UTC] Performing policy update
[2018-07-02 16:15:32.396999 UTC] Computing gradient in Euclidean space
[2018-07-02 16:15:32.445792 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:32.987749 UTC] Performing line search
[2018-07-02 16:15:33.032579 UTC] Updating baseline
[2018-07-02 16:15:33.831997 UTC] Computing logging information
------------------------------------
| Iteration            | 8         |
| ExpectedImprovement  | 0.0084326 |
| ActualImprovement    | 0.0081217 |
| ImprovementRatio     | 0.96313   |
| MeanKL               | 0.0092752 |
| Entropy              | 1.4602    |
| Perplexity           | 4.3066    |
| AveragePolicyStd     | 1.0421    |
| AveragePolicyStd[0]  | 1.0421    |
| AverageReturn        | -997.53   |
| MinReturn            | -1549.3   |
| MaxReturn            | -752.66   |
| StdReturn            | 132.06    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 448       |
| TotalNSamples        | 89600     |
| ExplainedVariance    | 0.77228   |
------------------------------------
[2018-07-02 16:15:34.694131 UTC] Saving snapshot
[2018-07-02 16:15:34.702014 UTC] Starting iteration 9
[2018-07-02 16:15:34.703525 UTC] Start collecting samples
[2018-07-02 16:15:36.323714 UTC] Computing input variables for policy optimization
[2018-07-02 16:15:36.395902 UTC] Performing policy update
[2018-07-02 16:15:36.396774 UTC] Computing gradient in Euclidean space
[2018-07-02 16:15:36.446381 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:36.990153 UTC] Performing line search
[2018-07-02 16:15:37.071687 UTC] Updating baseline
[2018-07-02 16:15:37.723778 UTC] Computing logging information
------------------------------------
| Iteration            | 9         |
| ExpectedImprovement  | 0.0058477 |
| ActualImprovement    | 0.0057634 |
| ImprovementRatio     | 0.98558   |
| MeanKL               | 0.0067567 |
| Entropy              | 1.4655    |
| Perplexity           | 4.3297    |
| AveragePolicyStd     | 1.0477    |
| AveragePolicyStd[0]  | 1.0477    |
| AverageReturn        | -984.09   |
| MinReturn            | -1549.3   |
| MaxReturn            | -754.89   |
| StdReturn            | 130.11    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 496       |
| TotalNSamples        | 99200     |
| ExplainedVariance    | 0.82135   |
------------------------------------
[2018-07-02 16:15:38.445024 UTC] Saving snapshot
[2018-07-02 16:15:38.454767 UTC] Starting iteration 10
[2018-07-02 16:15:38.455414 UTC] Start collecting samples
[2018-07-02 16:15:40.188385 UTC] Computing input variables for policy optimization
[2018-07-02 16:15:40.259900 UTC] Performing policy update
[2018-07-02 16:15:40.261101 UTC] Computing gradient in Euclidean space
[2018-07-02 16:15:40.313747 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:40.890602 UTC] Performing line search
[2018-07-02 16:15:40.971177 UTC] Updating baseline
[2018-07-02 16:15:41.809018 UTC] Computing logging information
------------------------------------
| Iteration            | 10        |
| ExpectedImprovement  | 0.0081349 |
| ActualImprovement    | 0.0079244 |
| ImprovementRatio     | 0.97413   |
| MeanKL               | 0.006545  |
| Entropy              | 1.4867    |
| Perplexity           | 4.4226    |
| AveragePolicyStd     | 1.0701    |
| AveragePolicyStd[0]  | 1.0701    |
| AverageReturn        | -944.62   |
| MinReturn            | -1240.6   |
| MaxReturn            | -630.45   |
| StdReturn            | 119.67    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 544       |
| TotalNSamples        | 1.088e+05 |
| ExplainedVariance    | 0.76669   |
------------------------------------
[2018-07-02 16:15:43.014130 UTC] Saving snapshot
[2018-07-02 16:15:43.023454 UTC] Starting iteration 11
[2018-07-02 16:15:43.023974 UTC] Start collecting samples
[2018-07-02 16:15:44.811878 UTC] Computing input variables for policy optimization
[2018-07-02 16:15:44.892849 UTC] Performing policy update
[2018-07-02 16:15:44.895676 UTC] Computing gradient in Euclidean space
[2018-07-02 16:15:44.953640 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:45.505034 UTC] Performing line search
[2018-07-02 16:15:45.583421 UTC] Updating baseline
[2018-07-02 16:15:46.213495 UTC] Computing logging information
------------------------------------
| Iteration            | 11        |
| ExpectedImprovement  | 0.0068304 |
| ActualImprovement    | 0.0069706 |
| ImprovementRatio     | 1.0205    |
| MeanKL               | 0.0068269 |
| Entropy              | 1.4794    |
| Perplexity           | 4.3905    |
| AveragePolicyStd     | 1.0624    |
| AveragePolicyStd[0]  | 1.0624    |
| AverageReturn        | -919.35   |
| MinReturn            | -1218.1   |
| MaxReturn            | -630.45   |
| StdReturn            | 125.14    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 592       |
| TotalNSamples        | 1.184e+05 |
| ExplainedVariance    | 0.68364   |
------------------------------------
[2018-07-02 16:15:47.108122 UTC] Saving snapshot
[2018-07-02 16:15:47.122184 UTC] Starting iteration 12
[2018-07-02 16:15:47.122804 UTC] Start collecting samples
[2018-07-02 16:15:49.267129 UTC] Computing input variables for policy optimization
[2018-07-02 16:15:49.351941 UTC] Performing policy update
[2018-07-02 16:15:49.352984 UTC] Computing gradient in Euclidean space
[2018-07-02 16:15:49.408826 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:50.005403 UTC] Performing line search
[2018-07-02 16:15:50.088599 UTC] Updating baseline
[2018-07-02 16:15:50.734863 UTC] Computing logging information
------------------------------------
| Iteration            | 12        |
| ExpectedImprovement  | 0.0059831 |
| ActualImprovement    | 0.005822  |
| ImprovementRatio     | 0.97307   |
| MeanKL               | 0.0072906 |
| Entropy              | 1.4801    |
| Perplexity           | 4.3935    |
| AveragePolicyStd     | 1.0631    |
| AveragePolicyStd[0]  | 1.0631    |
| AverageReturn        | -918.93   |
| MinReturn            | -1210.5   |
| MaxReturn            | -640.42   |
| StdReturn            | 137.35    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 640       |
| TotalNSamples        | 1.28e+05  |
| ExplainedVariance    | 0.68628   |
------------------------------------
[2018-07-02 16:15:51.624853 UTC] Saving snapshot
[2018-07-02 16:15:51.632259 UTC] Starting iteration 13
[2018-07-02 16:15:51.633030 UTC] Start collecting samples
[2018-07-02 16:15:53.299931 UTC] Computing input variables for policy optimization
[2018-07-02 16:15:53.371198 UTC] Performing policy update
[2018-07-02 16:15:53.373885 UTC] Computing gradient in Euclidean space
[2018-07-02 16:15:53.428669 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:53.982507 UTC] Performing line search
[2018-07-02 16:15:54.073345 UTC] Updating baseline
[2018-07-02 16:15:54.791483 UTC] Computing logging information
------------------------------------
| Iteration            | 13        |
| ExpectedImprovement  | 0.0056546 |
| ActualImprovement    | 0.005265  |
| ImprovementRatio     | 0.93111   |
| MeanKL               | 0.0067343 |
| Entropy              | 1.4345    |
| Perplexity           | 4.1975    |
| AveragePolicyStd     | 1.0157    |
| AveragePolicyStd[0]  | 1.0157    |
| AverageReturn        | -913.56   |
| MinReturn            | -1230.7   |
| MaxReturn            | -634.12   |
| StdReturn            | 145.52    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 688       |
| TotalNSamples        | 1.376e+05 |
| ExplainedVariance    | 0.67996   |
------------------------------------
[2018-07-02 16:15:55.730313 UTC] Saving snapshot
[2018-07-02 16:15:55.740625 UTC] Starting iteration 14
[2018-07-02 16:15:55.741204 UTC] Start collecting samples
[2018-07-02 16:15:57.357116 UTC] Computing input variables for policy optimization
[2018-07-02 16:15:57.432267 UTC] Performing policy update
[2018-07-02 16:15:57.433857 UTC] Computing gradient in Euclidean space
[2018-07-02 16:15:57.481391 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:15:58.034434 UTC] Performing line search
[2018-07-02 16:15:58.115206 UTC] Updating baseline
[2018-07-02 16:15:58.882077 UTC] Computing logging information
------------------------------------
| Iteration            | 14        |
| ExpectedImprovement  | 0.0059588 |
| ActualImprovement    | 0.0060024 |
| ImprovementRatio     | 1.0073    |
| MeanKL               | 0.0073304 |
| Entropy              | 1.4262    |
| Perplexity           | 4.1628    |
| AveragePolicyStd     | 1.0073    |
| AveragePolicyStd[0]  | 1.0073    |
| AverageReturn        | -880.69   |
| MinReturn            | -1230.7   |
| MaxReturn            | -628.71   |
| StdReturn            | 126.71    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 736       |
| TotalNSamples        | 1.472e+05 |
| ExplainedVariance    | 0.61816   |
------------------------------------
[2018-07-02 16:15:59.766237 UTC] Saving snapshot
[2018-07-02 16:15:59.777057 UTC] Starting iteration 15
[2018-07-02 16:15:59.777900 UTC] Start collecting samples
[2018-07-02 16:16:01.623434 UTC] Computing input variables for policy optimization
[2018-07-02 16:16:01.705836 UTC] Performing policy update
[2018-07-02 16:16:01.708061 UTC] Computing gradient in Euclidean space
[2018-07-02 16:16:01.759473 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:16:02.307466 UTC] Performing line search
[2018-07-02 16:16:02.352843 UTC] Updating baseline
[2018-07-02 16:16:02.991095 UTC] Computing logging information
------------------------------------
| Iteration            | 15        |
| ExpectedImprovement  | 0.0076537 |
| ActualImprovement    | 0.0076306 |
| ImprovementRatio     | 0.99699   |
| MeanKL               | 0.009114  |
| Entropy              | 1.412     |
| Perplexity           | 4.1042    |
| AveragePolicyStd     | 0.9931    |
| AveragePolicyStd[0]  | 0.9931    |
| AverageReturn        | -857.33   |
| MinReturn            | -1138.6   |
| MaxReturn            | -618.89   |
| StdReturn            | 118.6     |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 800       |
| TotalNSamples        | 1.6e+05   |
| ExplainedVariance    | 0.76602   |
------------------------------------
[2018-07-02 16:16:03.906707 UTC] Saving snapshot
[2018-07-02 16:16:03.916517 UTC] Starting iteration 16
[2018-07-02 16:16:03.917082 UTC] Start collecting samples
[2018-07-02 16:16:05.474716 UTC] Computing input variables for policy optimization
[2018-07-02 16:16:05.555319 UTC] Performing policy update
[2018-07-02 16:16:05.556837 UTC] Computing gradient in Euclidean space
[2018-07-02 16:16:05.614452 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:16:06.348222 UTC] Performing line search
[2018-07-02 16:16:06.576242 UTC] Updating baseline
[2018-07-02 16:16:07.779635 UTC] Computing logging information
------------------------------------
| Iteration            | 16        |
| ExpectedImprovement  | 0.0082387 |
| ActualImprovement    | 0.0083606 |
| ImprovementRatio     | 1.0148    |
| MeanKL               | 0.0067298 |
| Entropy              | 1.4174    |
| Perplexity           | 4.1265    |
| AveragePolicyStd     | 0.99849   |
| AveragePolicyStd[0]  | 0.99849   |
| AverageReturn        | -843.43   |
| MinReturn            | -1129.7   |
| MaxReturn            | -618.89   |
| StdReturn            | 123.48    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 848       |
| TotalNSamples        | 1.696e+05 |
| ExplainedVariance    | 0.87313   |
------------------------------------
[2018-07-02 16:16:08.654393 UTC] Saving snapshot
[2018-07-02 16:16:08.662996 UTC] Starting iteration 17
[2018-07-02 16:16:08.663732 UTC] Start collecting samples
[2018-07-02 16:16:10.385715 UTC] Computing input variables for policy optimization
[2018-07-02 16:16:10.460497 UTC] Performing policy update
[2018-07-02 16:16:10.461814 UTC] Computing gradient in Euclidean space
[2018-07-02 16:16:10.513799 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:16:11.074084 UTC] Performing line search
[2018-07-02 16:16:11.154409 UTC] Updating baseline
[2018-07-02 16:16:11.815255 UTC] Computing logging information
------------------------------------
| Iteration            | 17        |
| ExpectedImprovement  | 0.0064197 |
| ActualImprovement    | 0.0061825 |
| ImprovementRatio     | 0.96305   |
| MeanKL               | 0.0064342 |
| Entropy              | 1.4139    |
| Perplexity           | 4.1118    |
| AveragePolicyStd     | 0.99493   |
| AveragePolicyStd[0]  | 0.99493   |
| AverageReturn        | -818.06   |
| MinReturn            | -1129.7   |
| MaxReturn            | -626.5    |
| StdReturn            | 111.45    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 896       |
| TotalNSamples        | 1.792e+05 |
| ExplainedVariance    | 0.84936   |
------------------------------------
[2018-07-02 16:16:12.827433 UTC] Saving snapshot
[2018-07-02 16:16:12.837053 UTC] Starting iteration 18
[2018-07-02 16:16:12.837879 UTC] Start collecting samples
[2018-07-02 16:16:14.535320 UTC] Computing input variables for policy optimization
[2018-07-02 16:16:14.652551 UTC] Performing policy update
[2018-07-02 16:16:14.658935 UTC] Computing gradient in Euclidean space
[2018-07-02 16:16:14.739216 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:16:15.343206 UTC] Performing line search
[2018-07-02 16:16:15.433911 UTC] Updating baseline
[2018-07-02 16:16:16.112738 UTC] Computing logging information
------------------------------------
| Iteration            | 18        |
| ExpectedImprovement  | 0.0079295 |
| ActualImprovement    | 0.0083029 |
| ImprovementRatio     | 1.0471    |
| MeanKL               | 0.0071816 |
| Entropy              | 1.3959    |
| Perplexity           | 4.0387    |
| AveragePolicyStd     | 0.97725   |
| AveragePolicyStd[0]  | 0.97725   |
| AverageReturn        | -778.67   |
| MinReturn            | -1028.8   |
| MaxReturn            | -503.67   |
| StdReturn            | 96.122    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 944       |
| TotalNSamples        | 1.888e+05 |
| ExplainedVariance    | 0.80655   |
------------------------------------
[2018-07-02 16:16:17.047517 UTC] Saving snapshot
[2018-07-02 16:16:17.056662 UTC] Starting iteration 19
[2018-07-02 16:16:17.057476 UTC] Start collecting samples
[2018-07-02 16:16:18.675422 UTC] Computing input variables for policy optimization
[2018-07-02 16:16:18.760797 UTC] Performing policy update
[2018-07-02 16:16:18.762178 UTC] Computing gradient in Euclidean space
[2018-07-02 16:16:18.812134 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:16:19.405135 UTC] Performing line search
[2018-07-02 16:16:19.491615 UTC] Updating baseline
[2018-07-02 16:16:20.268735 UTC] Computing logging information
------------------------------------
| Iteration            | 19        |
| ExpectedImprovement  | 0.0086258 |
| ActualImprovement    | 0.0084924 |
| ImprovementRatio     | 0.98454   |
| MeanKL               | 0.0065604 |
| Entropy              | 1.3997    |
| Perplexity           | 4.054     |
| AveragePolicyStd     | 0.98095   |
| AveragePolicyStd[0]  | 0.98095   |
| AverageReturn        | -737.24   |
| MinReturn            | -927.71   |
| MaxReturn            | -500.69   |
| StdReturn            | 105.17    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 992       |
| TotalNSamples        | 1.984e+05 |
| ExplainedVariance    | 0.69228   |
------------------------------------
[2018-07-02 16:16:21.144524 UTC] Saving snapshot
[2018-07-02 16:16:21.152936 UTC] Starting iteration 20
[2018-07-02 16:16:21.153570 UTC] Start collecting samples
[2018-07-02 16:16:22.541904 UTC] Computing input variables for policy optimization
[2018-07-02 16:16:22.630414 UTC] Performing policy update
[2018-07-02 16:16:22.631592 UTC] Computing gradient in Euclidean space
[2018-07-02 16:16:22.682729 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:16:23.215258 UTC] Performing line search
[2018-07-02 16:16:23.293458 UTC] Updating baseline
[2018-07-02 16:16:23.915081 UTC] Computing logging information
------------------------------------
| Iteration            | 20        |
| ExpectedImprovement  | 0.0079874 |
| ActualImprovement    | 0.0077948 |
| ImprovementRatio     | 0.97589   |
| MeanKL               | 0.0068987 |
| Entropy              | 1.396     |
| Perplexity           | 4.0392    |
| AveragePolicyStd     | 0.97737   |
| AveragePolicyStd[0]  | 0.97737   |
| AverageReturn        | -705.76   |
| MinReturn            | -981.13   |
| MaxReturn            | -500.69   |
| StdReturn            | 109.76    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1040      |
| TotalNSamples        | 2.08e+05  |
| ExplainedVariance    | 0.68417   |
------------------------------------
[2018-07-02 16:16:24.865393 UTC] Saving snapshot
[2018-07-02 16:16:24.873290 UTC] Starting iteration 21
[2018-07-02 16:16:24.874712 UTC] Start collecting samples
[2018-07-02 16:16:26.451546 UTC] Computing input variables for policy optimization
[2018-07-02 16:16:26.530804 UTC] Performing policy update
[2018-07-02 16:16:26.531855 UTC] Computing gradient in Euclidean space
[2018-07-02 16:16:26.580283 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:16:27.148765 UTC] Performing line search
[2018-07-02 16:16:27.226266 UTC] Updating baseline
[2018-07-02 16:16:27.950830 UTC] Computing logging information
------------------------------------
| Iteration            | 21        |
| ExpectedImprovement  | 0.0094396 |
| ActualImprovement    | 0.0093998 |
| ImprovementRatio     | 0.99579   |
| MeanKL               | 0.0068089 |
| Entropy              | 1.3898    |
| Perplexity           | 4.0141    |
| AveragePolicyStd     | 0.9713    |
| AveragePolicyStd[0]  | 0.9713    |
| AverageReturn        | -689.61   |
| MinReturn            | -981.13   |
| MaxReturn            | -500.49   |
| StdReturn            | 103.18    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1088      |
| TotalNSamples        | 2.176e+05 |
| ExplainedVariance    | 0.68272   |
------------------------------------
[2018-07-02 16:16:28.858689 UTC] Saving snapshot
[2018-07-02 16:16:28.868106 UTC] Starting iteration 22
[2018-07-02 16:16:28.868760 UTC] Start collecting samples
[2018-07-02 16:16:30.703516 UTC] Computing input variables for policy optimization
[2018-07-02 16:16:30.778293 UTC] Performing policy update
[2018-07-02 16:16:30.779854 UTC] Computing gradient in Euclidean space
[2018-07-02 16:16:30.827972 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:16:31.362962 UTC] Performing line search
[2018-07-02 16:16:31.440265 UTC] Updating baseline
[2018-07-02 16:16:32.057377 UTC] Computing logging information
------------------------------------
| Iteration            | 22        |
| ExpectedImprovement  | 0.0078021 |
| ActualImprovement    | 0.0077804 |
| ImprovementRatio     | 0.99722   |
| MeanKL               | 0.0071165 |
| Entropy              | 1.3643    |
| Perplexity           | 3.913     |
| AveragePolicyStd     | 0.94684   |
| AveragePolicyStd[0]  | 0.94684   |
| AverageReturn        | -657.79   |
| MinReturn            | -880.67   |
| MaxReturn            | -379.72   |
| StdReturn            | 104.55    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1136      |
| TotalNSamples        | 2.272e+05 |
| ExplainedVariance    | 0.67742   |
------------------------------------
[2018-07-02 16:16:32.917438 UTC] Saving snapshot
[2018-07-02 16:16:32.927702 UTC] Starting iteration 23
[2018-07-02 16:16:32.928385 UTC] Start collecting samples
[2018-07-02 16:16:34.737863 UTC] Computing input variables for policy optimization
[2018-07-02 16:16:34.818285 UTC] Performing policy update
[2018-07-02 16:16:34.820141 UTC] Computing gradient in Euclidean space
[2018-07-02 16:16:34.868693 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:16:35.421034 UTC] Performing line search
[2018-07-02 16:16:35.499857 UTC] Updating baseline
[2018-07-02 16:16:36.149186 UTC] Computing logging information
------------------------------------
| Iteration            | 23        |
| ExpectedImprovement  | 0.0099959 |
| ActualImprovement    | 0.0096026 |
| ImprovementRatio     | 0.96066   |
| MeanKL               | 0.0065112 |
| Entropy              | 1.3455    |
| Perplexity           | 3.8399    |
| AveragePolicyStd     | 0.92915   |
| AveragePolicyStd[0]  | 0.92915   |
| AverageReturn        | -620.17   |
| MinReturn            | -907.38   |
| MaxReturn            | -377.24   |
| StdReturn            | 118.6     |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1200      |
| TotalNSamples        | 2.4e+05   |
| ExplainedVariance    | 0.77183   |
------------------------------------
[2018-07-02 16:16:37.328570 UTC] Saving snapshot
[2018-07-02 16:16:37.339824 UTC] Starting iteration 24
[2018-07-02 16:16:37.340585 UTC] Start collecting samples
[2018-07-02 16:16:39.442781 UTC] Computing input variables for policy optimization
[2018-07-02 16:16:39.522134 UTC] Performing policy update
[2018-07-02 16:16:39.523435 UTC] Computing gradient in Euclidean space
[2018-07-02 16:16:39.574828 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:16:40.117836 UTC] Performing line search
[2018-07-02 16:16:40.201263 UTC] Updating baseline
[2018-07-02 16:16:40.853432 UTC] Computing logging information
------------------------------------
| Iteration            | 24        |
| ExpectedImprovement  | 0.012567  |
| ActualImprovement    | 0.01235   |
| ImprovementRatio     | 0.98274   |
| MeanKL               | 0.0067939 |
| Entropy              | 1.3289    |
| Perplexity           | 3.7768    |
| AveragePolicyStd     | 0.91388   |
| AveragePolicyStd[0]  | 0.91388   |
| AverageReturn        | -575.44   |
| MinReturn            | -859.93   |
| MaxReturn            | -259.11   |
| StdReturn            | 121.56    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1248      |
| TotalNSamples        | 2.496e+05 |
| ExplainedVariance    | 0.81833   |
------------------------------------
[2018-07-02 16:16:42.588844 UTC] Saving snapshot
[2018-07-02 16:16:42.601650 UTC] Starting iteration 25
[2018-07-02 16:16:42.602968 UTC] Start collecting samples
[2018-07-02 16:16:44.464154 UTC] Computing input variables for policy optimization
[2018-07-02 16:16:44.536615 UTC] Performing policy update
[2018-07-02 16:16:44.539189 UTC] Computing gradient in Euclidean space
[2018-07-02 16:16:44.588538 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:16:45.108732 UTC] Performing line search
[2018-07-02 16:16:45.183770 UTC] Updating baseline
[2018-07-02 16:16:45.874080 UTC] Computing logging information
------------------------------------
| Iteration            | 25        |
| ExpectedImprovement  | 0.017395  |
| ActualImprovement    | 0.017288  |
| ImprovementRatio     | 0.99386   |
| MeanKL               | 0.0072199 |
| Entropy              | 1.3068    |
| Perplexity           | 3.6944    |
| AveragePolicyStd     | 0.89394   |
| AveragePolicyStd[0]  | 0.89394   |
| AverageReturn        | -524.18   |
| MinReturn            | -780.31   |
| MaxReturn            | -124.59   |
| StdReturn            | 137.3     |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1296      |
| TotalNSamples        | 2.592e+05 |
| ExplainedVariance    | 0.81032   |
------------------------------------
[2018-07-02 16:16:46.738096 UTC] Saving snapshot
[2018-07-02 16:16:46.746969 UTC] Starting iteration 26
[2018-07-02 16:16:46.747586 UTC] Start collecting samples
[2018-07-02 16:16:48.894759 UTC] Computing input variables for policy optimization
[2018-07-02 16:16:49.084770 UTC] Performing policy update
[2018-07-02 16:16:49.086868 UTC] Computing gradient in Euclidean space
[2018-07-02 16:16:49.188879 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:16:50.302414 UTC] Performing line search
[2018-07-02 16:16:50.405905 UTC] Updating baseline
[2018-07-02 16:16:51.609911 UTC] Computing logging information
------------------------------------
| Iteration            | 26        |
| ExpectedImprovement  | 0.018397  |
| ActualImprovement    | 0.018357  |
| ImprovementRatio     | 0.99785   |
| MeanKL               | 0.0069234 |
| Entropy              | 1.2856    |
| Perplexity           | 3.6168    |
| AveragePolicyStd     | 0.87517   |
| AveragePolicyStd[0]  | 0.87517   |
| AverageReturn        | -441.26   |
| MinReturn            | -766.98   |
| MaxReturn            | -124.59   |
| StdReturn            | 155.77    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1344      |
| TotalNSamples        | 2.688e+05 |
| ExplainedVariance    | 0.76863   |
------------------------------------
[2018-07-02 16:16:52.932867 UTC] Saving snapshot
[2018-07-02 16:16:52.945557 UTC] Starting iteration 27
[2018-07-02 16:16:52.946352 UTC] Start collecting samples
[2018-07-02 16:16:56.655258 UTC] Computing input variables for policy optimization
[2018-07-02 16:16:56.879305 UTC] Performing policy update
[2018-07-02 16:16:56.881917 UTC] Computing gradient in Euclidean space
[2018-07-02 16:16:56.984805 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:16:57.774224 UTC] Performing line search
[2018-07-02 16:16:57.866871 UTC] Updating baseline
[2018-07-02 16:16:59.088258 UTC] Computing logging information
------------------------------------
| Iteration            | 27        |
| ExpectedImprovement  | 0.015703  |
| ActualImprovement    | 0.015697  |
| ImprovementRatio     | 0.99962   |
| MeanKL               | 0.0065273 |
| Entropy              | 1.2755    |
| Perplexity           | 3.5806    |
| AveragePolicyStd     | 0.86639   |
| AveragePolicyStd[0]  | 0.86639   |
| AverageReturn        | -327.16   |
| MinReturn            | -720.43   |
| MaxReturn            | -5.1273   |
| StdReturn            | 156.05    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1392      |
| TotalNSamples        | 2.784e+05 |
| ExplainedVariance    | 0.74389   |
------------------------------------
[2018-07-02 16:17:00.488202 UTC] Saving snapshot
[2018-07-02 16:17:00.497255 UTC] Starting iteration 28
[2018-07-02 16:17:00.499020 UTC] Start collecting samples
[2018-07-02 16:17:05.401648 UTC] Computing input variables for policy optimization
[2018-07-02 16:17:05.630197 UTC] Performing policy update
[2018-07-02 16:17:05.634525 UTC] Computing gradient in Euclidean space
[2018-07-02 16:17:05.739677 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:17:06.957116 UTC] Performing line search
[2018-07-02 16:17:07.156857 UTC] Updating baseline
[2018-07-02 16:17:09.090223 UTC] Computing logging information
------------------------------------
| Iteration            | 28        |
| ExpectedImprovement  | 0.013775  |
| ActualImprovement    | 0.012426  |
| ImprovementRatio     | 0.90205   |
| MeanKL               | 0.0073518 |
| Entropy              | 1.259     |
| Perplexity           | 3.5219    |
| AveragePolicyStd     | 0.85219   |
| AveragePolicyStd[0]  | 0.85219   |
| AverageReturn        | -267.28   |
| MinReturn            | -620.89   |
| MaxReturn            | -4.8953   |
| StdReturn            | 139.72    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1440      |
| TotalNSamples        | 2.88e+05  |
| ExplainedVariance    | 0.9135    |
------------------------------------
[2018-07-02 16:17:10.707026 UTC] Saving snapshot
[2018-07-02 16:17:10.716935 UTC] Starting iteration 29
[2018-07-02 16:17:10.717880 UTC] Start collecting samples
[2018-07-02 16:17:15.124993 UTC] Computing input variables for policy optimization
[2018-07-02 16:17:15.256727 UTC] Performing policy update
[2018-07-02 16:17:15.258444 UTC] Computing gradient in Euclidean space
[2018-07-02 16:17:15.330501 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:17:16.113487 UTC] Performing line search
[2018-07-02 16:17:16.220519 UTC] Updating baseline
[2018-07-02 16:17:17.501818 UTC] Computing logging information
------------------------------------
| Iteration            | 29        |
| ExpectedImprovement  | 0.015656  |
| ActualImprovement    | 0.014688  |
| ImprovementRatio     | 0.93821   |
| MeanKL               | 0.0066261 |
| Entropy              | 1.2357    |
| Perplexity           | 3.4409    |
| AveragePolicyStd     | 0.83259   |
| AveragePolicyStd[0]  | 0.83259   |
| AverageReturn        | -238.68   |
| MinReturn            | -551.96   |
| MaxReturn            | -1.5834   |
| StdReturn            | 131.06    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1488      |
| TotalNSamples        | 2.976e+05 |
| ExplainedVariance    | 0.89932   |
------------------------------------
[2018-07-02 16:17:18.778006 UTC] Saving snapshot
[2018-07-02 16:17:18.792477 UTC] Starting iteration 30
[2018-07-02 16:17:18.793533 UTC] Start collecting samples
[2018-07-02 16:17:22.929643 UTC] Computing input variables for policy optimization
[2018-07-02 16:17:23.119873 UTC] Performing policy update
[2018-07-02 16:17:23.121370 UTC] Computing gradient in Euclidean space
[2018-07-02 16:17:23.198557 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:17:24.192551 UTC] Performing line search
[2018-07-02 16:17:24.454628 UTC] Updating baseline
[2018-07-02 16:17:26.112379 UTC] Computing logging information
------------------------------------
| Iteration            | 30        |
| ExpectedImprovement  | 0.01224   |
| ActualImprovement    | 0.012169  |
| ImprovementRatio     | 0.99424   |
| MeanKL               | 0.0064459 |
| Entropy              | 1.2233    |
| Perplexity           | 3.3985    |
| AveragePolicyStd     | 0.82235   |
| AveragePolicyStd[0]  | 0.82235   |
| AverageReturn        | -227.06   |
| MinReturn            | -551.96   |
| MaxReturn            | -1.5834   |
| StdReturn            | 127.76    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1536      |
| TotalNSamples        | 3.072e+05 |
| ExplainedVariance    | 0.92144   |
------------------------------------
[2018-07-02 16:17:27.362977 UTC] Saving snapshot
[2018-07-02 16:17:27.379118 UTC] Starting iteration 31
[2018-07-02 16:17:27.380307 UTC] Start collecting samples
[2018-07-02 16:17:31.218012 UTC] Computing input variables for policy optimization
[2018-07-02 16:17:31.364362 UTC] Performing policy update
[2018-07-02 16:17:31.366266 UTC] Computing gradient in Euclidean space
[2018-07-02 16:17:31.442542 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:17:32.342528 UTC] Performing line search
[2018-07-02 16:17:32.545534 UTC] Updating baseline
[2018-07-02 16:17:33.554827 UTC] Computing logging information
------------------------------------
| Iteration            | 31        |
| ExpectedImprovement  | 0.014111  |
| ActualImprovement    | 0.012208  |
| ImprovementRatio     | 0.86512   |
| MeanKL               | 0.0065514 |
| Entropy              | 1.2165    |
| Perplexity           | 3.3752    |
| AveragePolicyStd     | 0.81671   |
| AveragePolicyStd[0]  | 0.81671   |
| AverageReturn        | -215.21   |
| MinReturn            | -549.99   |
| MaxReturn            | -1.4792   |
| StdReturn            | 123.25    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1600      |
| TotalNSamples        | 3.2e+05   |
| ExplainedVariance    | 0.95754   |
------------------------------------
[2018-07-02 16:17:34.833880 UTC] Saving snapshot
[2018-07-02 16:17:34.847433 UTC] Starting iteration 32
[2018-07-02 16:17:34.848784 UTC] Start collecting samples
[2018-07-02 16:17:38.384839 UTC] Computing input variables for policy optimization
[2018-07-02 16:17:38.533441 UTC] Performing policy update
[2018-07-02 16:17:38.535016 UTC] Computing gradient in Euclidean space
[2018-07-02 16:17:38.602123 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:17:39.311482 UTC] Performing line search
[2018-07-02 16:17:39.489531 UTC] Updating baseline
[2018-07-02 16:17:40.713507 UTC] Computing logging information
------------------------------------
| Iteration            | 32        |
| ExpectedImprovement  | 0.012729  |
| ActualImprovement    | 0.011245  |
| ImprovementRatio     | 0.88343   |
| MeanKL               | 0.0064203 |
| Entropy              | 1.2083    |
| Perplexity           | 3.3479    |
| AveragePolicyStd     | 0.8101    |
| AveragePolicyStd[0]  | 0.8101    |
| AverageReturn        | -191.23   |
| MinReturn            | -549.99   |
| MaxReturn            | -1.4792   |
| StdReturn            | 118.75    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1648      |
| TotalNSamples        | 3.296e+05 |
| ExplainedVariance    | 0.96109   |
------------------------------------
[2018-07-02 16:17:42.047418 UTC] Saving snapshot
[2018-07-02 16:17:42.057286 UTC] Starting iteration 33
[2018-07-02 16:17:42.058117 UTC] Start collecting samples
[2018-07-02 16:17:46.506549 UTC] Computing input variables for policy optimization
[2018-07-02 16:17:46.651905 UTC] Performing policy update
[2018-07-02 16:17:46.653277 UTC] Computing gradient in Euclidean space
[2018-07-02 16:17:46.771247 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:17:48.490923 UTC] Performing line search
[2018-07-02 16:17:48.561713 UTC] Updating baseline
[2018-07-02 16:17:49.731257 UTC] Computing logging information
------------------------------------
| Iteration            | 33        |
| ExpectedImprovement  | 0.014966  |
| ActualImprovement    | 0.014081  |
| ImprovementRatio     | 0.9409    |
| MeanKL               | 0.0099644 |
| Entropy              | 1.1947    |
| Perplexity           | 3.3024    |
| AveragePolicyStd     | 0.79909   |
| AveragePolicyStd[0]  | 0.79909   |
| AverageReturn        | -186.57   |
| MinReturn            | -434.63   |
| MaxReturn            | -1.5348   |
| StdReturn            | 120.81    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1696      |
| TotalNSamples        | 3.392e+05 |
| ExplainedVariance    | 0.94793   |
------------------------------------
[2018-07-02 16:17:51.012211 UTC] Saving snapshot
[2018-07-02 16:17:51.023332 UTC] Starting iteration 34
[2018-07-02 16:17:51.024091 UTC] Start collecting samples
[2018-07-02 16:17:55.150217 UTC] Computing input variables for policy optimization
[2018-07-02 16:17:55.357209 UTC] Performing policy update
[2018-07-02 16:17:55.359285 UTC] Computing gradient in Euclidean space
[2018-07-02 16:17:55.437466 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:17:56.639048 UTC] Performing line search
[2018-07-02 16:17:56.751342 UTC] Updating baseline
[2018-07-02 16:17:57.824450 UTC] Computing logging information
------------------------------------
| Iteration            | 34        |
| ExpectedImprovement  | 0.012844  |
| ActualImprovement    | 0.012629  |
| ImprovementRatio     | 0.98332   |
| MeanKL               | 0.0064742 |
| Entropy              | 1.1714    |
| Perplexity           | 3.2263    |
| AveragePolicyStd     | 0.78068   |
| AveragePolicyStd[0]  | 0.78068   |
| AverageReturn        | -187.98   |
| MinReturn            | -508.94   |
| MaxReturn            | -1.5348   |
| StdReturn            | 128.52    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1744      |
| TotalNSamples        | 3.488e+05 |
| ExplainedVariance    | 0.95324   |
------------------------------------
[2018-07-02 16:17:59.310155 UTC] Saving snapshot
[2018-07-02 16:17:59.324556 UTC] Starting iteration 35
[2018-07-02 16:17:59.327406 UTC] Start collecting samples
[2018-07-02 16:18:03.749027 UTC] Computing input variables for policy optimization
[2018-07-02 16:18:04.064598 UTC] Performing policy update
[2018-07-02 16:18:04.068101 UTC] Computing gradient in Euclidean space
[2018-07-02 16:18:04.228222 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:18:05.566939 UTC] Performing line search
[2018-07-02 16:18:05.643229 UTC] Updating baseline
[2018-07-02 16:18:06.741947 UTC] Computing logging information
------------------------------------
| Iteration            | 35        |
| ExpectedImprovement  | 0.016158  |
| ActualImprovement    | 0.01327   |
| ImprovementRatio     | 0.82127   |
| MeanKL               | 0.009907  |
| Entropy              | 1.1689    |
| Perplexity           | 3.2185    |
| AveragePolicyStd     | 0.77879   |
| AveragePolicyStd[0]  | 0.77879   |
| AverageReturn        | -184.01   |
| MinReturn            | -508.94   |
| MaxReturn            | -1.5348   |
| StdReturn            | 125.86    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1792      |
| TotalNSamples        | 3.584e+05 |
| ExplainedVariance    | 0.95633   |
------------------------------------
[2018-07-02 16:18:08.067818 UTC] Saving snapshot
[2018-07-02 16:18:08.079323 UTC] Starting iteration 36
[2018-07-02 16:18:08.080624 UTC] Start collecting samples
[2018-07-02 16:18:12.778202 UTC] Computing input variables for policy optimization
[2018-07-02 16:18:12.948302 UTC] Performing policy update
[2018-07-02 16:18:12.950864 UTC] Computing gradient in Euclidean space
[2018-07-02 16:18:13.066476 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:18:14.317842 UTC] Performing line search
[2018-07-02 16:18:14.374198 UTC] Updating baseline
[2018-07-02 16:18:15.398439 UTC] Computing logging information
------------------------------------
| Iteration            | 36        |
| ExpectedImprovement  | 0.025035  |
| ActualImprovement    | 0.021096  |
| ImprovementRatio     | 0.84267   |
| MeanKL               | 0.0095688 |
| Entropy              | 1.1456    |
| Perplexity           | 3.1443    |
| AveragePolicyStd     | 0.76083   |
| AveragePolicyStd[0]  | 0.76083   |
| AverageReturn        | -185.49   |
| MinReturn            | -508.94   |
| MaxReturn            | -1.8639   |
| StdReturn            | 121.26    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1840      |
| TotalNSamples        | 3.68e+05  |
| ExplainedVariance    | 0.8913    |
------------------------------------
[2018-07-02 16:18:16.771148 UTC] Saving snapshot
[2018-07-02 16:18:16.782861 UTC] Starting iteration 37
[2018-07-02 16:18:16.783850 UTC] Start collecting samples
[2018-07-02 16:18:19.432491 UTC] Computing input variables for policy optimization
[2018-07-02 16:18:19.608977 UTC] Performing policy update
[2018-07-02 16:18:19.610980 UTC] Computing gradient in Euclidean space
[2018-07-02 16:18:19.684544 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:18:20.505761 UTC] Performing line search
[2018-07-02 16:18:20.558046 UTC] Updating baseline
[2018-07-02 16:18:21.496002 UTC] Computing logging information
------------------------------------
| Iteration            | 37        |
| ExpectedImprovement  | 0.014831  |
| ActualImprovement    | 0.013476  |
| ImprovementRatio     | 0.90864   |
| MeanKL               | 0.0094511 |
| Entropy              | 1.1431    |
| Perplexity           | 3.1364    |
| AveragePolicyStd     | 0.75891   |
| AveragePolicyStd[0]  | 0.75891   |
| AverageReturn        | -183.85   |
| MinReturn            | -517.77   |
| MaxReturn            | -1.9949   |
| StdReturn            | 108.95    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1888      |
| TotalNSamples        | 3.776e+05 |
| ExplainedVariance    | 0.95708   |
------------------------------------
[2018-07-02 16:18:22.689722 UTC] Saving snapshot
[2018-07-02 16:18:22.704059 UTC] Starting iteration 38
[2018-07-02 16:18:22.706189 UTC] Start collecting samples
[2018-07-02 16:18:25.651402 UTC] Computing input variables for policy optimization
[2018-07-02 16:18:25.880467 UTC] Performing policy update
[2018-07-02 16:18:25.885314 UTC] Computing gradient in Euclidean space
[2018-07-02 16:18:25.969032 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:18:27.095984 UTC] Performing line search
[2018-07-02 16:18:27.224340 UTC] Updating baseline
[2018-07-02 16:18:28.529373 UTC] Computing logging information
------------------------------------
| Iteration            | 38        |
| ExpectedImprovement  | 0.0098534 |
| ActualImprovement    | 0.007231  |
| ImprovementRatio     | 0.73386   |
| MeanKL               | 0.0064016 |
| Entropy              | 1.1418    |
| Perplexity           | 3.1324    |
| AveragePolicyStd     | 0.75794   |
| AveragePolicyStd[0]  | 0.75794   |
| AverageReturn        | -180.94   |
| MinReturn            | -517.77   |
| MaxReturn            | -1.891    |
| StdReturn            | 106.93    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 1936      |
| TotalNSamples        | 3.872e+05 |
| ExplainedVariance    | 0.96844   |
------------------------------------
[2018-07-02 16:18:30.261160 UTC] Saving snapshot
[2018-07-02 16:18:30.272911 UTC] Starting iteration 39
[2018-07-02 16:18:30.273963 UTC] Start collecting samples
[2018-07-02 16:18:34.927415 UTC] Computing input variables for policy optimization
[2018-07-02 16:18:35.169986 UTC] Performing policy update
[2018-07-02 16:18:35.174345 UTC] Computing gradient in Euclidean space
[2018-07-02 16:18:35.292894 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:18:36.674219 UTC] Performing line search
[2018-07-02 16:18:36.830547 UTC] Updating baseline
[2018-07-02 16:18:38.235484 UTC] Computing logging information
-----------------------------------
| Iteration            | 39       |
| ExpectedImprovement  | 0.014999 |
| ActualImprovement    | 0.012423 |
| ImprovementRatio     | 0.82824  |
| MeanKL               | 0.006544 |
| Entropy              | 1.1414   |
| Perplexity           | 3.1311   |
| AveragePolicyStd     | 0.75763  |
| AveragePolicyStd[0]  | 0.75763  |
| AverageReturn        | -156.91  |
| MinReturn            | -378.12  |
| MaxReturn            | -1.3428  |
| StdReturn            | 100.88   |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 2000     |
| TotalNSamples        | 4e+05    |
| ExplainedVariance    | 0.97344  |
-----------------------------------
[2018-07-02 16:18:39.791947 UTC] Saving snapshot
[2018-07-02 16:18:39.818543 UTC] Starting iteration 40
[2018-07-02 16:18:39.821137 UTC] Start collecting samples
[2018-07-02 16:18:43.611739 UTC] Computing input variables for policy optimization
[2018-07-02 16:18:43.783369 UTC] Performing policy update
[2018-07-02 16:18:43.785108 UTC] Computing gradient in Euclidean space
[2018-07-02 16:18:43.860879 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:18:44.699794 UTC] Performing line search
[2018-07-02 16:18:44.792222 UTC] Updating baseline
[2018-07-02 16:18:45.972677 UTC] Computing logging information
------------------------------------
| Iteration            | 40        |
| ExpectedImprovement  | 0.020082  |
| ActualImprovement    | 0.018079  |
| ImprovementRatio     | 0.90024   |
| MeanKL               | 0.0096709 |
| Entropy              | 1.1342    |
| Perplexity           | 3.1085    |
| AveragePolicyStd     | 0.75218   |
| AveragePolicyStd[0]  | 0.75218   |
| AverageReturn        | -157.46   |
| MinReturn            | -398.21   |
| MaxReturn            | -1.622    |
| StdReturn            | 98.619    |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 2048      |
| TotalNSamples        | 4.096e+05 |
| ExplainedVariance    | 0.97348   |
------------------------------------
[2018-07-02 16:18:47.646688 UTC] Saving snapshot
[2018-07-02 16:18:47.659094 UTC] Starting iteration 41
[2018-07-02 16:18:47.661097 UTC] Start collecting samples
