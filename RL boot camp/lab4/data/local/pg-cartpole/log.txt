[2018-07-02 15:55:32.754535 UTC] Starting env pool
[2018-07-02 15:55:32.803657 UTC] Starting iteration 0
[2018-07-02 15:55:32.804745 UTC] Start collecting samples
[2018-07-02 15:55:33.779736 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:33.832336 UTC] Computing policy gradient
[2018-07-02 15:55:33.851946 UTC] Updating baseline
[2018-07-02 15:55:33.957009 UTC] Computing logging information
-------------------------------------
| Iteration            | 0          |
| SurrLoss             | -0.0026496 |
| Entropy              | 0.6925     |
| Perplexity           | 1.9987     |
| AveragePolicyProb[0] | 0.50155    |
| AveragePolicyProb[1] | 0.49845    |
| AverageReturn        | 23.462     |
| MinReturn            | 9          |
| MaxReturn            | 81         |
| StdReturn            | 11.748     |
| AverageEpisodeLength | 23.462     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 81         |
| StdEpisodeLength     | 11.748     |
| TotalNEpisodes       | 78         |
| TotalNSamples        | 1830       |
| ExplainedVariance    | -0.0058665 |
-------------------------------------
[2018-07-02 15:55:34.866327 UTC] Saving snapshot
[2018-07-02 15:55:34.876106 UTC] Starting iteration 1
[2018-07-02 15:55:34.877017 UTC] Start collecting samples
[2018-07-02 15:55:35.560133 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:35.597301 UTC] Computing policy gradient
[2018-07-02 15:55:35.604771 UTC] Updating baseline
[2018-07-02 15:55:35.693869 UTC] Computing logging information
------------------------------------
| Iteration            | 1         |
| SurrLoss             | -0.028403 |
| Entropy              | 0.63881   |
| Perplexity           | 1.8942    |
| AveragePolicyProb[0] | 0.48601   |
| AveragePolicyProb[1] | 0.51399   |
| AverageReturn        | 30.72     |
| MinReturn            | 9         |
| MaxReturn            | 109       |
| StdReturn            | 18.103    |
| AverageEpisodeLength | 30.72     |
| MinEpisodeLength     | 9         |
| MaxEpisodeLength     | 109       |
| StdEpisodeLength     | 18.103    |
| TotalNEpisodes       | 124       |
| TotalNSamples        | 3619      |
| ExplainedVariance    | 0.15902   |
------------------------------------
[2018-07-02 15:55:36.362854 UTC] Saving snapshot
[2018-07-02 15:55:36.373020 UTC] Starting iteration 2
[2018-07-02 15:55:36.373807 UTC] Start collecting samples
[2018-07-02 15:55:36.842553 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:36.869763 UTC] Computing policy gradient
[2018-07-02 15:55:36.881173 UTC] Updating baseline
[2018-07-02 15:55:36.968256 UTC] Computing logging information
------------------------------------
| Iteration            | 2         |
| SurrLoss             | -0.044707 |
| Entropy              | 0.60104   |
| Perplexity           | 1.824     |
| AveragePolicyProb[0] | 0.48011   |
| AveragePolicyProb[1] | 0.51989   |
| AverageReturn        | 38.42     |
| MinReturn            | 10        |
| MaxReturn            | 112       |
| StdReturn            | 22.32     |
| AverageEpisodeLength | 38.42     |
| MinEpisodeLength     | 10        |
| MaxEpisodeLength     | 112       |
| StdEpisodeLength     | 22.32     |
| TotalNEpisodes       | 148       |
| TotalNSamples        | 5017      |
| ExplainedVariance    | 0.33974   |
------------------------------------
[2018-07-02 15:55:37.787206 UTC] Saving snapshot
[2018-07-02 15:55:37.798847 UTC] Starting iteration 3
[2018-07-02 15:55:37.799680 UTC] Start collecting samples
[2018-07-02 15:55:38.118284 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:38.138793 UTC] Computing policy gradient
[2018-07-02 15:55:38.150098 UTC] Updating baseline
[2018-07-02 15:55:38.241483 UTC] Computing logging information
------------------------------------
| Iteration            | 3         |
| SurrLoss             | -0.021752 |
| Entropy              | 0.56557   |
| Perplexity           | 1.7605    |
| AveragePolicyProb[0] | 0.51612   |
| AveragePolicyProb[1] | 0.48388   |
| AverageReturn        | 53.1      |
| MinReturn            | 10        |
| MaxReturn            | 200       |
| StdReturn            | 42.011    |
| AverageEpisodeLength | 53.1      |
| MinEpisodeLength     | 10        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 42.011    |
| TotalNEpisodes       | 161       |
| TotalNSamples        | 6783      |
| ExplainedVariance    | 0.33004   |
------------------------------------
[2018-07-02 15:55:39.019885 UTC] Saving snapshot
[2018-07-02 15:55:39.033116 UTC] Starting iteration 4
[2018-07-02 15:55:39.033884 UTC] Start collecting samples
[2018-07-02 15:55:39.310647 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:39.332047 UTC] Computing policy gradient
[2018-07-02 15:55:39.341367 UTC] Updating baseline
[2018-07-02 15:55:39.441857 UTC] Computing logging information
-----------------------------------
| Iteration            | 4        |
| SurrLoss             | -0.01343 |
| Entropy              | 0.52271  |
| Perplexity           | 1.6866   |
| AveragePolicyProb[0] | 0.49949  |
| AveragePolicyProb[1] | 0.50051  |
| AverageReturn        | 68.93    |
| MinReturn            | 10       |
| MaxReturn            | 200      |
| StdReturn            | 52.911   |
| AverageEpisodeLength | 68.93    |
| MinEpisodeLength     | 10       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 52.911   |
| TotalNEpisodes       | 173      |
| TotalNSamples        | 8606     |
| ExplainedVariance    | 0.76978  |
-----------------------------------
[2018-07-02 15:55:40.218648 UTC] Saving snapshot
[2018-07-02 15:55:40.229323 UTC] Starting iteration 5
[2018-07-02 15:55:40.229973 UTC] Start collecting samples
[2018-07-02 15:55:40.481849 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:40.501681 UTC] Computing policy gradient
[2018-07-02 15:55:40.510980 UTC] Updating baseline
[2018-07-02 15:55:40.596406 UTC] Computing logging information
------------------------------------
| Iteration            | 5         |
| SurrLoss             | -0.011786 |
| Entropy              | 0.48944   |
| Perplexity           | 1.6314    |
| AveragePolicyProb[0] | 0.50122   |
| AveragePolicyProb[1] | 0.49878   |
| AverageReturn        | 84.48     |
| MinReturn            | 16        |
| MaxReturn            | 200       |
| StdReturn            | 59.894    |
| AverageEpisodeLength | 84.48     |
| MinEpisodeLength     | 16        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 59.894    |
| TotalNEpisodes       | 183       |
| TotalNSamples        | 10391     |
| ExplainedVariance    | 0.72004   |
------------------------------------
[2018-07-02 15:55:41.325871 UTC] Saving snapshot
[2018-07-02 15:55:41.334552 UTC] Starting iteration 6
[2018-07-02 15:55:41.335600 UTC] Start collecting samples
[2018-07-02 15:55:41.662665 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:41.681872 UTC] Computing policy gradient
[2018-07-02 15:55:41.689464 UTC] Updating baseline
[2018-07-02 15:55:41.771888 UTC] Computing logging information
------------------------------------
| Iteration            | 6         |
| SurrLoss             | -0.023091 |
| Entropy              | 0.45278   |
| Perplexity           | 1.5727    |
| AveragePolicyProb[0] | 0.492     |
| AveragePolicyProb[1] | 0.508     |
| AverageReturn        | 102.91    |
| MinReturn            | 18        |
| MaxReturn            | 200       |
| StdReturn            | 62.442    |
| AverageEpisodeLength | 102.91    |
| MinEpisodeLength     | 18        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 62.442    |
| TotalNEpisodes       | 197       |
| TotalNSamples        | 12648     |
| ExplainedVariance    | 0.67661   |
------------------------------------
[2018-07-02 15:55:42.554633 UTC] Saving snapshot
[2018-07-02 15:55:42.564209 UTC] Starting iteration 7
[2018-07-02 15:55:42.564776 UTC] Start collecting samples
[2018-07-02 15:55:42.849793 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:42.869436 UTC] Computing policy gradient
[2018-07-02 15:55:42.877267 UTC] Updating baseline
[2018-07-02 15:55:42.959894 UTC] Computing logging information
------------------------------------
| Iteration            | 7         |
| SurrLoss             | -0.016464 |
| Entropy              | 0.42004   |
| Perplexity           | 1.522     |
| AveragePolicyProb[0] | 0.50509   |
| AveragePolicyProb[1] | 0.49491   |
| AverageReturn        | 119.87    |
| MinReturn            | 18        |
| MaxReturn            | 200       |
| StdReturn            | 61.119    |
| AverageEpisodeLength | 119.87    |
| MinEpisodeLength     | 18        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 61.119    |
| TotalNEpisodes       | 211       |
| TotalNSamples        | 14932     |
| ExplainedVariance    | 0.67818   |
------------------------------------
[2018-07-02 15:55:43.719996 UTC] Saving snapshot
[2018-07-02 15:55:43.730939 UTC] Starting iteration 8
[2018-07-02 15:55:43.731631 UTC] Start collecting samples
[2018-07-02 15:55:44.014694 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:44.029977 UTC] Computing policy gradient
[2018-07-02 15:55:44.038267 UTC] Updating baseline
[2018-07-02 15:55:44.191339 UTC] Computing logging information
------------------------------------
| Iteration            | 8         |
| SurrLoss             | 0.0022306 |
| Entropy              | 0.3878    |
| Perplexity           | 1.4737    |
| AveragePolicyProb[0] | 0.50556   |
| AveragePolicyProb[1] | 0.49444   |
| AverageReturn        | 128.76    |
| MinReturn            | 29        |
| MaxReturn            | 200       |
| StdReturn            | 58.596    |
| AverageEpisodeLength | 128.76    |
| MinEpisodeLength     | 29        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 58.596    |
| TotalNEpisodes       | 219       |
| TotalNSamples        | 16195     |
| ExplainedVariance    | 0.76633   |
------------------------------------
[2018-07-02 15:55:44.922827 UTC] Saving snapshot
[2018-07-02 15:55:44.931548 UTC] Starting iteration 9
[2018-07-02 15:55:44.932341 UTC] Start collecting samples
[2018-07-02 15:55:45.165385 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:45.181750 UTC] Computing policy gradient
[2018-07-02 15:55:45.191609 UTC] Updating baseline
[2018-07-02 15:55:45.275843 UTC] Computing logging information
-------------------------------------
| Iteration            | 9          |
| SurrLoss             | -0.0028893 |
| Entropy              | 0.36246    |
| Perplexity           | 1.4369     |
| AveragePolicyProb[0] | 0.5021     |
| AveragePolicyProb[1] | 0.4979     |
| AverageReturn        | 142.68     |
| MinReturn            | 29         |
| MaxReturn            | 200        |
| StdReturn            | 54.707     |
| AverageEpisodeLength | 142.68     |
| MinEpisodeLength     | 29         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 54.707     |
| TotalNEpisodes       | 230        |
| TotalNSamples        | 18204      |
| ExplainedVariance    | 0.82493    |
-------------------------------------
[2018-07-02 15:55:46.073371 UTC] Saving snapshot
[2018-07-02 15:55:46.083411 UTC] Starting iteration 10
[2018-07-02 15:55:46.084100 UTC] Start collecting samples
[2018-07-02 15:55:46.386850 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:46.408939 UTC] Computing policy gradient
[2018-07-02 15:55:46.417138 UTC] Updating baseline
[2018-07-02 15:55:46.507832 UTC] Computing logging information
-----------------------------------
| Iteration            | 10       |
| SurrLoss             | 0.014146 |
| Entropy              | 0.33789  |
| Perplexity           | 1.402    |
| AveragePolicyProb[0] | 0.51755  |
| AveragePolicyProb[1] | 0.48245  |
| AverageReturn        | 161.44   |
| MinReturn            | 33       |
| MaxReturn            | 200      |
| StdReturn            | 45.801   |
| AverageEpisodeLength | 161.44   |
| MinEpisodeLength     | 33       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 45.801   |
| TotalNEpisodes       | 244      |
| TotalNSamples        | 20963    |
| ExplainedVariance    | 0.68108  |
-----------------------------------
[2018-07-02 15:55:47.261986 UTC] Saving snapshot
[2018-07-02 15:55:47.271219 UTC] Starting iteration 11
[2018-07-02 15:55:47.271880 UTC] Start collecting samples
[2018-07-02 15:55:47.532873 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:47.550179 UTC] Computing policy gradient
[2018-07-02 15:55:47.561065 UTC] Updating baseline
[2018-07-02 15:55:47.646059 UTC] Computing logging information
------------------------------------
| Iteration            | 11        |
| SurrLoss             | -0.010883 |
| Entropy              | 0.32872   |
| Perplexity           | 1.3892    |
| AveragePolicyProb[0] | 0.50891   |
| AveragePolicyProb[1] | 0.49109   |
| AverageReturn        | 168.99    |
| MinReturn            | 64        |
| MaxReturn            | 200       |
| StdReturn            | 38.386    |
| AverageEpisodeLength | 168.99    |
| MinEpisodeLength     | 64        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 38.386    |
| TotalNEpisodes       | 250       |
| TotalNSamples        | 22163     |
| ExplainedVariance    | 0.91311   |
------------------------------------
[2018-07-02 15:55:48.451318 UTC] Saving snapshot
[2018-07-02 15:55:48.460813 UTC] Starting iteration 12
[2018-07-02 15:55:48.461637 UTC] Start collecting samples
[2018-07-02 15:55:48.747517 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:48.767202 UTC] Computing policy gradient
[2018-07-02 15:55:48.778387 UTC] Updating baseline
[2018-07-02 15:55:48.868487 UTC] Computing logging information
-----------------------------------
| Iteration            | 12       |
| SurrLoss             | 0.020091 |
| Entropy              | 0.30299  |
| Perplexity           | 1.3539   |
| AveragePolicyProb[0] | 0.50256  |
| AveragePolicyProb[1] | 0.49744  |
| AverageReturn        | 175.57   |
| MinReturn            | 64       |
| MaxReturn            | 200      |
| StdReturn            | 35.272   |
| AverageEpisodeLength | 175.57   |
| MinEpisodeLength     | 64       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 35.272   |
| TotalNEpisodes       | 260      |
| TotalNSamples        | 24140    |
| ExplainedVariance    | 0.56413  |
-----------------------------------
[2018-07-02 15:55:49.695597 UTC] Saving snapshot
[2018-07-02 15:55:49.705197 UTC] Starting iteration 13
[2018-07-02 15:55:49.706448 UTC] Start collecting samples
[2018-07-02 15:55:50.075638 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:50.099676 UTC] Computing policy gradient
[2018-07-02 15:55:50.110525 UTC] Updating baseline
[2018-07-02 15:55:50.206776 UTC] Computing logging information
-----------------------------------
| Iteration            | 13       |
| SurrLoss             | 0.013758 |
| Entropy              | 0.29758  |
| Perplexity           | 1.3466   |
| AveragePolicyProb[0] | 0.51499  |
| AveragePolicyProb[1] | 0.48501  |
| AverageReturn        | 179.78   |
| MinReturn            | 80       |
| MaxReturn            | 200      |
| StdReturn            | 31.141   |
| AverageEpisodeLength | 179.78   |
| MinEpisodeLength     | 80       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 31.141   |
| TotalNEpisodes       | 273      |
| TotalNSamples        | 26584    |
| ExplainedVariance    | 0.77148  |
-----------------------------------
[2018-07-02 15:55:51.164996 UTC] Saving snapshot
[2018-07-02 15:55:51.175708 UTC] Starting iteration 14
[2018-07-02 15:55:51.176707 UTC] Start collecting samples
[2018-07-02 15:55:51.476674 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:51.500661 UTC] Computing policy gradient
[2018-07-02 15:55:51.509484 UTC] Updating baseline
[2018-07-02 15:55:51.586027 UTC] Computing logging information
------------------------------------
| Iteration            | 14        |
| SurrLoss             | 0.0056843 |
| Entropy              | 0.29567   |
| Perplexity           | 1.344     |
| AveragePolicyProb[0] | 0.53547   |
| AveragePolicyProb[1] | 0.46453   |
| AverageReturn        | 180.73    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 30.907    |
| AverageEpisodeLength | 180.73    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 30.907    |
| TotalNEpisodes       | 284       |
| TotalNSamples        | 28630     |
| ExplainedVariance    | 0.81334   |
------------------------------------
[2018-07-02 15:55:52.348780 UTC] Saving snapshot
[2018-07-02 15:55:52.359714 UTC] Starting iteration 15
[2018-07-02 15:55:52.360359 UTC] Start collecting samples
[2018-07-02 15:55:52.641613 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:52.658667 UTC] Computing policy gradient
[2018-07-02 15:55:52.667420 UTC] Updating baseline
[2018-07-02 15:55:52.746025 UTC] Computing logging information
------------------------------------
| Iteration            | 15        |
| SurrLoss             | 0.0039076 |
| Entropy              | 0.29026   |
| Perplexity           | 1.3368    |
| AveragePolicyProb[0] | 0.53314   |
| AveragePolicyProb[1] | 0.46686   |
| AverageReturn        | 182.4     |
| MinReturn            | 84        |
| MaxReturn            | 200       |
| StdReturn            | 26.478    |
| AverageEpisodeLength | 182.4     |
| MinEpisodeLength     | 84        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 26.478    |
| TotalNEpisodes       | 296       |
| TotalNSamples        | 30720     |
| ExplainedVariance    | 0.89693   |
------------------------------------
[2018-07-02 15:55:53.633097 UTC] Saving snapshot
[2018-07-02 15:55:53.647046 UTC] Starting iteration 16
[2018-07-02 15:55:53.648041 UTC] Start collecting samples
[2018-07-02 15:55:53.943431 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:53.967986 UTC] Computing policy gradient
[2018-07-02 15:55:53.975536 UTC] Updating baseline
[2018-07-02 15:55:54.060608 UTC] Computing logging information
------------------------------------
| Iteration            | 16        |
| SurrLoss             | -0.018331 |
| Entropy              | 0.29744   |
| Perplexity           | 1.3464    |
| AveragePolicyProb[0] | 0.51573   |
| AveragePolicyProb[1] | 0.48427   |
| AverageReturn        | 185.31    |
| MinReturn            | 84        |
| MaxReturn            | 200       |
| StdReturn            | 22.824    |
| AverageEpisodeLength | 185.31    |
| MinEpisodeLength     | 84        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 22.824    |
| TotalNEpisodes       | 307       |
| TotalNSamples        | 32743     |
| ExplainedVariance    | 0.88666   |
------------------------------------
[2018-07-02 15:55:54.896198 UTC] Saving snapshot
[2018-07-02 15:55:54.906878 UTC] Starting iteration 17
[2018-07-02 15:55:54.907844 UTC] Start collecting samples
[2018-07-02 15:55:55.199626 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:55.217973 UTC] Computing policy gradient
[2018-07-02 15:55:55.225459 UTC] Updating baseline
[2018-07-02 15:55:55.304900 UTC] Computing logging information
------------------------------------
| Iteration            | 17        |
| SurrLoss             | -0.023674 |
| Entropy              | 0.29992   |
| Perplexity           | 1.3498    |
| AveragePolicyProb[0] | 0.49612   |
| AveragePolicyProb[1] | 0.50388   |
| AverageReturn        | 188.4     |
| MinReturn            | 106       |
| MaxReturn            | 200       |
| StdReturn            | 19.282    |
| AverageEpisodeLength | 188.4     |
| MinEpisodeLength     | 106       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 19.282    |
| TotalNEpisodes       | 315       |
| TotalNSamples        | 34343     |
| ExplainedVariance    | 0.73718   |
------------------------------------
[2018-07-02 15:55:56.284258 UTC] Saving snapshot
[2018-07-02 15:55:56.296939 UTC] Starting iteration 18
[2018-07-02 15:55:56.297778 UTC] Start collecting samples
[2018-07-02 15:55:56.555359 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:56.569614 UTC] Computing policy gradient
[2018-07-02 15:55:56.578992 UTC] Updating baseline
[2018-07-02 15:55:56.668929 UTC] Computing logging information
------------------------------------
| Iteration            | 18        |
| SurrLoss             | 0.0011825 |
| Entropy              | 0.28939   |
| Perplexity           | 1.3356    |
| AveragePolicyProb[0] | 0.49316   |
| AveragePolicyProb[1] | 0.50684   |
| AverageReturn        | 190.43    |
| MinReturn            | 131       |
| MaxReturn            | 200       |
| StdReturn            | 17.027    |
| AverageEpisodeLength | 190.43    |
| MinEpisodeLength     | 131       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 17.027    |
| TotalNEpisodes       | 324       |
| TotalNSamples        | 36143     |
| ExplainedVariance    | 0.57379   |
------------------------------------
[2018-07-02 15:55:57.568324 UTC] Saving snapshot
[2018-07-02 15:55:57.586676 UTC] Starting iteration 19
[2018-07-02 15:55:57.588583 UTC] Start collecting samples
[2018-07-02 15:55:57.918154 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:57.943377 UTC] Computing policy gradient
[2018-07-02 15:55:57.952708 UTC] Updating baseline
[2018-07-02 15:55:58.047713 UTC] Computing logging information
-------------------------------------
| Iteration            | 19         |
| SurrLoss             | -0.0085643 |
| Entropy              | 0.28966    |
| Perplexity           | 1.336      |
| AveragePolicyProb[0] | 0.50282    |
| AveragePolicyProb[1] | 0.49718    |
| AverageReturn        | 191.76     |
| MinReturn            | 144        |
| MaxReturn            | 200        |
| StdReturn            | 15.87      |
| AverageEpisodeLength | 191.76     |
| MinEpisodeLength     | 144        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.87      |
| TotalNEpisodes       | 336        |
| TotalNSamples        | 38543      |
| ExplainedVariance    | 0.355      |
-------------------------------------
[2018-07-02 15:55:58.985867 UTC] Saving snapshot
[2018-07-02 15:55:58.997240 UTC] Starting iteration 20
[2018-07-02 15:55:58.998539 UTC] Start collecting samples
[2018-07-02 15:55:59.272871 UTC] Computing input variables for policy optimization
[2018-07-02 15:55:59.290821 UTC] Computing policy gradient
[2018-07-02 15:55:59.300352 UTC] Updating baseline
[2018-07-02 15:55:59.408826 UTC] Computing logging information
--------------------------------------
| Iteration            | 20          |
| SurrLoss             | -0.00011771 |
| Entropy              | 0.27784     |
| Perplexity           | 1.3203      |
| AveragePolicyProb[0] | 0.49682     |
| AveragePolicyProb[1] | 0.50318     |
| AverageReturn        | 191.8       |
| MinReturn            | 144         |
| MaxReturn            | 200         |
| StdReturn            | 15.886      |
| AverageEpisodeLength | 191.8       |
| MinEpisodeLength     | 144         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 15.886      |
| TotalNEpisodes       | 344         |
| TotalNSamples        | 40143       |
| ExplainedVariance    | 0.58615     |
--------------------------------------
[2018-07-02 15:56:00.168881 UTC] Saving snapshot
[2018-07-02 15:56:00.179009 UTC] Starting iteration 21
[2018-07-02 15:56:00.180195 UTC] Start collecting samples
[2018-07-02 15:56:00.456381 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:00.472835 UTC] Computing policy gradient
[2018-07-02 15:56:00.482589 UTC] Updating baseline
[2018-07-02 15:56:00.567317 UTC] Computing logging information
-------------------------------------
| Iteration            | 21         |
| SurrLoss             | -0.0073582 |
| Entropy              | 0.2709     |
| Perplexity           | 1.3111     |
| AveragePolicyProb[0] | 0.5097     |
| AveragePolicyProb[1] | 0.49031    |
| AverageReturn        | 191.95     |
| MinReturn            | 144        |
| MaxReturn            | 200        |
| StdReturn            | 15.892     |
| AverageEpisodeLength | 191.95     |
| MinEpisodeLength     | 144        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.892     |
| TotalNEpisodes       | 356        |
| TotalNSamples        | 42543      |
| ExplainedVariance    | 0.11105    |
-------------------------------------
[2018-07-02 15:56:01.325763 UTC] Saving snapshot
[2018-07-02 15:56:01.336505 UTC] Starting iteration 22
[2018-07-02 15:56:01.337144 UTC] Start collecting samples
[2018-07-02 15:56:01.596345 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:01.611142 UTC] Computing policy gradient
[2018-07-02 15:56:01.620764 UTC] Updating baseline
[2018-07-02 15:56:01.702015 UTC] Computing logging information
-------------------------------------
| Iteration            | 22         |
| SurrLoss             | 6.8436e-06 |
| Entropy              | 0.2683     |
| Perplexity           | 1.3077     |
| AveragePolicyProb[0] | 0.50357    |
| AveragePolicyProb[1] | 0.49643    |
| AverageReturn        | 192.03     |
| MinReturn            | 144        |
| MaxReturn            | 200        |
| StdReturn            | 15.912     |
| AverageEpisodeLength | 192.03     |
| MinEpisodeLength     | 144        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.912     |
| TotalNEpisodes       | 365        |
| TotalNSamples        | 44343      |
| ExplainedVariance    | 0.45192    |
-------------------------------------
[2018-07-02 15:56:02.732891 UTC] Saving snapshot
[2018-07-02 15:56:02.749561 UTC] Starting iteration 23
[2018-07-02 15:56:02.751198 UTC] Start collecting samples
[2018-07-02 15:56:03.230070 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:03.287547 UTC] Computing policy gradient
[2018-07-02 15:56:03.300327 UTC] Updating baseline
[2018-07-02 15:56:03.537396 UTC] Computing logging information
------------------------------------
| Iteration            | 23        |
| SurrLoss             | 0.0052665 |
| Entropy              | 0.27096   |
| Perplexity           | 1.3112    |
| AveragePolicyProb[0] | 0.51269   |
| AveragePolicyProb[1] | 0.48731   |
| AverageReturn        | 193.59    |
| MinReturn            | 144       |
| MaxReturn            | 200       |
| StdReturn            | 14.559    |
| AverageEpisodeLength | 193.59    |
| MinEpisodeLength     | 144       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 14.559    |
| TotalNEpisodes       | 376       |
| TotalNSamples        | 46543     |
| ExplainedVariance    | 0.27349   |
------------------------------------
[2018-07-02 15:56:04.629905 UTC] Saving snapshot
[2018-07-02 15:56:04.642454 UTC] Starting iteration 24
[2018-07-02 15:56:04.643349 UTC] Start collecting samples
[2018-07-02 15:56:04.923928 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:04.943546 UTC] Computing policy gradient
[2018-07-02 15:56:04.956014 UTC] Updating baseline
[2018-07-02 15:56:05.039531 UTC] Computing logging information
------------------------------------
| Iteration            | 24        |
| SurrLoss             | 0.0009762 |
| Entropy              | 0.259     |
| Perplexity           | 1.2956    |
| AveragePolicyProb[0] | 0.50239   |
| AveragePolicyProb[1] | 0.49761   |
| AverageReturn        | 195.97    |
| MinReturn            | 144       |
| MaxReturn            | 200       |
| StdReturn            | 11.851    |
| AverageEpisodeLength | 195.97    |
| MinEpisodeLength     | 144       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 11.851    |
| TotalNEpisodes       | 387       |
| TotalNSamples        | 48743     |
| ExplainedVariance    | 0.27557   |
------------------------------------
[2018-07-02 15:56:05.940354 UTC] Saving snapshot
[2018-07-02 15:56:05.967675 UTC] Starting iteration 25
[2018-07-02 15:56:05.968585 UTC] Start collecting samples
[2018-07-02 15:56:06.823620 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:06.958389 UTC] Computing policy gradient
[2018-07-02 15:56:06.989131 UTC] Updating baseline
[2018-07-02 15:56:07.360088 UTC] Computing logging information
------------------------------------
| Iteration            | 25        |
| SurrLoss             | -0.032023 |
| Entropy              | 0.26588   |
| Perplexity           | 1.3046    |
| AveragePolicyProb[0] | 0.49839   |
| AveragePolicyProb[1] | 0.50161   |
| AverageReturn        | 197.67    |
| MinReturn            | 144       |
| MaxReturn            | 200       |
| StdReturn            | 9.0896    |
| AverageEpisodeLength | 197.67    |
| MinEpisodeLength     | 144       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 9.0896    |
| TotalNEpisodes       | 395       |
| TotalNSamples        | 50343     |
| ExplainedVariance    | 0.40014   |
------------------------------------
[2018-07-02 15:56:08.346994 UTC] Saving snapshot
[2018-07-02 15:56:08.363845 UTC] Starting iteration 26
[2018-07-02 15:56:08.365210 UTC] Start collecting samples
[2018-07-02 15:56:08.927588 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:08.989568 UTC] Computing policy gradient
[2018-07-02 15:56:09.012491 UTC] Updating baseline
[2018-07-02 15:56:09.376101 UTC] Computing logging information
------------------------------------
| Iteration            | 26        |
| SurrLoss             | -0.022943 |
| Entropy              | 0.25296   |
| Perplexity           | 1.2878    |
| AveragePolicyProb[0] | 0.49816   |
| AveragePolicyProb[1] | 0.50184   |
| AverageReturn        | 199.88    |
| MinReturn            | 188       |
| MaxReturn            | 200       |
| StdReturn            | 1.194     |
| AverageEpisodeLength | 199.88    |
| MinEpisodeLength     | 188       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 1.194     |
| TotalNEpisodes       | 404       |
| TotalNSamples        | 52143     |
| ExplainedVariance    | 0.076352  |
------------------------------------
[2018-07-02 15:56:10.514369 UTC] Saving snapshot
[2018-07-02 15:56:10.532404 UTC] Starting iteration 27
[2018-07-02 15:56:10.533300 UTC] Start collecting samples
[2018-07-02 15:56:11.163236 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:11.218777 UTC] Computing policy gradient
[2018-07-02 15:56:11.246062 UTC] Updating baseline
[2018-07-02 15:56:11.534279 UTC] Computing logging information
-------------------------------------
| Iteration            | 27         |
| SurrLoss             | -0.0095373 |
| Entropy              | 0.24784    |
| Perplexity           | 1.2813     |
| AveragePolicyProb[0] | 0.51072    |
| AveragePolicyProb[1] | 0.48928    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 416        |
| TotalNSamples        | 54543      |
| ExplainedVariance    | 0.27292    |
-------------------------------------
[2018-07-02 15:56:12.780122 UTC] Saving snapshot
[2018-07-02 15:56:12.795802 UTC] Starting iteration 28
[2018-07-02 15:56:12.798112 UTC] Start collecting samples
[2018-07-02 15:56:13.682598 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:13.753756 UTC] Computing policy gradient
[2018-07-02 15:56:13.782324 UTC] Updating baseline
[2018-07-02 15:56:14.229334 UTC] Computing logging information
------------------------------------
| Iteration            | 28        |
| SurrLoss             | -0.015847 |
| Entropy              | 0.24704   |
| Perplexity           | 1.2802    |
| AveragePolicyProb[0] | 0.49776   |
| AveragePolicyProb[1] | 0.50224   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 424       |
| TotalNSamples        | 56143     |
| ExplainedVariance    | 0.47993   |
------------------------------------
[2018-07-02 15:56:15.469394 UTC] Saving snapshot
[2018-07-02 15:56:15.492710 UTC] Starting iteration 29
[2018-07-02 15:56:15.493771 UTC] Start collecting samples
[2018-07-02 15:56:16.170526 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:16.250536 UTC] Computing policy gradient
[2018-07-02 15:56:16.291439 UTC] Updating baseline
[2018-07-02 15:56:16.767679 UTC] Computing logging information
------------------------------------
| Iteration            | 29        |
| SurrLoss             | 0.0075635 |
| Entropy              | 0.24616   |
| Perplexity           | 1.2791    |
| AveragePolicyProb[0] | 0.50721   |
| AveragePolicyProb[1] | 0.49279   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 436       |
| TotalNSamples        | 58543     |
| ExplainedVariance    | 0.23601   |
------------------------------------
[2018-07-02 15:56:18.013816 UTC] Saving snapshot
[2018-07-02 15:56:18.026126 UTC] Starting iteration 30
[2018-07-02 15:56:18.027762 UTC] Start collecting samples
[2018-07-02 15:56:19.041945 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:19.171954 UTC] Computing policy gradient
[2018-07-02 15:56:19.213896 UTC] Updating baseline
[2018-07-02 15:56:19.735432 UTC] Computing logging information
-----------------------------------
| Iteration            | 30       |
| SurrLoss             | 0.005787 |
| Entropy              | 0.22823  |
| Perplexity           | 1.2564   |
| AveragePolicyProb[0] | 0.49127  |
| AveragePolicyProb[1] | 0.50873  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 445      |
| TotalNSamples        | 60343    |
| ExplainedVariance    | 0.29669  |
-----------------------------------
[2018-07-02 15:56:20.872817 UTC] Saving snapshot
[2018-07-02 15:56:20.883609 UTC] Starting iteration 31
[2018-07-02 15:56:20.884553 UTC] Start collecting samples
[2018-07-02 15:56:21.875117 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:21.939873 UTC] Computing policy gradient
[2018-07-02 15:56:21.962386 UTC] Updating baseline
[2018-07-02 15:56:22.312713 UTC] Computing logging information
-------------------------------------
| Iteration            | 31         |
| SurrLoss             | -0.0040467 |
| Entropy              | 0.23133    |
| Perplexity           | 1.2603     |
| AveragePolicyProb[0] | 0.51008    |
| AveragePolicyProb[1] | 0.48992    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 456        |
| TotalNSamples        | 62543      |
| ExplainedVariance    | 0.28968    |
-------------------------------------
[2018-07-02 15:56:23.574415 UTC] Saving snapshot
[2018-07-02 15:56:23.586804 UTC] Starting iteration 32
[2018-07-02 15:56:23.587923 UTC] Start collecting samples
[2018-07-02 15:56:24.493041 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:24.539541 UTC] Computing policy gradient
[2018-07-02 15:56:24.547995 UTC] Updating baseline
[2018-07-02 15:56:24.675943 UTC] Computing logging information
------------------------------------
| Iteration            | 32        |
| SurrLoss             | 0.0025263 |
| Entropy              | 0.22913   |
| Perplexity           | 1.2575    |
| AveragePolicyProb[0] | 0.50072   |
| AveragePolicyProb[1] | 0.49928   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 467       |
| TotalNSamples        | 64743     |
| ExplainedVariance    | 0.33729   |
------------------------------------
[2018-07-02 15:56:25.662025 UTC] Saving snapshot
[2018-07-02 15:56:25.672146 UTC] Starting iteration 33
[2018-07-02 15:56:25.673511 UTC] Start collecting samples
[2018-07-02 15:56:25.973090 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:25.992442 UTC] Computing policy gradient
[2018-07-02 15:56:26.002701 UTC] Updating baseline
[2018-07-02 15:56:26.097665 UTC] Computing logging information
-------------------------------------
| Iteration            | 33         |
| SurrLoss             | -0.0079408 |
| Entropy              | 0.22302    |
| Perplexity           | 1.2498     |
| AveragePolicyProb[0] | 0.49831    |
| AveragePolicyProb[1] | 0.50169    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 475        |
| TotalNSamples        | 66343      |
| ExplainedVariance    | 0.58555    |
-------------------------------------
[2018-07-02 15:56:27.295664 UTC] Saving snapshot
[2018-07-02 15:56:27.306304 UTC] Starting iteration 34
[2018-07-02 15:56:27.307290 UTC] Start collecting samples
[2018-07-02 15:56:27.937181 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:27.974849 UTC] Computing policy gradient
[2018-07-02 15:56:27.996257 UTC] Updating baseline
[2018-07-02 15:56:28.185851 UTC] Computing logging information
------------------------------------
| Iteration            | 34        |
| SurrLoss             | 0.0071279 |
| Entropy              | 0.22612   |
| Perplexity           | 1.2537    |
| AveragePolicyProb[0] | 0.4943    |
| AveragePolicyProb[1] | 0.5057    |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 484       |
| TotalNSamples        | 68143     |
| ExplainedVariance    | 0.44132   |
------------------------------------
[2018-07-02 15:56:29.210811 UTC] Saving snapshot
[2018-07-02 15:56:29.221888 UTC] Starting iteration 35
[2018-07-02 15:56:29.222878 UTC] Start collecting samples
[2018-07-02 15:56:29.796630 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:29.816415 UTC] Computing policy gradient
[2018-07-02 15:56:29.824526 UTC] Updating baseline
[2018-07-02 15:56:29.913302 UTC] Computing logging information
-----------------------------------
| Iteration            | 35       |
| SurrLoss             | 0.002427 |
| Entropy              | 0.20857  |
| Perplexity           | 1.2319   |
| AveragePolicyProb[0] | 0.49903  |
| AveragePolicyProb[1] | 0.50097  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 496      |
| TotalNSamples        | 70543    |
| ExplainedVariance    | 0.38423  |
-----------------------------------
[2018-07-02 15:56:30.634564 UTC] Saving snapshot
[2018-07-02 15:56:30.644021 UTC] Starting iteration 36
[2018-07-02 15:56:30.645456 UTC] Start collecting samples
[2018-07-02 15:56:30.862555 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:30.875871 UTC] Computing policy gradient
[2018-07-02 15:56:30.885041 UTC] Updating baseline
[2018-07-02 15:56:30.966320 UTC] Computing logging information
-----------------------------------
| Iteration            | 36       |
| SurrLoss             | 0.002791 |
| Entropy              | 0.2229   |
| Perplexity           | 1.2497   |
| AveragePolicyProb[0] | 0.49724  |
| AveragePolicyProb[1] | 0.50276  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 504      |
| TotalNSamples        | 72143    |
| ExplainedVariance    | 0.49591  |
-----------------------------------
[2018-07-02 15:56:31.752992 UTC] Saving snapshot
[2018-07-02 15:56:31.762744 UTC] Starting iteration 37
[2018-07-02 15:56:31.764643 UTC] Start collecting samples
[2018-07-02 15:56:32.075976 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:32.098837 UTC] Computing policy gradient
[2018-07-02 15:56:32.106097 UTC] Updating baseline
[2018-07-02 15:56:32.199047 UTC] Computing logging information
-----------------------------------
| Iteration            | 37       |
| SurrLoss             | 0.008625 |
| Entropy              | 0.20529  |
| Perplexity           | 1.2279   |
| AveragePolicyProb[0] | 0.4921   |
| AveragePolicyProb[1] | 0.5079   |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 516      |
| TotalNSamples        | 74543    |
| ExplainedVariance    | 0.25107  |
-----------------------------------
[2018-07-02 15:56:33.205766 UTC] Saving snapshot
[2018-07-02 15:56:33.216985 UTC] Starting iteration 38
[2018-07-02 15:56:33.217526 UTC] Start collecting samples
[2018-07-02 15:56:33.583127 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:33.605229 UTC] Computing policy gradient
[2018-07-02 15:56:33.619205 UTC] Updating baseline
[2018-07-02 15:56:33.735766 UTC] Computing logging information
------------------------------------
| Iteration            | 38        |
| SurrLoss             | -0.010286 |
| Entropy              | 0.21922   |
| Perplexity           | 1.2451    |
| AveragePolicyProb[0] | 0.50004   |
| AveragePolicyProb[1] | 0.49996   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 525       |
| TotalNSamples        | 76343     |
| ExplainedVariance    | 0.44444   |
------------------------------------
[2018-07-02 15:56:34.620373 UTC] Saving snapshot
[2018-07-02 15:56:34.633489 UTC] Starting iteration 39
[2018-07-02 15:56:34.634624 UTC] Start collecting samples
[2018-07-02 15:56:34.966124 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:34.985196 UTC] Computing policy gradient
[2018-07-02 15:56:34.994514 UTC] Updating baseline
[2018-07-02 15:56:35.078115 UTC] Computing logging information
-----------------------------------
| Iteration            | 39       |
| SurrLoss             | 0.013774 |
| Entropy              | 0.21234  |
| Perplexity           | 1.2366   |
| AveragePolicyProb[0] | 0.50515  |
| AveragePolicyProb[1] | 0.49485  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 536      |
| TotalNSamples        | 78543    |
| ExplainedVariance    | 0.32796  |
-----------------------------------
[2018-07-02 15:56:35.867930 UTC] Saving snapshot
[2018-07-02 15:56:35.879963 UTC] Starting iteration 40
[2018-07-02 15:56:35.880741 UTC] Start collecting samples
[2018-07-02 15:56:36.250353 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:36.267492 UTC] Computing policy gradient
[2018-07-02 15:56:36.276804 UTC] Updating baseline
[2018-07-02 15:56:36.363394 UTC] Computing logging information
-------------------------------------
| Iteration            | 40         |
| SurrLoss             | -0.0053414 |
| Entropy              | 0.21783    |
| Perplexity           | 1.2434     |
| AveragePolicyProb[0] | 0.50383    |
| AveragePolicyProb[1] | 0.49617    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 547        |
| TotalNSamples        | 80743      |
| ExplainedVariance    | 0.56323    |
-------------------------------------
[2018-07-02 15:56:37.514398 UTC] Saving snapshot
[2018-07-02 15:56:37.524870 UTC] Starting iteration 41
[2018-07-02 15:56:37.526861 UTC] Start collecting samples
[2018-07-02 15:56:37.821923 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:37.839917 UTC] Computing policy gradient
[2018-07-02 15:56:37.849337 UTC] Updating baseline
[2018-07-02 15:56:37.933354 UTC] Computing logging information
-----------------------------------
| Iteration            | 41       |
| SurrLoss             | 0.013858 |
| Entropy              | 0.23469  |
| Perplexity           | 1.2645   |
| AveragePolicyProb[0] | 0.4953   |
| AveragePolicyProb[1] | 0.5047   |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 555      |
| TotalNSamples        | 82343    |
| ExplainedVariance    | 0.53998  |
-----------------------------------
[2018-07-02 15:56:38.804123 UTC] Saving snapshot
[2018-07-02 15:56:38.814000 UTC] Starting iteration 42
[2018-07-02 15:56:38.814871 UTC] Start collecting samples
[2018-07-02 15:56:39.090448 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:39.113106 UTC] Computing policy gradient
[2018-07-02 15:56:39.127696 UTC] Updating baseline
[2018-07-02 15:56:39.215733 UTC] Computing logging information
-----------------------------------
| Iteration            | 42       |
| SurrLoss             | 0.004802 |
| Entropy              | 0.22141  |
| Perplexity           | 1.2478   |
| AveragePolicyProb[0] | 0.49836  |
| AveragePolicyProb[1] | 0.50164  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 564      |
| TotalNSamples        | 84143    |
| ExplainedVariance    | 0.49174  |
-----------------------------------
[2018-07-02 15:56:39.862297 UTC] Saving snapshot
[2018-07-02 15:56:39.870979 UTC] Starting iteration 43
[2018-07-02 15:56:39.871716 UTC] Start collecting samples
[2018-07-02 15:56:40.182905 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:40.202169 UTC] Computing policy gradient
[2018-07-02 15:56:40.213146 UTC] Updating baseline
[2018-07-02 15:56:40.304454 UTC] Computing logging information
-----------------------------------
| Iteration            | 43       |
| SurrLoss             | 0.023474 |
| Entropy              | 0.22058  |
| Perplexity           | 1.2468   |
| AveragePolicyProb[0] | 0.50365  |
| AveragePolicyProb[1] | 0.49636  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 576      |
| TotalNSamples        | 86543    |
| ExplainedVariance    | 0.39104  |
-----------------------------------
[2018-07-02 15:56:41.033454 UTC] Saving snapshot
[2018-07-02 15:56:41.044128 UTC] Starting iteration 44
[2018-07-02 15:56:41.044837 UTC] Start collecting samples
[2018-07-02 15:56:41.320421 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:41.337147 UTC] Computing policy gradient
[2018-07-02 15:56:41.345711 UTC] Updating baseline
[2018-07-02 15:56:41.431449 UTC] Computing logging information
------------------------------------
| Iteration            | 44        |
| SurrLoss             | 0.0061807 |
| Entropy              | 0.22472   |
| Perplexity           | 1.252     |
| AveragePolicyProb[0] | 0.50292   |
| AveragePolicyProb[1] | 0.49708   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 584       |
| TotalNSamples        | 88143     |
| ExplainedVariance    | 0.48984   |
------------------------------------
[2018-07-02 15:56:42.159233 UTC] Saving snapshot
[2018-07-02 15:56:42.168657 UTC] Starting iteration 45
[2018-07-02 15:56:42.169226 UTC] Start collecting samples
[2018-07-02 15:56:42.489451 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:42.511614 UTC] Computing policy gradient
[2018-07-02 15:56:42.519234 UTC] Updating baseline
[2018-07-02 15:56:42.602413 UTC] Computing logging information
------------------------------------
| Iteration            | 45        |
| SurrLoss             | 0.0047472 |
| Entropy              | 0.24152   |
| Perplexity           | 1.2732    |
| AveragePolicyProb[0] | 0.49427   |
| AveragePolicyProb[1] | 0.50573   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 596       |
| TotalNSamples        | 90543     |
| ExplainedVariance    | 0.28464   |
------------------------------------
[2018-07-02 15:56:43.411057 UTC] Saving snapshot
[2018-07-02 15:56:43.422888 UTC] Starting iteration 46
[2018-07-02 15:56:43.425321 UTC] Start collecting samples
[2018-07-02 15:56:43.782345 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:43.802725 UTC] Computing policy gradient
[2018-07-02 15:56:43.813554 UTC] Updating baseline
[2018-07-02 15:56:43.907836 UTC] Computing logging information
------------------------------------
| Iteration            | 46        |
| SurrLoss             | -0.016625 |
| Entropy              | 0.25168   |
| Perplexity           | 1.2862    |
| AveragePolicyProb[0] | 0.50086   |
| AveragePolicyProb[1] | 0.49914   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 605       |
| TotalNSamples        | 92343     |
| ExplainedVariance    | 0.25357   |
------------------------------------
[2018-07-02 15:56:44.686373 UTC] Saving snapshot
[2018-07-02 15:56:44.696959 UTC] Starting iteration 47
[2018-07-02 15:56:44.697846 UTC] Start collecting samples
[2018-07-02 15:56:44.951505 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:44.967602 UTC] Computing policy gradient
[2018-07-02 15:56:44.978142 UTC] Updating baseline
[2018-07-02 15:56:45.057813 UTC] Computing logging information
------------------------------------
| Iteration            | 47        |
| SurrLoss             | -0.015742 |
| Entropy              | 0.25732   |
| Perplexity           | 1.2935    |
| AveragePolicyProb[0] | 0.49697   |
| AveragePolicyProb[1] | 0.50303   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 616       |
| TotalNSamples        | 94543     |
| ExplainedVariance    | 0.19877   |
------------------------------------
[2018-07-02 15:56:46.260601 UTC] Saving snapshot
[2018-07-02 15:56:46.273869 UTC] Starting iteration 48
[2018-07-02 15:56:46.275042 UTC] Start collecting samples
[2018-07-02 15:56:46.737834 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:46.753698 UTC] Computing policy gradient
[2018-07-02 15:56:46.766017 UTC] Updating baseline
[2018-07-02 15:56:46.854930 UTC] Computing logging information
--------------------------------------
| Iteration            | 48          |
| SurrLoss             | -0.00068493 |
| Entropy              | 0.25687     |
| Perplexity           | 1.2929      |
| AveragePolicyProb[0] | 0.50604     |
| AveragePolicyProb[1] | 0.49396     |
| AverageReturn        | 200         |
| MinReturn            | 200         |
| MaxReturn            | 200         |
| StdReturn            | 0           |
| AverageEpisodeLength | 200         |
| MinEpisodeLength     | 200         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 0           |
| TotalNEpisodes       | 627         |
| TotalNSamples        | 96743       |
| ExplainedVariance    | 0.32934     |
--------------------------------------
[2018-07-02 15:56:48.055565 UTC] Saving snapshot
[2018-07-02 15:56:48.069055 UTC] Starting iteration 49
[2018-07-02 15:56:48.070803 UTC] Start collecting samples
[2018-07-02 15:56:48.420973 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:48.441881 UTC] Computing policy gradient
[2018-07-02 15:56:48.453451 UTC] Updating baseline
[2018-07-02 15:56:48.561002 UTC] Computing logging information
------------------------------------
| Iteration            | 49        |
| SurrLoss             | 0.0078327 |
| Entropy              | 0.27114   |
| Perplexity           | 1.3115    |
| AveragePolicyProb[0] | 0.49124   |
| AveragePolicyProb[1] | 0.50876   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 635       |
| TotalNSamples        | 98343     |
| ExplainedVariance    | 0.55039   |
------------------------------------
[2018-07-02 15:56:49.409170 UTC] Saving snapshot
[2018-07-02 15:56:49.419857 UTC] Starting iteration 50
[2018-07-02 15:56:49.420696 UTC] Start collecting samples
[2018-07-02 15:56:49.720875 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:49.738256 UTC] Computing policy gradient
[2018-07-02 15:56:49.747076 UTC] Updating baseline
[2018-07-02 15:56:49.836462 UTC] Computing logging information
-------------------------------------
| Iteration            | 50         |
| SurrLoss             | -0.014792  |
| Entropy              | 0.28492    |
| Perplexity           | 1.3297     |
| AveragePolicyProb[0] | 0.50422    |
| AveragePolicyProb[1] | 0.49578    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 644        |
| TotalNSamples        | 1.0014e+05 |
| ExplainedVariance    | 0.35474    |
-------------------------------------
[2018-07-02 15:56:50.648433 UTC] Saving snapshot
[2018-07-02 15:56:50.659733 UTC] Starting iteration 51
[2018-07-02 15:56:50.660810 UTC] Start collecting samples
[2018-07-02 15:56:50.991859 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:51.011163 UTC] Computing policy gradient
[2018-07-02 15:56:51.022317 UTC] Updating baseline
[2018-07-02 15:56:51.164504 UTC] Computing logging information
-------------------------------------
| Iteration            | 51         |
| SurrLoss             | 0.0054623  |
| Entropy              | 0.28651    |
| Perplexity           | 1.3318     |
| AveragePolicyProb[0] | 0.50818    |
| AveragePolicyProb[1] | 0.49182    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 656        |
| TotalNSamples        | 1.0254e+05 |
| ExplainedVariance    | 0.27268    |
-------------------------------------
[2018-07-02 15:56:52.212852 UTC] Saving snapshot
[2018-07-02 15:56:52.222572 UTC] Starting iteration 52
[2018-07-02 15:56:52.223276 UTC] Start collecting samples
[2018-07-02 15:56:52.455222 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:52.474055 UTC] Computing policy gradient
[2018-07-02 15:56:52.481543 UTC] Updating baseline
[2018-07-02 15:56:52.612605 UTC] Computing logging information
-------------------------------------
| Iteration            | 52         |
| SurrLoss             | -0.029588  |
| Entropy              | 0.30086    |
| Perplexity           | 1.351      |
| AveragePolicyProb[0] | 0.50746    |
| AveragePolicyProb[1] | 0.49254    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 664        |
| TotalNSamples        | 1.0414e+05 |
| ExplainedVariance    | 0.17376    |
-------------------------------------
[2018-07-02 15:56:53.441255 UTC] Saving snapshot
[2018-07-02 15:56:53.453865 UTC] Starting iteration 53
[2018-07-02 15:56:53.454974 UTC] Start collecting samples
[2018-07-02 15:56:53.883592 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:53.904498 UTC] Computing policy gradient
[2018-07-02 15:56:53.914066 UTC] Updating baseline
[2018-07-02 15:56:53.992368 UTC] Computing logging information
-------------------------------------
| Iteration            | 53         |
| SurrLoss             | -0.015965  |
| Entropy              | 0.29288    |
| Perplexity           | 1.3403     |
| AveragePolicyProb[0] | 0.4939     |
| AveragePolicyProb[1] | 0.5061     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 676        |
| TotalNSamples        | 1.0654e+05 |
| ExplainedVariance    | 0.19981    |
-------------------------------------
[2018-07-02 15:56:54.875501 UTC] Saving snapshot
[2018-07-02 15:56:54.886897 UTC] Starting iteration 54
[2018-07-02 15:56:54.887750 UTC] Start collecting samples
[2018-07-02 15:56:55.165327 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:55.184987 UTC] Computing policy gradient
[2018-07-02 15:56:55.194940 UTC] Updating baseline
[2018-07-02 15:56:55.285767 UTC] Computing logging information
-------------------------------------
| Iteration            | 54         |
| SurrLoss             | -0.0065934 |
| Entropy              | 0.28847    |
| Perplexity           | 1.3344     |
| AveragePolicyProb[0] | 0.48555    |
| AveragePolicyProb[1] | 0.51445    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 685        |
| TotalNSamples        | 1.0834e+05 |
| ExplainedVariance    | 0.33892    |
-------------------------------------
[2018-07-02 15:56:56.132546 UTC] Saving snapshot
[2018-07-02 15:56:56.146284 UTC] Starting iteration 55
[2018-07-02 15:56:56.147192 UTC] Start collecting samples
[2018-07-02 15:56:56.436477 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:56.457389 UTC] Computing policy gradient
[2018-07-02 15:56:56.466629 UTC] Updating baseline
[2018-07-02 15:56:56.554953 UTC] Computing logging information
-------------------------------------
| Iteration            | 55         |
| SurrLoss             | 0.0058793  |
| Entropy              | 0.29937    |
| Perplexity           | 1.349      |
| AveragePolicyProb[0] | 0.51262    |
| AveragePolicyProb[1] | 0.48738    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 696        |
| TotalNSamples        | 1.1054e+05 |
| ExplainedVariance    | 0.092993   |
-------------------------------------
[2018-07-02 15:56:57.713339 UTC] Saving snapshot
[2018-07-02 15:56:57.723814 UTC] Starting iteration 56
[2018-07-02 15:56:57.725093 UTC] Start collecting samples
[2018-07-02 15:56:58.008948 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:58.032487 UTC] Computing policy gradient
[2018-07-02 15:56:58.041384 UTC] Updating baseline
[2018-07-02 15:56:58.146369 UTC] Computing logging information
-------------------------------------
| Iteration            | 56         |
| SurrLoss             | -0.0049614 |
| Entropy              | 0.29723    |
| Perplexity           | 1.3461     |
| AveragePolicyProb[0] | 0.48553    |
| AveragePolicyProb[1] | 0.51447    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 707        |
| TotalNSamples        | 1.1274e+05 |
| ExplainedVariance    | 0.31305    |
-------------------------------------
[2018-07-02 15:56:59.028244 UTC] Saving snapshot
[2018-07-02 15:56:59.040237 UTC] Starting iteration 57
[2018-07-02 15:56:59.041199 UTC] Start collecting samples
[2018-07-02 15:56:59.417795 UTC] Computing input variables for policy optimization
[2018-07-02 15:56:59.448512 UTC] Computing policy gradient
[2018-07-02 15:56:59.462566 UTC] Updating baseline
[2018-07-02 15:56:59.564460 UTC] Computing logging information
-------------------------------------
| Iteration            | 57         |
| SurrLoss             | -0.0026451 |
| Entropy              | 0.3004     |
| Perplexity           | 1.3504     |
| AveragePolicyProb[0] | 0.50657    |
| AveragePolicyProb[1] | 0.49343    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 715        |
| TotalNSamples        | 1.1434e+05 |
| ExplainedVariance    | 0.29851    |
-------------------------------------
[2018-07-02 15:57:00.404346 UTC] Saving snapshot
[2018-07-02 15:57:00.415616 UTC] Starting iteration 58
[2018-07-02 15:57:00.416245 UTC] Start collecting samples
[2018-07-02 15:57:00.720760 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:00.736272 UTC] Computing policy gradient
[2018-07-02 15:57:00.742907 UTC] Updating baseline
[2018-07-02 15:57:00.824736 UTC] Computing logging information
-------------------------------------
| Iteration            | 58         |
| SurrLoss             | -0.033169  |
| Entropy              | 0.30783    |
| Perplexity           | 1.3605     |
| AveragePolicyProb[0] | 0.50619    |
| AveragePolicyProb[1] | 0.49381    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 724        |
| TotalNSamples        | 1.1614e+05 |
| ExplainedVariance    | 0.37634    |
-------------------------------------
[2018-07-02 15:57:01.763806 UTC] Saving snapshot
[2018-07-02 15:57:01.772767 UTC] Starting iteration 59
[2018-07-02 15:57:01.773745 UTC] Start collecting samples
[2018-07-02 15:57:02.052411 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:02.074664 UTC] Computing policy gradient
[2018-07-02 15:57:02.084414 UTC] Updating baseline
[2018-07-02 15:57:02.165992 UTC] Computing logging information
-------------------------------------
| Iteration            | 59         |
| SurrLoss             | 0.01208    |
| Entropy              | 0.29629    |
| Perplexity           | 1.3449     |
| AveragePolicyProb[0] | 0.50863    |
| AveragePolicyProb[1] | 0.49137    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 736        |
| TotalNSamples        | 1.1854e+05 |
| ExplainedVariance    | 0.44955    |
-------------------------------------
[2018-07-02 15:57:03.273185 UTC] Saving snapshot
[2018-07-02 15:57:03.283003 UTC] Starting iteration 60
[2018-07-02 15:57:03.283852 UTC] Start collecting samples
[2018-07-02 15:57:03.628260 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:03.651051 UTC] Computing policy gradient
[2018-07-02 15:57:03.664662 UTC] Updating baseline
[2018-07-02 15:57:03.754710 UTC] Computing logging information
-------------------------------------
| Iteration            | 60         |
| SurrLoss             | 0.011792   |
| Entropy              | 0.30345    |
| Perplexity           | 1.3545     |
| AveragePolicyProb[0] | 0.49837    |
| AveragePolicyProb[1] | 0.50163    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 744        |
| TotalNSamples        | 1.2014e+05 |
| ExplainedVariance    | 0.67098    |
-------------------------------------
[2018-07-02 15:57:04.448102 UTC] Saving snapshot
[2018-07-02 15:57:04.457325 UTC] Starting iteration 61
[2018-07-02 15:57:04.457863 UTC] Start collecting samples
[2018-07-02 15:57:04.760742 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:04.775790 UTC] Computing policy gradient
[2018-07-02 15:57:04.783827 UTC] Updating baseline
[2018-07-02 15:57:04.860063 UTC] Computing logging information
-------------------------------------
| Iteration            | 61         |
| SurrLoss             | -0.004161  |
| Entropy              | 0.32662    |
| Perplexity           | 1.3863     |
| AveragePolicyProb[0] | 0.51158    |
| AveragePolicyProb[1] | 0.48842    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 756        |
| TotalNSamples        | 1.2254e+05 |
| ExplainedVariance    | 0.74187    |
-------------------------------------
[2018-07-02 15:57:05.712389 UTC] Saving snapshot
[2018-07-02 15:57:05.722382 UTC] Starting iteration 62
[2018-07-02 15:57:05.723805 UTC] Start collecting samples
[2018-07-02 15:57:05.996117 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:06.011492 UTC] Computing policy gradient
[2018-07-02 15:57:06.019673 UTC] Updating baseline
[2018-07-02 15:57:06.096947 UTC] Computing logging information
-------------------------------------
| Iteration            | 62         |
| SurrLoss             | -0.010567  |
| Entropy              | 0.33196    |
| Perplexity           | 1.3937     |
| AveragePolicyProb[0] | 0.5016     |
| AveragePolicyProb[1] | 0.4984     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 765        |
| TotalNSamples        | 1.2434e+05 |
| ExplainedVariance    | 0.73904    |
-------------------------------------
[2018-07-02 15:57:06.894025 UTC] Saving snapshot
[2018-07-02 15:57:06.904292 UTC] Starting iteration 63
[2018-07-02 15:57:06.905116 UTC] Start collecting samples
[2018-07-02 15:57:07.207869 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:07.229705 UTC] Computing policy gradient
[2018-07-02 15:57:07.238282 UTC] Updating baseline
[2018-07-02 15:57:07.319882 UTC] Computing logging information
-------------------------------------
| Iteration            | 63         |
| SurrLoss             | -0.0084199 |
| Entropy              | 0.32799    |
| Perplexity           | 1.3882     |
| AveragePolicyProb[0] | 0.50785    |
| AveragePolicyProb[1] | 0.49215    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 776        |
| TotalNSamples        | 1.2654e+05 |
| ExplainedVariance    | 0.60613    |
-------------------------------------
[2018-07-02 15:57:09.515146 UTC] Saving snapshot
[2018-07-02 15:57:09.526707 UTC] Starting iteration 64
[2018-07-02 15:57:09.527556 UTC] Start collecting samples
[2018-07-02 15:57:09.879974 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:09.904932 UTC] Computing policy gradient
[2018-07-02 15:57:09.916278 UTC] Updating baseline
[2018-07-02 15:57:10.006107 UTC] Computing logging information
-------------------------------------
| Iteration            | 64         |
| SurrLoss             | -0.015793  |
| Entropy              | 0.3336     |
| Perplexity           | 1.396      |
| AveragePolicyProb[0] | 0.50966    |
| AveragePolicyProb[1] | 0.49034    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 787        |
| TotalNSamples        | 1.2874e+05 |
| ExplainedVariance    | 0.58451    |
-------------------------------------
[2018-07-02 15:57:10.859998 UTC] Saving snapshot
[2018-07-02 15:57:10.869590 UTC] Starting iteration 65
[2018-07-02 15:57:10.870485 UTC] Start collecting samples
[2018-07-02 15:57:11.137837 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:11.151508 UTC] Computing policy gradient
[2018-07-02 15:57:11.158928 UTC] Updating baseline
[2018-07-02 15:57:11.247173 UTC] Computing logging information
-------------------------------------
| Iteration            | 65         |
| SurrLoss             | 0.0030379  |
| Entropy              | 0.33822    |
| Perplexity           | 1.4025     |
| AveragePolicyProb[0] | 0.48834    |
| AveragePolicyProb[1] | 0.51166    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 795        |
| TotalNSamples        | 1.3034e+05 |
| ExplainedVariance    | 0.43733    |
-------------------------------------
[2018-07-02 15:57:12.064313 UTC] Saving snapshot
[2018-07-02 15:57:12.074084 UTC] Starting iteration 66
[2018-07-02 15:57:12.074813 UTC] Start collecting samples
[2018-07-02 15:57:12.368479 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:12.387928 UTC] Computing policy gradient
[2018-07-02 15:57:12.399963 UTC] Updating baseline
[2018-07-02 15:57:12.513745 UTC] Computing logging information
-------------------------------------
| Iteration            | 66         |
| SurrLoss             | 0.0055713  |
| Entropy              | 0.34191    |
| Perplexity           | 1.4076     |
| AveragePolicyProb[0] | 0.50055    |
| AveragePolicyProb[1] | 0.49945    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 804        |
| TotalNSamples        | 1.3214e+05 |
| ExplainedVariance    | 0.44698    |
-------------------------------------
[2018-07-02 15:57:13.570557 UTC] Saving snapshot
[2018-07-02 15:57:13.587893 UTC] Starting iteration 67
[2018-07-02 15:57:13.589171 UTC] Start collecting samples
[2018-07-02 15:57:13.931074 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:13.968683 UTC] Computing policy gradient
[2018-07-02 15:57:13.975151 UTC] Updating baseline
[2018-07-02 15:57:14.055902 UTC] Computing logging information
--------------------------------------
| Iteration            | 67          |
| SurrLoss             | -0.00046859 |
| Entropy              | 0.35142     |
| Perplexity           | 1.4211      |
| AveragePolicyProb[0] | 0.50166     |
| AveragePolicyProb[1] | 0.49834     |
| AverageReturn        | 200         |
| MinReturn            | 200         |
| MaxReturn            | 200         |
| StdReturn            | 0           |
| AverageEpisodeLength | 200         |
| MinEpisodeLength     | 200         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 0           |
| TotalNEpisodes       | 816         |
| TotalNSamples        | 1.3454e+05  |
| ExplainedVariance    | 0.22474     |
--------------------------------------
[2018-07-02 15:57:15.216367 UTC] Saving snapshot
[2018-07-02 15:57:15.226992 UTC] Starting iteration 68
[2018-07-02 15:57:15.227906 UTC] Start collecting samples
[2018-07-02 15:57:15.543756 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:15.561237 UTC] Computing policy gradient
[2018-07-02 15:57:15.569353 UTC] Updating baseline
[2018-07-02 15:57:15.675204 UTC] Computing logging information
-------------------------------------
| Iteration            | 68         |
| SurrLoss             | 0.0061919  |
| Entropy              | 0.37468    |
| Perplexity           | 1.4545     |
| AveragePolicyProb[0] | 0.5017     |
| AveragePolicyProb[1] | 0.4983     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 824        |
| TotalNSamples        | 1.3614e+05 |
| ExplainedVariance    | 0.2511     |
-------------------------------------
[2018-07-02 15:57:16.457064 UTC] Saving snapshot
[2018-07-02 15:57:16.467049 UTC] Starting iteration 69
[2018-07-02 15:57:16.467922 UTC] Start collecting samples
[2018-07-02 15:57:16.784951 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:16.811315 UTC] Computing policy gradient
[2018-07-02 15:57:16.820165 UTC] Updating baseline
[2018-07-02 15:57:16.900874 UTC] Computing logging information
-------------------------------------
| Iteration            | 69         |
| SurrLoss             | 0.016379   |
| Entropy              | 0.39508    |
| Perplexity           | 1.4845     |
| AveragePolicyProb[0] | 0.4992     |
| AveragePolicyProb[1] | 0.5008     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 836        |
| TotalNSamples        | 1.3854e+05 |
| ExplainedVariance    | 0.013234   |
-------------------------------------
[2018-07-02 15:57:17.842385 UTC] Saving snapshot
[2018-07-02 15:57:17.853582 UTC] Starting iteration 70
[2018-07-02 15:57:17.854394 UTC] Start collecting samples
[2018-07-02 15:57:18.198445 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:18.217125 UTC] Computing policy gradient
[2018-07-02 15:57:18.225271 UTC] Updating baseline
[2018-07-02 15:57:18.318207 UTC] Computing logging information
-------------------------------------
| Iteration            | 70         |
| SurrLoss             | 0.008201   |
| Entropy              | 0.43069    |
| Perplexity           | 1.5383     |
| AveragePolicyProb[0] | 0.50651    |
| AveragePolicyProb[1] | 0.49349    |
| AverageReturn        | 198.67     |
| MinReturn            | 101        |
| MaxReturn            | 200        |
| StdReturn            | 10.383     |
| AverageEpisodeLength | 198.67     |
| MinEpisodeLength     | 101        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 10.383     |
| TotalNEpisodes       | 846        |
| TotalNSamples        | 1.4041e+05 |
| ExplainedVariance    | 0.39768    |
-------------------------------------
[2018-07-02 15:57:19.583436 UTC] Saving snapshot
[2018-07-02 15:57:19.593320 UTC] Starting iteration 71
[2018-07-02 15:57:19.594125 UTC] Start collecting samples
[2018-07-02 15:57:20.223071 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:20.262676 UTC] Computing policy gradient
[2018-07-02 15:57:20.270210 UTC] Updating baseline
[2018-07-02 15:57:20.362512 UTC] Computing logging information
-------------------------------------
| Iteration            | 71         |
| SurrLoss             | 0.045265   |
| Entropy              | 0.37878    |
| Perplexity           | 1.4605     |
| AveragePolicyProb[0] | 0.54676    |
| AveragePolicyProb[1] | 0.45324    |
| AverageReturn        | 143.07     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 77.978     |
| AverageEpisodeLength | 143.07     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 77.978     |
| TotalNEpisodes       | 889        |
| TotalNSamples        | 1.4345e+05 |
| ExplainedVariance    | -0.22732   |
-------------------------------------
[2018-07-02 15:57:21.966236 UTC] Saving snapshot
[2018-07-02 15:57:21.979809 UTC] Starting iteration 72
[2018-07-02 15:57:21.981095 UTC] Start collecting samples
[2018-07-02 15:57:22.257168 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:22.271759 UTC] Computing policy gradient
[2018-07-02 15:57:22.279999 UTC] Updating baseline
[2018-07-02 15:57:22.372459 UTC] Computing logging information
-------------------------------------
| Iteration            | 72         |
| SurrLoss             | 0.032453   |
| Entropy              | 0.38764    |
| Perplexity           | 1.4735     |
| AveragePolicyProb[0] | 0.49079    |
| AveragePolicyProb[1] | 0.50921    |
| AverageReturn        | 135.03     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.074     |
| AverageEpisodeLength | 135.03     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.074     |
| TotalNEpisodes       | 896        |
| TotalNSamples        | 1.4405e+05 |
| ExplainedVariance    | 0.46846    |
-------------------------------------
[2018-07-02 15:57:23.107074 UTC] Saving snapshot
[2018-07-02 15:57:23.117453 UTC] Starting iteration 73
[2018-07-02 15:57:23.118194 UTC] Start collecting samples
[2018-07-02 15:57:23.455957 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:23.474714 UTC] Computing policy gradient
[2018-07-02 15:57:23.482242 UTC] Updating baseline
[2018-07-02 15:57:23.567689 UTC] Computing logging information
-------------------------------------
| Iteration            | 73         |
| SurrLoss             | -0.0044491 |
| Entropy              | 0.35521    |
| Perplexity           | 1.4265     |
| AveragePolicyProb[0] | 0.49371    |
| AveragePolicyProb[1] | 0.50629    |
| AverageReturn        | 135.03     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.074     |
| AverageEpisodeLength | 135.03     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.074     |
| TotalNEpisodes       | 909        |
| TotalNSamples        | 1.4665e+05 |
| ExplainedVariance    | 0.73063    |
-------------------------------------
[2018-07-02 15:57:24.502503 UTC] Saving snapshot
[2018-07-02 15:57:24.512927 UTC] Starting iteration 74
[2018-07-02 15:57:24.513765 UTC] Start collecting samples
[2018-07-02 15:57:24.742599 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:24.758193 UTC] Computing policy gradient
[2018-07-02 15:57:24.767795 UTC] Updating baseline
[2018-07-02 15:57:24.851694 UTC] Computing logging information
-------------------------------------
| Iteration            | 74         |
| SurrLoss             | -0.0023291 |
| Entropy              | 0.32163    |
| Perplexity           | 1.3794     |
| AveragePolicyProb[0] | 0.49411    |
| AveragePolicyProb[1] | 0.50589    |
| AverageReturn        | 135.03     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.074     |
| AverageEpisodeLength | 135.03     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.074     |
| TotalNEpisodes       | 916        |
| TotalNSamples        | 1.4805e+05 |
| ExplainedVariance    | 0.8342     |
-------------------------------------
[2018-07-02 15:57:25.766168 UTC] Saving snapshot
[2018-07-02 15:57:25.777056 UTC] Starting iteration 75
[2018-07-02 15:57:25.778005 UTC] Start collecting samples
[2018-07-02 15:57:26.113477 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:26.134640 UTC] Computing policy gradient
[2018-07-02 15:57:26.144342 UTC] Updating baseline
[2018-07-02 15:57:26.234994 UTC] Computing logging information
-------------------------------------
| Iteration            | 75         |
| SurrLoss             | -0.010632  |
| Entropy              | 0.27663    |
| Perplexity           | 1.3187     |
| AveragePolicyProb[0] | 0.51146    |
| AveragePolicyProb[1] | 0.48854    |
| AverageReturn        | 135.03     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.074     |
| AverageEpisodeLength | 135.03     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.074     |
| TotalNEpisodes       | 927        |
| TotalNSamples        | 1.5025e+05 |
| ExplainedVariance    | 0.8223     |
-------------------------------------
[2018-07-02 15:57:27.075751 UTC] Saving snapshot
[2018-07-02 15:57:27.087377 UTC] Starting iteration 76
[2018-07-02 15:57:27.088183 UTC] Start collecting samples
[2018-07-02 15:57:27.392834 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:27.412479 UTC] Computing policy gradient
[2018-07-02 15:57:27.419885 UTC] Updating baseline
[2018-07-02 15:57:27.500544 UTC] Computing logging information
-------------------------------------
| Iteration            | 76         |
| SurrLoss             | -0.0065858 |
| Entropy              | 0.25984    |
| Perplexity           | 1.2967     |
| AveragePolicyProb[0] | 0.49691    |
| AveragePolicyProb[1] | 0.50309    |
| AverageReturn        | 136.02     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.262     |
| AverageEpisodeLength | 136.02     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.262     |
| TotalNEpisodes       | 941        |
| TotalNSamples        | 1.5305e+05 |
| ExplainedVariance    | 0.75918    |
-------------------------------------
[2018-07-02 15:57:28.403674 UTC] Saving snapshot
[2018-07-02 15:57:28.414983 UTC] Starting iteration 77
[2018-07-02 15:57:28.415745 UTC] Start collecting samples
[2018-07-02 15:57:28.609753 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:28.627619 UTC] Computing policy gradient
[2018-07-02 15:57:28.636403 UTC] Updating baseline
[2018-07-02 15:57:28.723020 UTC] Computing logging information
-------------------------------------
| Iteration            | 77         |
| SurrLoss             | -0.0041449 |
| Entropy              | 0.23782    |
| Perplexity           | 1.2685     |
| AveragePolicyProb[0] | 0.50053    |
| AveragePolicyProb[1] | 0.49947    |
| AverageReturn        | 136.36     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 79.462     |
| AverageEpisodeLength | 136.36     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 79.462     |
| TotalNEpisodes       | 945        |
| TotalNSamples        | 1.5385e+05 |
| ExplainedVariance    | 0.66709    |
-------------------------------------
[2018-07-02 15:57:29.644547 UTC] Saving snapshot
[2018-07-02 15:57:29.653831 UTC] Starting iteration 78
[2018-07-02 15:57:29.654522 UTC] Start collecting samples
[2018-07-02 15:57:29.933028 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:29.953318 UTC] Computing policy gradient
[2018-07-02 15:57:29.962361 UTC] Updating baseline
[2018-07-02 15:57:30.047505 UTC] Computing logging information
-------------------------------------
| Iteration            | 78         |
| SurrLoss             | 0.0011773  |
| Entropy              | 0.22558    |
| Perplexity           | 1.2531     |
| AveragePolicyProb[0] | 0.49331    |
| AveragePolicyProb[1] | 0.50669    |
| AverageReturn        | 146.62     |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 76.754     |
| AverageEpisodeLength | 146.62     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 76.754     |
| TotalNEpisodes       | 958        |
| TotalNSamples        | 1.5645e+05 |
| ExplainedVariance    | 0.43945    |
-------------------------------------
[2018-07-02 15:57:31.041446 UTC] Saving snapshot
[2018-07-02 15:57:31.050858 UTC] Starting iteration 79
[2018-07-02 15:57:31.051521 UTC] Start collecting samples
[2018-07-02 15:57:31.385895 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:31.418568 UTC] Computing policy gradient
[2018-07-02 15:57:31.438192 UTC] Updating baseline
[2018-07-02 15:57:31.544180 UTC] Computing logging information
-------------------------------------
| Iteration            | 79         |
| SurrLoss             | -0.0034739 |
| Entropy              | 0.23803    |
| Perplexity           | 1.2687     |
| AveragePolicyProb[0] | 0.50235    |
| AveragePolicyProb[1] | 0.49765    |
| AverageReturn        | 164.7      |
| MinReturn            | 9          |
| MaxReturn            | 200        |
| StdReturn            | 66.74      |
| AverageEpisodeLength | 164.7      |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 66.74      |
| TotalNEpisodes       | 971        |
| TotalNSamples        | 1.5905e+05 |
| ExplainedVariance    | 0.2124     |
-------------------------------------
[2018-07-02 15:57:32.436860 UTC] Saving snapshot
[2018-07-02 15:57:32.447659 UTC] Starting iteration 80
[2018-07-02 15:57:32.448619 UTC] Start collecting samples
[2018-07-02 15:57:32.705284 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:32.719581 UTC] Computing policy gradient
[2018-07-02 15:57:32.729172 UTC] Updating baseline
[2018-07-02 15:57:32.816884 UTC] Computing logging information
--------------------------------------
| Iteration            | 80          |
| SurrLoss             | -0.00021024 |
| Entropy              | 0.20728     |
| Perplexity           | 1.2303      |
| AveragePolicyProb[0] | 0.50687     |
| AveragePolicyProb[1] | 0.49313     |
| AverageReturn        | 172.85      |
| MinReturn            | 10          |
| MaxReturn            | 200         |
| StdReturn            | 59.749      |
| AverageEpisodeLength | 172.85      |
| MinEpisodeLength     | 10          |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 59.749      |
| TotalNEpisodes       | 976         |
| TotalNSamples        | 1.6005e+05  |
| ExplainedVariance    | 0.14898     |
--------------------------------------
[2018-07-02 15:57:33.755278 UTC] Saving snapshot
[2018-07-02 15:57:33.767312 UTC] Starting iteration 81
[2018-07-02 15:57:33.768079 UTC] Start collecting samples
[2018-07-02 15:57:34.052595 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:34.071049 UTC] Computing policy gradient
[2018-07-02 15:57:34.078139 UTC] Updating baseline
[2018-07-02 15:57:34.165454 UTC] Computing logging information
-------------------------------------
| Iteration            | 81         |
| SurrLoss             | -0.0025101 |
| Entropy              | 0.19662    |
| Perplexity           | 1.2173     |
| AveragePolicyProb[0] | 0.49724    |
| AveragePolicyProb[1] | 0.50276    |
| AverageReturn        | 191.96     |
| MinReturn            | 12         |
| MaxReturn            | 200        |
| StdReturn            | 32.978     |
| AverageEpisodeLength | 191.96     |
| MinEpisodeLength     | 12         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 32.978     |
| TotalNEpisodes       | 989        |
| TotalNSamples        | 1.6265e+05 |
| ExplainedVariance    | -0.11324   |
-------------------------------------
[2018-07-02 15:57:34.953239 UTC] Saving snapshot
[2018-07-02 15:57:34.963627 UTC] Starting iteration 82
[2018-07-02 15:57:34.964279 UTC] Start collecting samples
[2018-07-02 15:57:35.176007 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:35.196663 UTC] Computing policy gradient
[2018-07-02 15:57:35.206550 UTC] Updating baseline
[2018-07-02 15:57:35.290486 UTC] Computing logging information
-------------------------------------
| Iteration            | 82         |
| SurrLoss             | -0.0071702 |
| Entropy              | 0.21372    |
| Perplexity           | 1.2383     |
| AveragePolicyProb[0] | 0.49627    |
| AveragePolicyProb[1] | 0.50373    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 996        |
| TotalNSamples        | 1.6405e+05 |
| ExplainedVariance    | 0.49696    |
-------------------------------------
[2018-07-02 15:57:36.096236 UTC] Saving snapshot
[2018-07-02 15:57:36.107873 UTC] Starting iteration 83
[2018-07-02 15:57:36.108679 UTC] Start collecting samples
[2018-07-02 15:57:36.396481 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:36.415010 UTC] Computing policy gradient
[2018-07-02 15:57:36.426950 UTC] Updating baseline
[2018-07-02 15:57:36.526566 UTC] Computing logging information
-------------------------------------
| Iteration            | 83         |
| SurrLoss             | 0.017254   |
| Entropy              | 0.18241    |
| Perplexity           | 1.2001     |
| AveragePolicyProb[0] | 0.51173    |
| AveragePolicyProb[1] | 0.48827    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1007       |
| TotalNSamples        | 1.6625e+05 |
| ExplainedVariance    | 0.44712    |
-------------------------------------
[2018-07-02 15:57:37.404761 UTC] Saving snapshot
[2018-07-02 15:57:37.417495 UTC] Starting iteration 84
[2018-07-02 15:57:37.418400 UTC] Start collecting samples
[2018-07-02 15:57:37.732293 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:37.757390 UTC] Computing policy gradient
[2018-07-02 15:57:37.764018 UTC] Updating baseline
[2018-07-02 15:57:37.843715 UTC] Computing logging information
-------------------------------------
| Iteration            | 84         |
| SurrLoss             | 0.015674   |
| Entropy              | 0.18013    |
| Perplexity           | 1.1974     |
| AveragePolicyProb[0] | 0.50674    |
| AveragePolicyProb[1] | 0.49326    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1021       |
| TotalNSamples        | 1.6905e+05 |
| ExplainedVariance    | 0.70126    |
-------------------------------------
[2018-07-02 15:57:38.916075 UTC] Saving snapshot
[2018-07-02 15:57:38.927641 UTC] Starting iteration 85
[2018-07-02 15:57:38.928776 UTC] Start collecting samples
[2018-07-02 15:57:39.167884 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:39.186991 UTC] Computing policy gradient
[2018-07-02 15:57:39.200298 UTC] Updating baseline
[2018-07-02 15:57:39.282485 UTC] Computing logging information
-------------------------------------
| Iteration            | 85         |
| SurrLoss             | 0.0063141  |
| Entropy              | 0.1852     |
| Perplexity           | 1.2035     |
| AveragePolicyProb[0] | 0.50028    |
| AveragePolicyProb[1] | 0.49972    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1025       |
| TotalNSamples        | 1.6985e+05 |
| ExplainedVariance    | 0.6582     |
-------------------------------------
[2018-07-02 15:57:40.181254 UTC] Saving snapshot
[2018-07-02 15:57:40.192446 UTC] Starting iteration 86
[2018-07-02 15:57:40.193264 UTC] Start collecting samples
[2018-07-02 15:57:40.469829 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:40.490423 UTC] Computing policy gradient
[2018-07-02 15:57:40.498080 UTC] Updating baseline
[2018-07-02 15:57:40.586647 UTC] Computing logging information
-------------------------------------
| Iteration            | 86         |
| SurrLoss             | -0.013808  |
| Entropy              | 0.17897    |
| Perplexity           | 1.196      |
| AveragePolicyProb[0] | 0.49889    |
| AveragePolicyProb[1] | 0.50111    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1038       |
| TotalNSamples        | 1.7245e+05 |
| ExplainedVariance    | 0.79214    |
-------------------------------------
[2018-07-02 15:57:41.481918 UTC] Saving snapshot
[2018-07-02 15:57:41.492009 UTC] Starting iteration 87
[2018-07-02 15:57:41.494115 UTC] Start collecting samples
[2018-07-02 15:57:41.829597 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:41.852178 UTC] Computing policy gradient
[2018-07-02 15:57:41.862911 UTC] Updating baseline
[2018-07-02 15:57:41.954632 UTC] Computing logging information
-------------------------------------
| Iteration            | 87         |
| SurrLoss             | 0.007005   |
| Entropy              | 0.18276    |
| Perplexity           | 1.2005     |
| AveragePolicyProb[0] | 0.49716    |
| AveragePolicyProb[1] | 0.50284    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1051       |
| TotalNSamples        | 1.7505e+05 |
| ExplainedVariance    | 0.55669    |
-------------------------------------
[2018-07-02 15:57:42.922146 UTC] Saving snapshot
[2018-07-02 15:57:42.931935 UTC] Starting iteration 88
[2018-07-02 15:57:42.932852 UTC] Start collecting samples
[2018-07-02 15:57:43.163762 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:43.178503 UTC] Computing policy gradient
[2018-07-02 15:57:43.188389 UTC] Updating baseline
[2018-07-02 15:57:43.271078 UTC] Computing logging information
-------------------------------------
| Iteration            | 88         |
| SurrLoss             | 0.0073207  |
| Entropy              | 0.16911    |
| Perplexity           | 1.1842     |
| AveragePolicyProb[0] | 0.50635    |
| AveragePolicyProb[1] | 0.49365    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1056       |
| TotalNSamples        | 1.7605e+05 |
| ExplainedVariance    | 0.67203    |
-------------------------------------
[2018-07-02 15:57:44.174900 UTC] Saving snapshot
[2018-07-02 15:57:44.185328 UTC] Starting iteration 89
[2018-07-02 15:57:44.186291 UTC] Start collecting samples
[2018-07-02 15:57:44.500610 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:44.538897 UTC] Computing policy gradient
[2018-07-02 15:57:44.548587 UTC] Updating baseline
[2018-07-02 15:57:44.632416 UTC] Computing logging information
-------------------------------------
| Iteration            | 89         |
| SurrLoss             | -0.013039  |
| Entropy              | 0.15615    |
| Perplexity           | 1.169      |
| AveragePolicyProb[0] | 0.50077    |
| AveragePolicyProb[1] | 0.49923    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1069       |
| TotalNSamples        | 1.7865e+05 |
| ExplainedVariance    | 0.66265    |
-------------------------------------
[2018-07-02 15:57:45.682104 UTC] Saving snapshot
[2018-07-02 15:57:45.707392 UTC] Starting iteration 90
[2018-07-02 15:57:45.708537 UTC] Start collecting samples
[2018-07-02 15:57:46.070764 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:46.091178 UTC] Computing policy gradient
[2018-07-02 15:57:46.101678 UTC] Updating baseline
[2018-07-02 15:57:46.183454 UTC] Computing logging information
-------------------------------------
| Iteration            | 90         |
| SurrLoss             | 0.029525   |
| Entropy              | 0.16489    |
| Perplexity           | 1.1793     |
| AveragePolicyProb[0] | 0.49603    |
| AveragePolicyProb[1] | 0.50397    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1076       |
| TotalNSamples        | 1.8005e+05 |
| ExplainedVariance    | 0.65342    |
-------------------------------------
[2018-07-02 15:57:47.126169 UTC] Saving snapshot
[2018-07-02 15:57:47.139398 UTC] Starting iteration 91
[2018-07-02 15:57:47.140372 UTC] Start collecting samples
[2018-07-02 15:57:47.505303 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:47.522444 UTC] Computing policy gradient
[2018-07-02 15:57:47.529132 UTC] Updating baseline
[2018-07-02 15:57:47.617026 UTC] Computing logging information
-------------------------------------
| Iteration            | 91         |
| SurrLoss             | -0.0032053 |
| Entropy              | 0.16088    |
| Perplexity           | 1.1745     |
| AveragePolicyProb[0] | 0.49554    |
| AveragePolicyProb[1] | 0.50446    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1087       |
| TotalNSamples        | 1.8225e+05 |
| ExplainedVariance    | 0.67526    |
-------------------------------------
[2018-07-02 15:57:48.422884 UTC] Saving snapshot
[2018-07-02 15:57:48.432180 UTC] Starting iteration 92
[2018-07-02 15:57:48.432800 UTC] Start collecting samples
[2018-07-02 15:57:48.757647 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:48.774995 UTC] Computing policy gradient
[2018-07-02 15:57:48.781925 UTC] Updating baseline
[2018-07-02 15:57:48.858692 UTC] Computing logging information
-------------------------------------
| Iteration            | 92         |
| SurrLoss             | -0.012856  |
| Entropy              | 0.14707    |
| Perplexity           | 1.1584     |
| AveragePolicyProb[0] | 0.49622    |
| AveragePolicyProb[1] | 0.50378    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1101       |
| TotalNSamples        | 1.8505e+05 |
| ExplainedVariance    | 0.50678    |
-------------------------------------
[2018-07-02 15:57:49.752471 UTC] Saving snapshot
[2018-07-02 15:57:49.767382 UTC] Starting iteration 93
[2018-07-02 15:57:49.768659 UTC] Start collecting samples
[2018-07-02 15:57:50.072696 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:50.093172 UTC] Computing policy gradient
[2018-07-02 15:57:50.101962 UTC] Updating baseline
[2018-07-02 15:57:50.187999 UTC] Computing logging information
-------------------------------------
| Iteration            | 93         |
| SurrLoss             | 0.016223   |
| Entropy              | 0.1644     |
| Perplexity           | 1.1787     |
| AveragePolicyProb[0] | 0.49241    |
| AveragePolicyProb[1] | 0.50759    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1105       |
| TotalNSamples        | 1.8585e+05 |
| ExplainedVariance    | 0.54041    |
-------------------------------------
[2018-07-02 15:57:50.992196 UTC] Saving snapshot
[2018-07-02 15:57:51.008839 UTC] Starting iteration 94
[2018-07-02 15:57:51.010588 UTC] Start collecting samples
[2018-07-02 15:57:51.279728 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:51.298578 UTC] Computing policy gradient
[2018-07-02 15:57:51.309695 UTC] Updating baseline
[2018-07-02 15:57:51.407246 UTC] Computing logging information
-------------------------------------
| Iteration            | 94         |
| SurrLoss             | -0.010354  |
| Entropy              | 0.14803    |
| Perplexity           | 1.1595     |
| AveragePolicyProb[0] | 0.5109     |
| AveragePolicyProb[1] | 0.4891     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1118       |
| TotalNSamples        | 1.8845e+05 |
| ExplainedVariance    | 0.15993    |
-------------------------------------
[2018-07-02 15:57:52.173038 UTC] Saving snapshot
[2018-07-02 15:57:52.183410 UTC] Starting iteration 95
[2018-07-02 15:57:52.184270 UTC] Start collecting samples
[2018-07-02 15:57:52.468891 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:52.487739 UTC] Computing policy gradient
[2018-07-02 15:57:52.495321 UTC] Updating baseline
[2018-07-02 15:57:52.571947 UTC] Computing logging information
-------------------------------------
| Iteration            | 95         |
| SurrLoss             | 0.012306   |
| Entropy              | 0.15255    |
| Perplexity           | 1.1648     |
| AveragePolicyProb[0] | 0.50327    |
| AveragePolicyProb[1] | 0.49673    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1131       |
| TotalNSamples        | 1.9105e+05 |
| ExplainedVariance    | 0.45237    |
-------------------------------------
[2018-07-02 15:57:53.470146 UTC] Saving snapshot
[2018-07-02 15:57:53.481729 UTC] Starting iteration 96
[2018-07-02 15:57:53.482821 UTC] Start collecting samples
[2018-07-02 15:57:53.724201 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:53.737630 UTC] Computing policy gradient
[2018-07-02 15:57:53.745448 UTC] Updating baseline
[2018-07-02 15:57:53.824931 UTC] Computing logging information
-------------------------------------
| Iteration            | 96         |
| SurrLoss             | -0.0063822 |
| Entropy              | 0.13737    |
| Perplexity           | 1.1472     |
| AveragePolicyProb[0] | 0.50604    |
| AveragePolicyProb[1] | 0.49396    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1136       |
| TotalNSamples        | 1.9205e+05 |
| ExplainedVariance    | 0.2063     |
-------------------------------------
[2018-07-02 15:57:54.754708 UTC] Saving snapshot
[2018-07-02 15:57:54.767602 UTC] Starting iteration 97
[2018-07-02 15:57:54.768794 UTC] Start collecting samples
[2018-07-02 15:57:55.139050 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:55.171428 UTC] Computing policy gradient
[2018-07-02 15:57:55.179009 UTC] Updating baseline
[2018-07-02 15:57:55.252704 UTC] Computing logging information
-------------------------------------
| Iteration            | 97         |
| SurrLoss             | -0.0042873 |
| Entropy              | 0.13288    |
| Perplexity           | 1.1421     |
| AveragePolicyProb[0] | 0.49865    |
| AveragePolicyProb[1] | 0.50135    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1149       |
| TotalNSamples        | 1.9465e+05 |
| ExplainedVariance    | -0.10868   |
-------------------------------------
[2018-07-02 15:57:56.172446 UTC] Saving snapshot
[2018-07-02 15:57:56.182614 UTC] Starting iteration 98
[2018-07-02 15:57:56.186333 UTC] Start collecting samples
[2018-07-02 15:57:56.443496 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:56.456613 UTC] Computing policy gradient
[2018-07-02 15:57:56.464094 UTC] Updating baseline
[2018-07-02 15:57:56.543841 UTC] Computing logging information
--------------------------------------
| Iteration            | 98          |
| SurrLoss             | -0.00075876 |
| Entropy              | 0.13356     |
| Perplexity           | 1.1429      |
| AveragePolicyProb[0] | 0.49975     |
| AveragePolicyProb[1] | 0.50025     |
| AverageReturn        | 200         |
| MinReturn            | 200         |
| MaxReturn            | 200         |
| StdReturn            | 0           |
| AverageEpisodeLength | 200         |
| MinEpisodeLength     | 200         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 0           |
| TotalNEpisodes       | 1156        |
| TotalNSamples        | 1.9605e+05  |
| ExplainedVariance    | -0.20679    |
--------------------------------------
[2018-07-02 15:57:57.504185 UTC] Saving snapshot
[2018-07-02 15:57:57.514400 UTC] Starting iteration 99
[2018-07-02 15:57:57.516584 UTC] Start collecting samples
[2018-07-02 15:57:57.809081 UTC] Computing input variables for policy optimization
[2018-07-02 15:57:57.841025 UTC] Computing policy gradient
[2018-07-02 15:57:57.848855 UTC] Updating baseline
[2018-07-02 15:57:57.921808 UTC] Computing logging information
-------------------------------------
| Iteration            | 99         |
| SurrLoss             | -0.010159  |
| Entropy              | 0.13051    |
| Perplexity           | 1.1394     |
| AveragePolicyProb[0] | 0.49931    |
| AveragePolicyProb[1] | 0.50069    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1167       |
| TotalNSamples        | 1.9825e+05 |
| ExplainedVariance    | -0.031666  |
-------------------------------------
[2018-07-02 15:57:58.833719 UTC] Saving snapshot
