[2018-07-02 16:08:10.305075 UTC] Starting env pool
[2018-07-02 16:08:10.353336 UTC] Starting iteration 0
[2018-07-02 16:08:10.354364 UTC] Start collecting samples
[2018-07-02 16:08:11.336588 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:11.387849 UTC] Performing policy update
[2018-07-02 16:08:11.389081 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:11.406851 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:11.492277 UTC] Performing line search
[2018-07-02 16:08:11.496953 UTC] Updating baseline
[2018-07-02 16:08:11.594907 UTC] Computing logging information
-------------------------------------
| Iteration            | 0          |
| ExpectedImprovement  | 0.030797   |
| ActualImprovement    | 0.022022   |
| ImprovementRatio     | 0.71507    |
| MeanKL               | 0.0068129  |
| Entropy              | 0.6925     |
| Perplexity           | 1.9987     |
| AveragePolicyProb[0] | 0.50155    |
| AveragePolicyProb[1] | 0.49845    |
| AverageReturn        | 23.462     |
| MinReturn            | 9          |
| MaxReturn            | 81         |
| StdReturn            | 11.748     |
| AverageEpisodeLength | 23.462     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 81         |
| StdEpisodeLength     | 11.748     |
| TotalNEpisodes       | 78         |
| TotalNSamples        | 1830       |
| ExplainedVariance    | -0.0058665 |
-------------------------------------
[2018-07-02 16:08:12.381712 UTC] Saving snapshot
[2018-07-02 16:08:12.393392 UTC] Starting iteration 1
[2018-07-02 16:08:12.394228 UTC] Start collecting samples
[2018-07-02 16:08:13.189437 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:13.234316 UTC] Performing policy update
[2018-07-02 16:08:13.235225 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:13.245587 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:13.328901 UTC] Performing line search
[2018-07-02 16:08:13.339242 UTC] Updating baseline
[2018-07-02 16:08:13.425285 UTC] Computing logging information
------------------------------------
| Iteration            | 1         |
| ExpectedImprovement  | 0.031618  |
| ActualImprovement    | 0.027792  |
| ImprovementRatio     | 0.87901   |
| MeanKL               | 0.0068018 |
| Entropy              | 0.68362   |
| Perplexity           | 1.981     |
| AveragePolicyProb[0] | 0.49815   |
| AveragePolicyProb[1] | 0.50185   |
| AverageReturn        | 24.73     |
| MinReturn            | 10        |
| MaxReturn            | 68        |
| StdReturn            | 12.29     |
| AverageEpisodeLength | 24.73     |
| MinEpisodeLength     | 10        |
| MaxEpisodeLength     | 68        |
| StdEpisodeLength     | 12.29     |
| TotalNEpisodes       | 153       |
| TotalNSamples        | 3753      |
| ExplainedVariance    | 0.25776   |
------------------------------------
[2018-07-02 16:08:14.201043 UTC] Saving snapshot
[2018-07-02 16:08:14.209337 UTC] Starting iteration 2
[2018-07-02 16:08:14.210109 UTC] Start collecting samples
[2018-07-02 16:08:14.812836 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:14.874470 UTC] Performing policy update
[2018-07-02 16:08:14.875633 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:14.887460 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:14.968535 UTC] Performing line search
[2018-07-02 16:08:14.975251 UTC] Updating baseline
[2018-07-02 16:08:15.073168 UTC] Computing logging information
------------------------------------
| Iteration            | 2         |
| ExpectedImprovement  | 0.038525  |
| ActualImprovement    | 0.028546  |
| ImprovementRatio     | 0.74097   |
| MeanKL               | 0.0091362 |
| Entropy              | 0.66903   |
| Perplexity           | 1.9523    |
| AveragePolicyProb[0] | 0.51278   |
| AveragePolicyProb[1] | 0.48722   |
| AverageReturn        | 33.82     |
| MinReturn            | 11        |
| MaxReturn            | 99        |
| StdReturn            | 18.767    |
| AverageEpisodeLength | 33.82     |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 99        |
| StdEpisodeLength     | 18.767    |
| TotalNEpisodes       | 205       |
| TotalNSamples        | 5759      |
| ExplainedVariance    | 0.28436   |
------------------------------------
[2018-07-02 16:08:16.366273 UTC] Saving snapshot
[2018-07-02 16:08:16.374832 UTC] Starting iteration 3
[2018-07-02 16:08:16.375756 UTC] Start collecting samples
[2018-07-02 16:08:16.778312 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:16.809559 UTC] Performing policy update
[2018-07-02 16:08:16.810923 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:16.822615 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:16.916027 UTC] Performing line search
[2018-07-02 16:08:16.923384 UTC] Updating baseline
[2018-07-02 16:08:17.009461 UTC] Computing logging information
------------------------------------
| Iteration            | 3         |
| ExpectedImprovement  | 0.042562  |
| ActualImprovement    | 0.032059  |
| ImprovementRatio     | 0.75321   |
| MeanKL               | 0.0071554 |
| Entropy              | 0.64821   |
| Perplexity           | 1.9121    |
| AveragePolicyProb[0] | 0.53555   |
| AveragePolicyProb[1] | 0.46445   |
| AverageReturn        | 39.32     |
| MinReturn            | 11        |
| MaxReturn            | 108       |
| StdReturn            | 23.452    |
| AverageEpisodeLength | 39.32     |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 108       |
| StdEpisodeLength     | 23.452    |
| TotalNEpisodes       | 231       |
| TotalNSamples        | 7084      |
| ExplainedVariance    | 0.26632   |
------------------------------------
[2018-07-02 16:08:17.743313 UTC] Saving snapshot
[2018-07-02 16:08:17.752033 UTC] Starting iteration 4
[2018-07-02 16:08:17.752649 UTC] Start collecting samples
[2018-07-02 16:08:18.132489 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:18.157919 UTC] Performing policy update
[2018-07-02 16:08:18.159176 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:18.172620 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:18.263592 UTC] Performing line search
[2018-07-02 16:08:18.271086 UTC] Updating baseline
[2018-07-02 16:08:18.356699 UTC] Computing logging information
------------------------------------
| Iteration            | 4         |
| ExpectedImprovement  | 0.040151  |
| ActualImprovement    | 0.025013  |
| ImprovementRatio     | 0.62296   |
| MeanKL               | 0.0056086 |
| Entropy              | 0.62259   |
| Perplexity           | 1.8638    |
| AveragePolicyProb[0] | 0.5278    |
| AveragePolicyProb[1] | 0.4722    |
| AverageReturn        | 52.54     |
| MinReturn            | 11        |
| MaxReturn            | 200       |
| StdReturn            | 36.977    |
| AverageEpisodeLength | 52.54     |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 36.977    |
| TotalNEpisodes       | 256       |
| TotalNSamples        | 9069      |
| ExplainedVariance    | 0.32024   |
------------------------------------
[2018-07-02 16:08:19.255807 UTC] Saving snapshot
[2018-07-02 16:08:19.266310 UTC] Starting iteration 5
[2018-07-02 16:08:19.267264 UTC] Start collecting samples
[2018-07-02 16:08:19.596579 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:19.619541 UTC] Performing policy update
[2018-07-02 16:08:19.620662 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:19.630538 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:19.719833 UTC] Performing line search
[2018-07-02 16:08:19.727830 UTC] Updating baseline
[2018-07-02 16:08:19.819082 UTC] Computing logging information
------------------------------------
| Iteration            | 5         |
| ExpectedImprovement  | 0.040287  |
| ActualImprovement    | 0.015358  |
| ImprovementRatio     | 0.38123   |
| MeanKL               | 0.0048701 |
| Entropy              | 0.6009    |
| Perplexity           | 1.8238    |
| AveragePolicyProb[0] | 0.53886   |
| AveragePolicyProb[1] | 0.46114   |
| AverageReturn        | 65.3      |
| MinReturn            | 11        |
| MaxReturn            | 200       |
| StdReturn            | 43.765    |
| AverageEpisodeLength | 65.3      |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 43.765    |
| TotalNEpisodes       | 273       |
| TotalNSamples        | 10844     |
| ExplainedVariance    | 0.52397   |
------------------------------------
[2018-07-02 16:08:20.650875 UTC] Saving snapshot
[2018-07-02 16:08:20.660974 UTC] Starting iteration 6
[2018-07-02 16:08:20.661646 UTC] Start collecting samples
[2018-07-02 16:08:20.960886 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:20.981712 UTC] Performing policy update
[2018-07-02 16:08:20.982971 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:20.995524 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:21.087885 UTC] Performing line search
[2018-07-02 16:08:21.096321 UTC] Updating baseline
[2018-07-02 16:08:21.194816 UTC] Computing logging information
------------------------------------
| Iteration            | 6         |
| ExpectedImprovement  | 0.032289  |
| ActualImprovement    | 0.021494  |
| ImprovementRatio     | 0.66567   |
| MeanKL               | 0.0074451 |
| Entropy              | 0.59835   |
| Perplexity           | 1.8191    |
| AveragePolicyProb[0] | 0.52012   |
| AveragePolicyProb[1] | 0.47988   |
| AverageReturn        | 81.08     |
| MinReturn            | 11        |
| MaxReturn            | 200       |
| StdReturn            | 56.08     |
| AverageEpisodeLength | 81.08     |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 56.08     |
| TotalNEpisodes       | 286       |
| TotalNSamples        | 13046     |
| ExplainedVariance    | 0.55327   |
------------------------------------
[2018-07-02 16:08:22.136209 UTC] Saving snapshot
[2018-07-02 16:08:22.144799 UTC] Starting iteration 7
[2018-07-02 16:08:22.145700 UTC] Start collecting samples
[2018-07-02 16:08:22.353055 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:22.367052 UTC] Performing policy update
[2018-07-02 16:08:22.368205 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:22.382087 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:22.468011 UTC] Performing line search
[2018-07-02 16:08:22.474931 UTC] Updating baseline
[2018-07-02 16:08:22.561578 UTC] Computing logging information
------------------------------------
| Iteration            | 7         |
| ExpectedImprovement  | 0.048138  |
| ActualImprovement    | 0.016152  |
| ImprovementRatio     | 0.33553   |
| MeanKL               | 0.0091287 |
| Entropy              | 0.5869    |
| Perplexity           | 1.7984    |
| AveragePolicyProb[0] | 0.51492   |
| AveragePolicyProb[1] | 0.48508   |
| AverageReturn        | 86.41     |
| MinReturn            | 11        |
| MaxReturn            | 200       |
| StdReturn            | 58.341    |
| AverageEpisodeLength | 86.41     |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 58.341    |
| TotalNEpisodes       | 291       |
| TotalNSamples        | 13802     |
| ExplainedVariance    | 0.61086   |
------------------------------------
[2018-07-02 16:08:23.342566 UTC] Saving snapshot
[2018-07-02 16:08:23.350798 UTC] Starting iteration 8
[2018-07-02 16:08:23.351493 UTC] Start collecting samples
[2018-07-02 16:08:23.727109 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:23.752800 UTC] Performing policy update
[2018-07-02 16:08:23.753980 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:23.765393 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:23.860630 UTC] Performing line search
[2018-07-02 16:08:23.867811 UTC] Updating baseline
[2018-07-02 16:08:24.033107 UTC] Computing logging information
------------------------------------
| Iteration            | 8         |
| ExpectedImprovement  | 0.032954  |
| ActualImprovement    | 0.013727  |
| ImprovementRatio     | 0.41655   |
| MeanKL               | 0.0089253 |
| Entropy              | 0.57546   |
| Perplexity           | 1.778     |
| AveragePolicyProb[0] | 0.53174   |
| AveragePolicyProb[1] | 0.46826   |
| AverageReturn        | 108.73    |
| MinReturn            | 12        |
| MaxReturn            | 200       |
| StdReturn            | 64.261    |
| AverageEpisodeLength | 108.73    |
| MinEpisodeLength     | 12        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 64.261    |
| TotalNEpisodes       | 306       |
| TotalNSamples        | 16643     |
| ExplainedVariance    | 0.5108    |
------------------------------------
[2018-07-02 16:08:24.841532 UTC] Saving snapshot
[2018-07-02 16:08:24.855931 UTC] Starting iteration 9
[2018-07-02 16:08:24.857698 UTC] Start collecting samples
[2018-07-02 16:08:25.186391 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:25.212353 UTC] Performing policy update
[2018-07-02 16:08:25.213481 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:25.222917 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:25.307918 UTC] Performing line search
[2018-07-02 16:08:25.315361 UTC] Updating baseline
[2018-07-02 16:08:25.398658 UTC] Computing logging information
------------------------------------
| Iteration            | 9         |
| ExpectedImprovement  | 0.02405   |
| ActualImprovement    | 0.017855  |
| ImprovementRatio     | 0.74241   |
| MeanKL               | 0.0094825 |
| Entropy              | 0.57524   |
| Perplexity           | 1.7776    |
| AveragePolicyProb[0] | 0.49864   |
| AveragePolicyProb[1] | 0.50136   |
| AverageReturn        | 125.7     |
| MinReturn            | 13        |
| MaxReturn            | 200       |
| StdReturn            | 61.759    |
| AverageEpisodeLength | 125.7     |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 61.759    |
| TotalNEpisodes       | 320       |
| TotalNSamples        | 18968     |
| ExplainedVariance    | 0.54403   |
------------------------------------
[2018-07-02 16:08:26.280324 UTC] Saving snapshot
[2018-07-02 16:08:26.289236 UTC] Starting iteration 10
[2018-07-02 16:08:26.290020 UTC] Start collecting samples
[2018-07-02 16:08:26.568848 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:26.588726 UTC] Performing policy update
[2018-07-02 16:08:26.589802 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:26.601226 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:26.691702 UTC] Performing line search
[2018-07-02 16:08:26.697985 UTC] Updating baseline
[2018-07-02 16:08:26.788340 UTC] Computing logging information
------------------------------------
| Iteration            | 10        |
| ExpectedImprovement  | 0.029205  |
| ActualImprovement    | 0.0070201 |
| ImprovementRatio     | 0.24038   |
| MeanKL               | 0.0049543 |
| Entropy              | 0.57718   |
| Perplexity           | 1.781     |
| AveragePolicyProb[0] | 0.51091   |
| AveragePolicyProb[1] | 0.48909   |
| AverageReturn        | 135.29    |
| MinReturn            | 13        |
| MaxReturn            | 200       |
| StdReturn            | 59.134    |
| AverageEpisodeLength | 135.29    |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 59.134    |
| TotalNEpisodes       | 330       |
| TotalNSamples        | 20537     |
| ExplainedVariance    | 0.50223   |
------------------------------------
[2018-07-02 16:08:27.821646 UTC] Saving snapshot
[2018-07-02 16:08:27.831773 UTC] Starting iteration 11
[2018-07-02 16:08:27.833990 UTC] Start collecting samples
[2018-07-02 16:08:28.152684 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:28.168867 UTC] Performing policy update
[2018-07-02 16:08:28.169854 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:28.178759 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:28.261164 UTC] Performing line search
[2018-07-02 16:08:28.266261 UTC] Updating baseline
[2018-07-02 16:08:28.359297 UTC] Computing logging information
------------------------------------
| Iteration            | 11        |
| ExpectedImprovement  | 0.024512  |
| ActualImprovement    | 0.00631   |
| ImprovementRatio     | 0.25743   |
| MeanKL               | 0.0057628 |
| Entropy              | 0.57147   |
| Perplexity           | 1.7709    |
| AveragePolicyProb[0] | 0.52631   |
| AveragePolicyProb[1] | 0.47369   |
| AverageReturn        | 145.82    |
| MinReturn            | 13        |
| MaxReturn            | 200       |
| StdReturn            | 57.716    |
| AverageEpisodeLength | 145.82    |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 57.716    |
| TotalNEpisodes       | 341       |
| TotalNSamples        | 22437     |
| ExplainedVariance    | 0.31557   |
------------------------------------
[2018-07-02 16:08:29.266151 UTC] Saving snapshot
[2018-07-02 16:08:29.276424 UTC] Starting iteration 12
[2018-07-02 16:08:29.277080 UTC] Start collecting samples
[2018-07-02 16:08:29.582829 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:29.604296 UTC] Performing policy update
[2018-07-02 16:08:29.605396 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:29.618720 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:29.719148 UTC] Performing line search
[2018-07-02 16:08:29.723914 UTC] Updating baseline
[2018-07-02 16:08:29.811254 UTC] Computing logging information
------------------------------------
| Iteration            | 12        |
| ExpectedImprovement  | 0.030021  |
| ActualImprovement    | 0.020521  |
| ImprovementRatio     | 0.68355   |
| MeanKL               | 0.0074706 |
| Entropy              | 0.56822   |
| Perplexity           | 1.7651    |
| AveragePolicyProb[0] | 0.50389   |
| AveragePolicyProb[1] | 0.49611   |
| AverageReturn        | 155.98    |
| MinReturn            | 13        |
| MaxReturn            | 200       |
| StdReturn            | 49.756    |
| AverageEpisodeLength | 155.98    |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 49.756    |
| TotalNEpisodes       | 353       |
| TotalNSamples        | 24422     |
| ExplainedVariance    | 0.57472   |
------------------------------------
[2018-07-02 16:08:30.518326 UTC] Saving snapshot
[2018-07-02 16:08:30.528153 UTC] Starting iteration 13
[2018-07-02 16:08:30.528899 UTC] Start collecting samples
[2018-07-02 16:08:30.871548 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:30.893706 UTC] Performing policy update
[2018-07-02 16:08:30.894698 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:30.906115 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:30.992688 UTC] Performing line search
[2018-07-02 16:08:30.998171 UTC] Updating baseline
[2018-07-02 16:08:31.091411 UTC] Computing logging information
------------------------------------
| Iteration            | 13        |
| ExpectedImprovement  | 0.016747  |
| ActualImprovement    | 0.0094779 |
| ImprovementRatio     | 0.56595   |
| MeanKL               | 0.0070899 |
| Entropy              | 0.56001   |
| Perplexity           | 1.7507    |
| AveragePolicyProb[0] | 0.51011   |
| AveragePolicyProb[1] | 0.48989   |
| AverageReturn        | 166.98    |
| MinReturn            | 13        |
| MaxReturn            | 200       |
| StdReturn            | 41.932    |
| AverageEpisodeLength | 166.98    |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 41.932    |
| TotalNEpisodes       | 366       |
| TotalNSamples        | 26789     |
| ExplainedVariance    | 0.55077   |
------------------------------------
[2018-07-02 16:08:31.888945 UTC] Saving snapshot
[2018-07-02 16:08:31.897444 UTC] Starting iteration 14
[2018-07-02 16:08:31.897980 UTC] Start collecting samples
[2018-07-02 16:08:32.209120 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:32.225422 UTC] Performing policy update
[2018-07-02 16:08:32.226443 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:32.235369 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:32.315985 UTC] Performing line search
[2018-07-02 16:08:32.320867 UTC] Updating baseline
[2018-07-02 16:08:32.412452 UTC] Computing logging information
------------------------------------
| Iteration            | 14        |
| ExpectedImprovement  | 0.027797  |
| ActualImprovement    | 0.0040616 |
| ImprovementRatio     | 0.14612   |
| MeanKL               | 0.0047407 |
| Entropy              | 0.56193   |
| Perplexity           | 1.7541    |
| AveragePolicyProb[0] | 0.50187   |
| AveragePolicyProb[1] | 0.49813   |
| AverageReturn        | 172.05    |
| MinReturn            | 56        |
| MaxReturn            | 200       |
| StdReturn            | 37.392    |
| AverageEpisodeLength | 172.05    |
| MinEpisodeLength     | 56        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 37.392    |
| TotalNEpisodes       | 376       |
| TotalNSamples        | 28620     |
| ExplainedVariance    | 0.61533   |
------------------------------------
[2018-07-02 16:08:33.218952 UTC] Saving snapshot
[2018-07-02 16:08:33.227625 UTC] Starting iteration 15
[2018-07-02 16:08:33.228254 UTC] Start collecting samples
[2018-07-02 16:08:33.501963 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:33.522271 UTC] Performing policy update
[2018-07-02 16:08:33.523449 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:33.533992 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:33.615895 UTC] Performing line search
[2018-07-02 16:08:33.622183 UTC] Updating baseline
[2018-07-02 16:08:33.719150 UTC] Computing logging information
------------------------------------
| Iteration            | 15        |
| ExpectedImprovement  | 0.022687  |
| ActualImprovement    | 0.013512  |
| ImprovementRatio     | 0.59559   |
| MeanKL               | 0.0057329 |
| Entropy              | 0.55679   |
| Perplexity           | 1.7451    |
| AveragePolicyProb[0] | 0.50272   |
| AveragePolicyProb[1] | 0.49728   |
| AverageReturn        | 175.04    |
| MinReturn            | 56        |
| MaxReturn            | 200       |
| StdReturn            | 35.95     |
| AverageEpisodeLength | 175.04    |
| MinEpisodeLength     | 56        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 35.95     |
| TotalNEpisodes       | 385       |
| TotalNSamples        | 30357     |
| ExplainedVariance    | 0.81198   |
------------------------------------
[2018-07-02 16:08:34.516623 UTC] Saving snapshot
[2018-07-02 16:08:34.528646 UTC] Starting iteration 16
[2018-07-02 16:08:34.529323 UTC] Start collecting samples
[2018-07-02 16:08:34.909080 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:34.931863 UTC] Performing policy update
[2018-07-02 16:08:34.932950 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:34.942242 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:35.021627 UTC] Performing line search
[2018-07-02 16:08:35.028378 UTC] Updating baseline
[2018-07-02 16:08:35.117836 UTC] Computing logging information
------------------------------------
| Iteration            | 16        |
| ExpectedImprovement  | 0.021356  |
| ActualImprovement    | 0.012219  |
| ImprovementRatio     | 0.57214   |
| MeanKL               | 0.0080575 |
| Entropy              | 0.54645   |
| Perplexity           | 1.7271    |
| AveragePolicyProb[0] | 0.50401   |
| AveragePolicyProb[1] | 0.49599   |
| AverageReturn        | 177.42    |
| MinReturn            | 69        |
| MaxReturn            | 200       |
| StdReturn            | 33.957    |
| AverageEpisodeLength | 177.42    |
| MinEpisodeLength     | 69        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 33.957    |
| TotalNEpisodes       | 396       |
| TotalNSamples        | 32513     |
| ExplainedVariance    | 0.70691   |
------------------------------------
[2018-07-02 16:08:36.453531 UTC] Saving snapshot
[2018-07-02 16:08:36.462592 UTC] Starting iteration 17
[2018-07-02 16:08:36.463226 UTC] Start collecting samples
[2018-07-02 16:08:36.719442 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:36.733378 UTC] Performing policy update
[2018-07-02 16:08:36.734476 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:36.746783 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:36.824158 UTC] Performing line search
[2018-07-02 16:08:36.830729 UTC] Updating baseline
[2018-07-02 16:08:36.910008 UTC] Computing logging information
------------------------------------
| Iteration            | 17        |
| ExpectedImprovement  | 0.018952  |
| ActualImprovement    | 0.013304  |
| ImprovementRatio     | 0.70202   |
| MeanKL               | 0.0096248 |
| Entropy              | 0.55295   |
| Perplexity           | 1.7384    |
| AveragePolicyProb[0] | 0.50748   |
| AveragePolicyProb[1] | 0.49252   |
| AverageReturn        | 178.65    |
| MinReturn            | 69        |
| MaxReturn            | 200       |
| StdReturn            | 34.186    |
| AverageEpisodeLength | 178.65    |
| MinEpisodeLength     | 69        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 34.186    |
| TotalNEpisodes       | 405       |
| TotalNSamples        | 34308     |
| ExplainedVariance    | 0.78423   |
------------------------------------
[2018-07-02 16:08:37.764517 UTC] Saving snapshot
[2018-07-02 16:08:37.773536 UTC] Starting iteration 18
[2018-07-02 16:08:37.774219 UTC] Start collecting samples
[2018-07-02 16:08:38.045079 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:38.064021 UTC] Performing policy update
[2018-07-02 16:08:38.065052 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:38.075383 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:38.173378 UTC] Performing line search
[2018-07-02 16:08:38.179878 UTC] Updating baseline
[2018-07-02 16:08:38.261473 UTC] Computing logging information
-----------------------------------
| Iteration            | 18       |
| ExpectedImprovement  | 0.020206 |
| ActualImprovement    | 0.013893 |
| ImprovementRatio     | 0.68759  |
| MeanKL               | 0.007439 |
| Entropy              | 0.52458  |
| Perplexity           | 1.6897   |
| AveragePolicyProb[0] | 0.4811   |
| AveragePolicyProb[1] | 0.5189   |
| AverageReturn        | 181.79   |
| MinReturn            | 69       |
| MaxReturn            | 200      |
| StdReturn            | 32.578   |
| AverageEpisodeLength | 181.79   |
| MinEpisodeLength     | 69       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 32.578   |
| TotalNEpisodes       | 415      |
| TotalNSamples        | 36221    |
| ExplainedVariance    | 0.53088  |
-----------------------------------
[2018-07-02 16:08:39.432581 UTC] Saving snapshot
[2018-07-02 16:08:39.441170 UTC] Starting iteration 19
[2018-07-02 16:08:39.442263 UTC] Start collecting samples
[2018-07-02 16:08:39.702758 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:39.725943 UTC] Performing policy update
[2018-07-02 16:08:39.727897 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:39.738424 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:39.837569 UTC] Performing line search
[2018-07-02 16:08:39.846239 UTC] Updating baseline
[2018-07-02 16:08:39.932522 UTC] Computing logging information
------------------------------------
| Iteration            | 19        |
| ExpectedImprovement  | 0.01787   |
| ActualImprovement    | 0.0069319 |
| ImprovementRatio     | 0.38792   |
| MeanKL               | 0.0051355 |
| Entropy              | 0.52577   |
| Perplexity           | 1.6918    |
| AveragePolicyProb[0] | 0.48838   |
| AveragePolicyProb[1] | 0.51162   |
| AverageReturn        | 184.39    |
| MinReturn            | 69        |
| MaxReturn            | 200       |
| StdReturn            | 30.514    |
| AverageEpisodeLength | 184.39    |
| MinEpisodeLength     | 69        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 30.514    |
| TotalNEpisodes       | 427       |
| TotalNSamples        | 38565     |
| ExplainedVariance    | 0.54963   |
------------------------------------
[2018-07-02 16:08:40.999211 UTC] Saving snapshot
[2018-07-02 16:08:41.008650 UTC] Starting iteration 20
[2018-07-02 16:08:41.009819 UTC] Start collecting samples
[2018-07-02 16:08:41.320540 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:41.335990 UTC] Performing policy update
[2018-07-02 16:08:41.337624 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:41.346267 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:41.426487 UTC] Performing line search
[2018-07-02 16:08:41.431844 UTC] Updating baseline
[2018-07-02 16:08:41.515549 UTC] Computing logging information
------------------------------------
| Iteration            | 20        |
| ExpectedImprovement  | 0.020249  |
| ActualImprovement    | 0.0073768 |
| ImprovementRatio     | 0.3643    |
| MeanKL               | 0.0089851 |
| Entropy              | 0.52812   |
| Perplexity           | 1.6957    |
| AveragePolicyProb[0] | 0.50487   |
| AveragePolicyProb[1] | 0.49513   |
| AverageReturn        | 186.28    |
| MinReturn            | 69        |
| MaxReturn            | 200       |
| StdReturn            | 28.29     |
| AverageEpisodeLength | 186.28    |
| MinEpisodeLength     | 69        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 28.29     |
| TotalNEpisodes       | 436       |
| TotalNSamples        | 40365     |
| ExplainedVariance    | 0.64303   |
------------------------------------
[2018-07-02 16:08:42.663325 UTC] Saving snapshot
[2018-07-02 16:08:42.671211 UTC] Starting iteration 21
[2018-07-02 16:08:42.673010 UTC] Start collecting samples
[2018-07-02 16:08:42.960052 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:42.981131 UTC] Performing policy update
[2018-07-02 16:08:42.982284 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:42.993521 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:43.076177 UTC] Performing line search
[2018-07-02 16:08:43.087061 UTC] Updating baseline
[2018-07-02 16:08:43.165431 UTC] Computing logging information
------------------------------------
| Iteration            | 21        |
| ExpectedImprovement  | 0.011712  |
| ActualImprovement    | 0.0071429 |
| ImprovementRatio     | 0.60988   |
| MeanKL               | 0.0068688 |
| Entropy              | 0.51822   |
| Perplexity           | 1.679     |
| AveragePolicyProb[0] | 0.50266   |
| AveragePolicyProb[1] | 0.49734   |
| AverageReturn        | 190.63    |
| MinReturn            | 83        |
| MaxReturn            | 200       |
| StdReturn            | 21.614    |
| AverageEpisodeLength | 190.63    |
| MinEpisodeLength     | 83        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 21.614    |
| TotalNEpisodes       | 447       |
| TotalNSamples        | 42565     |
| ExplainedVariance    | 0.54662   |
------------------------------------
[2018-07-02 16:08:44.007265 UTC] Saving snapshot
[2018-07-02 16:08:44.016490 UTC] Starting iteration 22
[2018-07-02 16:08:44.017144 UTC] Start collecting samples
[2018-07-02 16:08:44.323498 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:44.338682 UTC] Performing policy update
[2018-07-02 16:08:44.339751 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:44.350333 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:44.428019 UTC] Performing line search
[2018-07-02 16:08:44.433602 UTC] Updating baseline
[2018-07-02 16:08:44.517811 UTC] Computing logging information
------------------------------------
| Iteration            | 22        |
| ExpectedImprovement  | 0.013051  |
| ActualImprovement    | 0.0099952 |
| ImprovementRatio     | 0.76586   |
| MeanKL               | 0.0092547 |
| Entropy              | 0.52829   |
| Perplexity           | 1.696     |
| AveragePolicyProb[0] | 0.50263   |
| AveragePolicyProb[1] | 0.49737   |
| AverageReturn        | 193.57    |
| MinReturn            | 83        |
| MaxReturn            | 200       |
| StdReturn            | 17.478    |
| AverageEpisodeLength | 193.57    |
| MinEpisodeLength     | 83        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 17.478    |
| TotalNEpisodes       | 456       |
| TotalNSamples        | 44365     |
| ExplainedVariance    | 0.45376   |
------------------------------------
[2018-07-02 16:08:45.327737 UTC] Saving snapshot
[2018-07-02 16:08:45.336472 UTC] Starting iteration 23
[2018-07-02 16:08:45.337047 UTC] Start collecting samples
[2018-07-02 16:08:45.632758 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:45.650465 UTC] Performing policy update
[2018-07-02 16:08:45.651324 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:45.659250 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:45.738835 UTC] Performing line search
[2018-07-02 16:08:45.744248 UTC] Updating baseline
[2018-07-02 16:08:45.831906 UTC] Computing logging information
------------------------------------
| Iteration            | 23        |
| ExpectedImprovement  | 0.027689  |
| ActualImprovement    | 0.013512  |
| ImprovementRatio     | 0.48798   |
| MeanKL               | 0.0060245 |
| Entropy              | 0.54924   |
| Perplexity           | 1.7319    |
| AveragePolicyProb[0] | 0.47882   |
| AveragePolicyProb[1] | 0.52118   |
| AverageReturn        | 194.56    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 18.883    |
| AverageEpisodeLength | 194.56    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 18.883    |
| TotalNEpisodes       | 466       |
| TotalNSamples        | 46245     |
| ExplainedVariance    | 0.49255   |
------------------------------------
[2018-07-02 16:08:46.760300 UTC] Saving snapshot
[2018-07-02 16:08:46.769903 UTC] Starting iteration 24
[2018-07-02 16:08:46.770587 UTC] Start collecting samples
[2018-07-02 16:08:47.147656 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:47.172775 UTC] Performing policy update
[2018-07-02 16:08:47.174028 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:47.185870 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:47.267759 UTC] Performing line search
[2018-07-02 16:08:47.274470 UTC] Updating baseline
[2018-07-02 16:08:47.352135 UTC] Computing logging information
------------------------------------
| Iteration            | 24        |
| ExpectedImprovement  | 0.018267  |
| ActualImprovement    | 0.012156  |
| ImprovementRatio     | 0.66545   |
| MeanKL               | 0.0065661 |
| Entropy              | 0.5368    |
| Perplexity           | 1.7105    |
| AveragePolicyProb[0] | 0.488     |
| AveragePolicyProb[1] | 0.512     |
| AverageReturn        | 196.25    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 15.037    |
| AverageEpisodeLength | 196.25    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 15.037    |
| TotalNEpisodes       | 477       |
| TotalNSamples        | 48445     |
| ExplainedVariance    | 0.25438   |
------------------------------------
[2018-07-02 16:08:48.456721 UTC] Saving snapshot
[2018-07-02 16:08:48.470575 UTC] Starting iteration 25
[2018-07-02 16:08:48.471480 UTC] Start collecting samples
[2018-07-02 16:08:48.812594 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:48.848420 UTC] Performing policy update
[2018-07-02 16:08:48.849985 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:48.867853 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:48.985826 UTC] Performing line search
[2018-07-02 16:08:48.993052 UTC] Updating baseline
[2018-07-02 16:08:49.104715 UTC] Computing logging information
------------------------------------
| Iteration            | 25        |
| ExpectedImprovement  | 0.017104  |
| ActualImprovement    | 0.012906  |
| ImprovementRatio     | 0.75458   |
| MeanKL               | 0.0072478 |
| Entropy              | 0.52527   |
| Perplexity           | 1.6909    |
| AveragePolicyProb[0] | 0.48982   |
| AveragePolicyProb[1] | 0.51018   |
| AverageReturn        | 197.04    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 14.451    |
| AverageEpisodeLength | 197.04    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 14.451    |
| TotalNEpisodes       | 487       |
| TotalNSamples        | 50442     |
| ExplainedVariance    | 0.40098   |
------------------------------------
[2018-07-02 16:08:49.903381 UTC] Saving snapshot
[2018-07-02 16:08:49.912076 UTC] Starting iteration 26
[2018-07-02 16:08:49.912734 UTC] Start collecting samples
[2018-07-02 16:08:50.216617 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:50.238237 UTC] Performing policy update
[2018-07-02 16:08:50.239619 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:50.256358 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:50.383565 UTC] Performing line search
[2018-07-02 16:08:50.393988 UTC] Updating baseline
[2018-07-02 16:08:50.490672 UTC] Computing logging information
------------------------------------
| Iteration            | 26        |
| ExpectedImprovement  | 0.020456  |
| ActualImprovement    | 0.013659  |
| ImprovementRatio     | 0.66776   |
| MeanKL               | 0.0065644 |
| Entropy              | 0.54372   |
| Perplexity           | 1.7224    |
| AveragePolicyProb[0] | 0.50611   |
| AveragePolicyProb[1] | 0.49389   |
| AverageReturn        | 197.29    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 14.282    |
| AverageEpisodeLength | 197.29    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 14.282    |
| TotalNEpisodes       | 495       |
| TotalNSamples        | 52042     |
| ExplainedVariance    | 0.79909   |
------------------------------------
[2018-07-02 16:08:51.333802 UTC] Saving snapshot
[2018-07-02 16:08:51.345603 UTC] Starting iteration 27
[2018-07-02 16:08:51.346466 UTC] Start collecting samples
[2018-07-02 16:08:51.724230 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:51.744874 UTC] Performing policy update
[2018-07-02 16:08:51.746173 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:51.756598 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:51.836404 UTC] Performing line search
[2018-07-02 16:08:51.843162 UTC] Updating baseline
[2018-07-02 16:08:51.925919 UTC] Computing logging information
------------------------------------
| Iteration            | 27        |
| ExpectedImprovement  | 0.017537  |
| ActualImprovement    | 0.010257  |
| ImprovementRatio     | 0.58487   |
| MeanKL               | 0.0090909 |
| Entropy              | 0.52944   |
| Perplexity           | 1.698     |
| AveragePolicyProb[0] | 0.49945   |
| AveragePolicyProb[1] | 0.50055   |
| AverageReturn        | 197.34    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 14.283    |
| AverageEpisodeLength | 197.34    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 14.283    |
| TotalNEpisodes       | 507       |
| TotalNSamples        | 54442     |
| ExplainedVariance    | 0.45788   |
------------------------------------
[2018-07-02 16:08:52.778038 UTC] Saving snapshot
[2018-07-02 16:08:52.785588 UTC] Starting iteration 28
[2018-07-02 16:08:52.786510 UTC] Start collecting samples
[2018-07-02 16:08:53.107512 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:53.122723 UTC] Performing policy update
[2018-07-02 16:08:53.123608 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:53.133428 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:53.213589 UTC] Performing line search
[2018-07-02 16:08:53.218328 UTC] Updating baseline
[2018-07-02 16:08:53.300738 UTC] Computing logging information
------------------------------------
| Iteration            | 28        |
| ExpectedImprovement  | 0.02235   |
| ActualImprovement    | 0.016105  |
| ImprovementRatio     | 0.72058   |
| MeanKL               | 0.0074602 |
| Entropy              | 0.51317   |
| Perplexity           | 1.6706    |
| AveragePolicyProb[0] | 0.501     |
| AveragePolicyProb[1] | 0.499     |
| AverageReturn        | 198.21    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 12.609    |
| AverageEpisodeLength | 198.21    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 12.609    |
| TotalNEpisodes       | 517       |
| TotalNSamples        | 56442     |
| ExplainedVariance    | 0.84096   |
------------------------------------
[2018-07-02 16:08:54.108491 UTC] Saving snapshot
[2018-07-02 16:08:54.118132 UTC] Starting iteration 29
[2018-07-02 16:08:54.119010 UTC] Start collecting samples
[2018-07-02 16:08:54.360707 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:54.377552 UTC] Performing policy update
[2018-07-02 16:08:54.379202 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:54.388615 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:54.472114 UTC] Performing line search
[2018-07-02 16:08:54.476963 UTC] Updating baseline
[2018-07-02 16:08:54.597243 UTC] Computing logging information
------------------------------------
| Iteration            | 29        |
| ExpectedImprovement  | 0.021469  |
| ActualImprovement    | 0.01338   |
| ImprovementRatio     | 0.62324   |
| MeanKL               | 0.0072699 |
| Entropy              | 0.50884   |
| Perplexity           | 1.6634    |
| AveragePolicyProb[0] | 0.50508   |
| AveragePolicyProb[1] | 0.49492   |
| AverageReturn        | 198.77    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 11.941    |
| AverageEpisodeLength | 198.77    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 11.941    |
| TotalNEpisodes       | 527       |
| TotalNSamples        | 58442     |
| ExplainedVariance    | 0.46372   |
------------------------------------
[2018-07-02 16:08:55.453723 UTC] Saving snapshot
[2018-07-02 16:08:55.462508 UTC] Starting iteration 30
[2018-07-02 16:08:55.463441 UTC] Start collecting samples
[2018-07-02 16:08:55.753784 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:55.780281 UTC] Performing policy update
[2018-07-02 16:08:55.781300 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:55.794675 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:55.902251 UTC] Performing line search
[2018-07-02 16:08:55.916140 UTC] Updating baseline
[2018-07-02 16:08:56.009258 UTC] Computing logging information
------------------------------------
| Iteration            | 30        |
| ExpectedImprovement  | 0.013844  |
| ActualImprovement    | 0.013294  |
| ImprovementRatio     | 0.96031   |
| MeanKL               | 0.0070825 |
| Entropy              | 0.49829   |
| Perplexity           | 1.6459    |
| AveragePolicyProb[0] | 0.48875   |
| AveragePolicyProb[1] | 0.51125   |
| AverageReturn        | 198.77    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 11.941    |
| AverageEpisodeLength | 198.77    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 11.941    |
| TotalNEpisodes       | 537       |
| TotalNSamples        | 60442     |
| ExplainedVariance    | 0.64632   |
------------------------------------
[2018-07-02 16:08:56.833349 UTC] Saving snapshot
[2018-07-02 16:08:56.842426 UTC] Starting iteration 31
[2018-07-02 16:08:56.843264 UTC] Start collecting samples
[2018-07-02 16:08:57.126328 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:57.160263 UTC] Performing policy update
[2018-07-02 16:08:57.162298 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:57.176673 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:57.277652 UTC] Performing line search
[2018-07-02 16:08:57.282690 UTC] Updating baseline
[2018-07-02 16:08:57.367565 UTC] Computing logging information
------------------------------------
| Iteration            | 31        |
| ExpectedImprovement  | 0.019214  |
| ActualImprovement    | 0.011918  |
| ImprovementRatio     | 0.62031   |
| MeanKL               | 0.0072267 |
| Entropy              | 0.50216   |
| Perplexity           | 1.6523    |
| AveragePolicyProb[0] | 0.49425   |
| AveragePolicyProb[1] | 0.50575   |
| AverageReturn        | 198.77    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 11.941    |
| AverageEpisodeLength | 198.77    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 11.941    |
| TotalNEpisodes       | 546       |
| TotalNSamples        | 62242     |
| ExplainedVariance    | 0.82319   |
------------------------------------
[2018-07-02 16:08:58.351796 UTC] Saving snapshot
[2018-07-02 16:08:58.360569 UTC] Starting iteration 32
[2018-07-02 16:08:58.361314 UTC] Start collecting samples
[2018-07-02 16:08:58.678962 UTC] Computing input variables for policy optimization
[2018-07-02 16:08:58.695539 UTC] Performing policy update
[2018-07-02 16:08:58.696536 UTC] Computing gradient in Euclidean space
[2018-07-02 16:08:58.705231 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:08:58.808945 UTC] Performing line search
[2018-07-02 16:08:58.815430 UTC] Updating baseline
[2018-07-02 16:08:58.934966 UTC] Computing logging information
------------------------------------
| Iteration            | 32        |
| ExpectedImprovement  | 0.017899  |
| ActualImprovement    | 0.0087773 |
| ImprovementRatio     | 0.49038   |
| MeanKL               | 0.0078339 |
| Entropy              | 0.50635   |
| Perplexity           | 1.6592    |
| AveragePolicyProb[0] | 0.49255   |
| AveragePolicyProb[1] | 0.50745   |
| AverageReturn        | 198.77    |
| MinReturn            | 80        |
| MaxReturn            | 200       |
| StdReturn            | 11.941    |
| AverageEpisodeLength | 198.77    |
| MinEpisodeLength     | 80        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 11.941    |
| TotalNEpisodes       | 557       |
| TotalNSamples        | 64442     |
| ExplainedVariance    | 0.6299    |
------------------------------------
[2018-07-02 16:08:59.820039 UTC] Saving snapshot
[2018-07-02 16:08:59.829738 UTC] Starting iteration 33
[2018-07-02 16:08:59.830357 UTC] Start collecting samples
[2018-07-02 16:09:00.211511 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:00.231948 UTC] Performing policy update
[2018-07-02 16:09:00.233062 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:00.243066 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:00.323703 UTC] Performing line search
[2018-07-02 16:09:00.328752 UTC] Updating baseline
[2018-07-02 16:09:00.412181 UTC] Computing logging information
------------------------------------
| Iteration            | 33        |
| ExpectedImprovement  | 0.022867  |
| ActualImprovement    | 0.015454  |
| ImprovementRatio     | 0.6758    |
| MeanKL               | 0.0087784 |
| Entropy              | 0.51275   |
| Perplexity           | 1.6699    |
| AveragePolicyProb[0] | 0.49929   |
| AveragePolicyProb[1] | 0.50071   |
| AverageReturn        | 199.97    |
| MinReturn            | 197       |
| MaxReturn            | 200       |
| StdReturn            | 0.2985    |
| AverageEpisodeLength | 199.97    |
| MinEpisodeLength     | 197       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0.2985    |
| TotalNEpisodes       | 567       |
| TotalNSamples        | 66442     |
| ExplainedVariance    | 0.76919   |
------------------------------------
[2018-07-02 16:09:01.214784 UTC] Saving snapshot
[2018-07-02 16:09:01.223777 UTC] Starting iteration 34
[2018-07-02 16:09:01.224348 UTC] Start collecting samples
[2018-07-02 16:09:01.497769 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:01.518451 UTC] Performing policy update
[2018-07-02 16:09:01.519506 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:01.528633 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:01.607218 UTC] Performing line search
[2018-07-02 16:09:01.617772 UTC] Updating baseline
[2018-07-02 16:09:01.703975 UTC] Computing logging information
------------------------------------
| Iteration            | 34        |
| ExpectedImprovement  | 0.013033  |
| ActualImprovement    | 0.0090645 |
| ImprovementRatio     | 0.6955    |
| MeanKL               | 0.0066992 |
| Entropy              | 0.51384   |
| Perplexity           | 1.6717    |
| AveragePolicyProb[0] | 0.50626   |
| AveragePolicyProb[1] | 0.49374   |
| AverageReturn        | 199.97    |
| MinReturn            | 197       |
| MaxReturn            | 200       |
| StdReturn            | 0.2985    |
| AverageEpisodeLength | 199.97    |
| MinEpisodeLength     | 197       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0.2985    |
| TotalNEpisodes       | 575       |
| TotalNSamples        | 68042     |
| ExplainedVariance    | 0.35097   |
------------------------------------
[2018-07-02 16:09:02.522803 UTC] Saving snapshot
[2018-07-02 16:09:02.534198 UTC] Starting iteration 35
[2018-07-02 16:09:02.535022 UTC] Start collecting samples
[2018-07-02 16:09:02.912163 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:02.933387 UTC] Performing policy update
[2018-07-02 16:09:02.934477 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:02.943368 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:03.025059 UTC] Performing line search
[2018-07-02 16:09:03.031824 UTC] Updating baseline
[2018-07-02 16:09:03.113940 UTC] Computing logging information
------------------------------------
| Iteration            | 35        |
| ExpectedImprovement  | 0.016133  |
| ActualImprovement    | 0.0083147 |
| ImprovementRatio     | 0.5154    |
| MeanKL               | 0.0080501 |
| Entropy              | 0.5023    |
| Perplexity           | 1.6525    |
| AveragePolicyProb[0] | 0.51704   |
| AveragePolicyProb[1] | 0.48296   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 587       |
| TotalNSamples        | 70442     |
| ExplainedVariance    | 0.43314   |
------------------------------------
[2018-07-02 16:09:04.132895 UTC] Saving snapshot
[2018-07-02 16:09:04.142305 UTC] Starting iteration 36
[2018-07-02 16:09:04.144149 UTC] Start collecting samples
[2018-07-02 16:09:04.433860 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:04.454011 UTC] Performing policy update
[2018-07-02 16:09:04.455106 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:04.464690 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:04.547872 UTC] Performing line search
[2018-07-02 16:09:04.553158 UTC] Updating baseline
[2018-07-02 16:09:04.648540 UTC] Computing logging information
------------------------------------
| Iteration            | 36        |
| ExpectedImprovement  | 0.013509  |
| ActualImprovement    | 0.0099222 |
| ImprovementRatio     | 0.73449   |
| MeanKL               | 0.0074007 |
| Entropy              | 0.50661   |
| Perplexity           | 1.6597    |
| AveragePolicyProb[0] | 0.49279   |
| AveragePolicyProb[1] | 0.50721   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 597       |
| TotalNSamples        | 72442     |
| ExplainedVariance    | 0.31546   |
------------------------------------
[2018-07-02 16:09:06.537582 UTC] Saving snapshot
[2018-07-02 16:09:06.548977 UTC] Starting iteration 37
[2018-07-02 16:09:06.550050 UTC] Start collecting samples
[2018-07-02 16:09:06.893597 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:06.911238 UTC] Performing policy update
[2018-07-02 16:09:06.912699 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:06.924780 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:07.006054 UTC] Performing line search
[2018-07-02 16:09:07.012984 UTC] Updating baseline
[2018-07-02 16:09:07.102999 UTC] Computing logging information
------------------------------------
| Iteration            | 37        |
| ExpectedImprovement  | 0.023395  |
| ActualImprovement    | 0.010316  |
| ImprovementRatio     | 0.44095   |
| MeanKL               | 0.0071043 |
| Entropy              | 0.49526   |
| Perplexity           | 1.6409    |
| AveragePolicyProb[0] | 0.51142   |
| AveragePolicyProb[1] | 0.48858   |
| AverageReturn        | 199.04    |
| MinReturn            | 104       |
| MaxReturn            | 200       |
| StdReturn            | 9.5519    |
| AverageEpisodeLength | 199.04    |
| MinEpisodeLength     | 104       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 9.5519    |
| TotalNEpisodes       | 608       |
| TotalNSamples        | 74546     |
| ExplainedVariance    | 0.27937   |
------------------------------------
[2018-07-02 16:09:08.180172 UTC] Saving snapshot
[2018-07-02 16:09:08.190457 UTC] Starting iteration 38
[2018-07-02 16:09:08.192285 UTC] Start collecting samples
[2018-07-02 16:09:08.490491 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:08.508901 UTC] Performing policy update
[2018-07-02 16:09:08.510497 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:08.524517 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:08.609903 UTC] Performing line search
[2018-07-02 16:09:08.614691 UTC] Updating baseline
[2018-07-02 16:09:08.698302 UTC] Computing logging information
------------------------------------
| Iteration            | 38        |
| ExpectedImprovement  | 0.016423  |
| ActualImprovement    | 0.013203  |
| ImprovementRatio     | 0.80393   |
| MeanKL               | 0.0098042 |
| Entropy              | 0.47869   |
| Perplexity           | 1.614     |
| AveragePolicyProb[0] | 0.51533   |
| AveragePolicyProb[1] | 0.48467   |
| AverageReturn        | 199.04    |
| MinReturn            | 104       |
| MaxReturn            | 200       |
| StdReturn            | 9.5519    |
| AverageEpisodeLength | 199.04    |
| MinEpisodeLength     | 104       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 9.5519    |
| TotalNEpisodes       | 617       |
| TotalNSamples        | 76346     |
| ExplainedVariance    | -0.13592  |
------------------------------------
[2018-07-02 16:09:09.558786 UTC] Saving snapshot
[2018-07-02 16:09:09.569082 UTC] Starting iteration 39
[2018-07-02 16:09:09.569969 UTC] Start collecting samples
[2018-07-02 16:09:09.826790 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:09.841866 UTC] Performing policy update
[2018-07-02 16:09:09.842884 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:09.852312 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:09.925187 UTC] Performing line search
[2018-07-02 16:09:09.930038 UTC] Updating baseline
[2018-07-02 16:09:10.021958 UTC] Computing logging information
------------------------------------
| Iteration            | 39        |
| ExpectedImprovement  | 0.016354  |
| ActualImprovement    | 0.0082362 |
| ImprovementRatio     | 0.50363   |
| MeanKL               | 0.0095238 |
| Entropy              | 0.47198   |
| Perplexity           | 1.6032    |
| AveragePolicyProb[0] | 0.50249   |
| AveragePolicyProb[1] | 0.49751   |
| AverageReturn        | 199.04    |
| MinReturn            | 104       |
| MaxReturn            | 200       |
| StdReturn            | 9.5519    |
| AverageEpisodeLength | 199.04    |
| MinEpisodeLength     | 104       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 9.5519    |
| TotalNEpisodes       | 626       |
| TotalNSamples        | 78146     |
| ExplainedVariance    | 0.188     |
------------------------------------
[2018-07-02 16:09:11.284303 UTC] Saving snapshot
[2018-07-02 16:09:11.293162 UTC] Starting iteration 40
[2018-07-02 16:09:11.294368 UTC] Start collecting samples
[2018-07-02 16:09:11.599394 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:11.620055 UTC] Performing policy update
[2018-07-02 16:09:11.621485 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:11.630897 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:11.709252 UTC] Performing line search
[2018-07-02 16:09:11.719185 UTC] Updating baseline
[2018-07-02 16:09:11.808953 UTC] Computing logging information
------------------------------------
| Iteration            | 40        |
| ExpectedImprovement  | 0.01173   |
| ActualImprovement    | 0.0093036 |
| ImprovementRatio     | 0.79316   |
| MeanKL               | 0.0068129 |
| Entropy              | 0.46039   |
| Perplexity           | 1.5847    |
| AveragePolicyProb[0] | 0.50112   |
| AveragePolicyProb[1] | 0.49888   |
| AverageReturn        | 199.04    |
| MinReturn            | 104       |
| MaxReturn            | 200       |
| StdReturn            | 9.5519    |
| AverageEpisodeLength | 199.04    |
| MinEpisodeLength     | 104       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 9.5519    |
| TotalNEpisodes       | 638       |
| TotalNSamples        | 80546     |
| ExplainedVariance    | 0.25895   |
------------------------------------
[2018-07-02 16:09:12.935502 UTC] Saving snapshot
[2018-07-02 16:09:12.945550 UTC] Starting iteration 41
[2018-07-02 16:09:12.946275 UTC] Start collecting samples
[2018-07-02 16:09:13.260844 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:13.280189 UTC] Performing policy update
[2018-07-02 16:09:13.281492 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:13.292284 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:13.373632 UTC] Performing line search
[2018-07-02 16:09:13.381154 UTC] Updating baseline
[2018-07-02 16:09:13.471355 UTC] Computing logging information
------------------------------------
| Iteration            | 41        |
| ExpectedImprovement  | 0.0069292 |
| ActualImprovement    | 0.0072198 |
| ImprovementRatio     | 1.0419    |
| MeanKL               | 0.0067996 |
| Entropy              | 0.46232   |
| Perplexity           | 1.5877    |
| AveragePolicyProb[0] | 0.49388   |
| AveragePolicyProb[1] | 0.50612   |
| AverageReturn        | 199.04    |
| MinReturn            | 104       |
| MaxReturn            | 200       |
| StdReturn            | 9.5519    |
| AverageEpisodeLength | 199.04    |
| MinEpisodeLength     | 104       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 9.5519    |
| TotalNEpisodes       | 647       |
| TotalNSamples        | 82346     |
| ExplainedVariance    | 0.19516   |
------------------------------------
[2018-07-02 16:09:14.401806 UTC] Saving snapshot
[2018-07-02 16:09:14.411157 UTC] Starting iteration 42
[2018-07-02 16:09:14.412013 UTC] Start collecting samples
[2018-07-02 16:09:14.702227 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:14.722340 UTC] Performing policy update
[2018-07-02 16:09:14.723743 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:14.734600 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:14.803258 UTC] Performing line search
[2018-07-02 16:09:14.808157 UTC] Updating baseline
[2018-07-02 16:09:14.886877 UTC] Computing logging information
------------------------------------
| Iteration            | 42        |
| ExpectedImprovement  | 0.017883  |
| ActualImprovement    | 0.0059608 |
| ImprovementRatio     | 0.33333   |
| MeanKL               | 0.0054238 |
| Entropy              | 0.4728    |
| Perplexity           | 1.6045    |
| AveragePolicyProb[0] | 0.51013   |
| AveragePolicyProb[1] | 0.48987   |
| AverageReturn        | 199.04    |
| MinReturn            | 104       |
| MaxReturn            | 200       |
| StdReturn            | 9.5519    |
| AverageEpisodeLength | 199.04    |
| MinEpisodeLength     | 104       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 9.5519    |
| TotalNEpisodes       | 656       |
| TotalNSamples        | 84146     |
| ExplainedVariance    | 0.48684   |
------------------------------------
[2018-07-02 16:09:15.924627 UTC] Saving snapshot
[2018-07-02 16:09:15.934177 UTC] Starting iteration 43
[2018-07-02 16:09:15.935834 UTC] Start collecting samples
[2018-07-02 16:09:16.201855 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:16.220110 UTC] Performing policy update
[2018-07-02 16:09:16.221369 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:16.230083 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:16.299114 UTC] Performing line search
[2018-07-02 16:09:16.305750 UTC] Updating baseline
[2018-07-02 16:09:16.388119 UTC] Computing logging information
------------------------------------
| Iteration            | 43        |
| ExpectedImprovement  | 0.016455  |
| ActualImprovement    | 0.0068024 |
| ImprovementRatio     | 0.4134    |
| MeanKL               | 0.0075803 |
| Entropy              | 0.43209   |
| Perplexity           | 1.5405    |
| AveragePolicyProb[0] | 0.48995   |
| AveragePolicyProb[1] | 0.51005   |
| AverageReturn        | 199.04    |
| MinReturn            | 104       |
| MaxReturn            | 200       |
| StdReturn            | 9.5519    |
| AverageEpisodeLength | 199.04    |
| MinEpisodeLength     | 104       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 9.5519    |
| TotalNEpisodes       | 668       |
| TotalNSamples        | 86546     |
| ExplainedVariance    | 0.41419   |
------------------------------------
[2018-07-02 16:09:17.255407 UTC] Saving snapshot
[2018-07-02 16:09:17.266288 UTC] Starting iteration 44
[2018-07-02 16:09:17.267277 UTC] Start collecting samples
[2018-07-02 16:09:17.563106 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:17.580677 UTC] Performing policy update
[2018-07-02 16:09:17.582567 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:17.591161 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:17.661649 UTC] Performing line search
[2018-07-02 16:09:17.668507 UTC] Updating baseline
[2018-07-02 16:09:17.747346 UTC] Computing logging information
------------------------------------
| Iteration            | 44        |
| ExpectedImprovement  | 0.01548   |
| ActualImprovement    | 0.0086258 |
| ImprovementRatio     | 0.55723   |
| MeanKL               | 0.0066104 |
| Entropy              | 0.41791   |
| Perplexity           | 1.5188    |
| AveragePolicyProb[0] | 0.48104   |
| AveragePolicyProb[1] | 0.51896   |
| AverageReturn        | 199.04    |
| MinReturn            | 104       |
| MaxReturn            | 200       |
| StdReturn            | 9.5519    |
| AverageEpisodeLength | 199.04    |
| MinEpisodeLength     | 104       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 9.5519    |
| TotalNEpisodes       | 677       |
| TotalNSamples        | 88346     |
| ExplainedVariance    | 0.43535   |
------------------------------------
[2018-07-02 16:09:18.508711 UTC] Saving snapshot
[2018-07-02 16:09:18.517834 UTC] Starting iteration 45
[2018-07-02 16:09:18.518324 UTC] Start collecting samples
[2018-07-02 16:09:18.800953 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:18.817709 UTC] Performing policy update
[2018-07-02 16:09:18.818410 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:18.826003 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:18.892271 UTC] Performing line search
[2018-07-02 16:09:18.897190 UTC] Updating baseline
[2018-07-02 16:09:18.976996 UTC] Computing logging information
------------------------------------
| Iteration            | 45        |
| ExpectedImprovement  | 0.01759   |
| ActualImprovement    | 0.010196  |
| ImprovementRatio     | 0.57963   |
| MeanKL               | 0.0084716 |
| Entropy              | 0.44339   |
| Perplexity           | 1.558     |
| AveragePolicyProb[0] | 0.48759   |
| AveragePolicyProb[1] | 0.51241   |
| AverageReturn        | 199.02    |
| MinReturn            | 104       |
| MaxReturn            | 200       |
| StdReturn            | 9.5519    |
| AverageEpisodeLength | 199.02    |
| MinEpisodeLength     | 104       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 9.5519    |
| TotalNEpisodes       | 688       |
| TotalNSamples        | 90544     |
| ExplainedVariance    | 0.48772   |
------------------------------------
[2018-07-02 16:09:20.159413 UTC] Saving snapshot
[2018-07-02 16:09:20.167700 UTC] Starting iteration 46
[2018-07-02 16:09:20.168259 UTC] Start collecting samples
[2018-07-02 16:09:20.434069 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:20.451117 UTC] Performing policy update
[2018-07-02 16:09:20.451979 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:20.460742 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:20.547371 UTC] Performing line search
[2018-07-02 16:09:20.559959 UTC] Updating baseline
[2018-07-02 16:09:20.682266 UTC] Computing logging information
------------------------------------
| Iteration            | 46        |
| ExpectedImprovement  | 0.015243  |
| ActualImprovement    | 0.0075036 |
| ImprovementRatio     | 0.49227   |
| MeanKL               | 0.0063021 |
| Entropy              | 0.44874   |
| Perplexity           | 1.5663    |
| AveragePolicyProb[0] | 0.49023   |
| AveragePolicyProb[1] | 0.50977   |
| AverageReturn        | 199.02    |
| MinReturn            | 104       |
| MaxReturn            | 200       |
| StdReturn            | 9.5519    |
| AverageEpisodeLength | 199.02    |
| MinEpisodeLength     | 104       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 9.5519    |
| TotalNEpisodes       | 697       |
| TotalNSamples        | 92344     |
| ExplainedVariance    | 0.46541   |
------------------------------------
[2018-07-02 16:09:21.420281 UTC] Saving snapshot
[2018-07-02 16:09:21.427295 UTC] Starting iteration 47
[2018-07-02 16:09:21.427805 UTC] Start collecting samples
[2018-07-02 16:09:21.719616 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:21.738227 UTC] Performing policy update
[2018-07-02 16:09:21.739218 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:21.749747 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:21.824705 UTC] Performing line search
[2018-07-02 16:09:21.831255 UTC] Updating baseline
[2018-07-02 16:09:21.918817 UTC] Computing logging information
------------------------------------
| Iteration            | 47        |
| ExpectedImprovement  | 0.023194  |
| ActualImprovement    | 0.015251  |
| ImprovementRatio     | 0.65754   |
| MeanKL               | 0.0071357 |
| Entropy              | 0.45691   |
| Perplexity           | 1.5792    |
| AveragePolicyProb[0] | 0.50796   |
| AveragePolicyProb[1] | 0.49204   |
| AverageReturn        | 199.98    |
| MinReturn            | 198       |
| MaxReturn            | 200       |
| StdReturn            | 0.199     |
| AverageEpisodeLength | 199.98    |
| MinEpisodeLength     | 198       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0.199     |
| TotalNEpisodes       | 706       |
| TotalNSamples        | 94144     |
| ExplainedVariance    | 0.60841   |
------------------------------------
[2018-07-02 16:09:22.725965 UTC] Saving snapshot
[2018-07-02 16:09:22.737372 UTC] Starting iteration 48
[2018-07-02 16:09:22.738398 UTC] Start collecting samples
[2018-07-02 16:09:23.072580 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:23.097327 UTC] Performing policy update
[2018-07-02 16:09:23.098588 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:23.111574 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:23.210386 UTC] Performing line search
[2018-07-02 16:09:23.221932 UTC] Updating baseline
[2018-07-02 16:09:23.302088 UTC] Computing logging information
------------------------------------
| Iteration            | 48        |
| ExpectedImprovement  | 0.0097026 |
| ActualImprovement    | 0.0080255 |
| ImprovementRatio     | 0.82714   |
| MeanKL               | 0.0063274 |
| Entropy              | 0.46314   |
| Perplexity           | 1.5891    |
| AveragePolicyProb[0] | 0.49395   |
| AveragePolicyProb[1] | 0.50605   |
| AverageReturn        | 199.55    |
| MinReturn            | 157       |
| MaxReturn            | 200       |
| StdReturn            | 4.2811    |
| AverageEpisodeLength | 199.55    |
| MinEpisodeLength     | 157       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 4.2811    |
| TotalNEpisodes       | 718       |
| TotalNSamples        | 96501     |
| ExplainedVariance    | 0.44702   |
------------------------------------
[2018-07-02 16:09:23.992147 UTC] Saving snapshot
[2018-07-02 16:09:24.000280 UTC] Starting iteration 49
[2018-07-02 16:09:24.000808 UTC] Start collecting samples
[2018-07-02 16:09:24.295006 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:24.309974 UTC] Performing policy update
[2018-07-02 16:09:24.310885 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:24.319220 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:24.385704 UTC] Performing line search
[2018-07-02 16:09:24.395352 UTC] Updating baseline
[2018-07-02 16:09:24.480496 UTC] Computing logging information
------------------------------------
| Iteration            | 49        |
| ExpectedImprovement  | 0.0083803 |
| ActualImprovement    | 0.0041371 |
| ImprovementRatio     | 0.49367   |
| MeanKL               | 0.007349  |
| Entropy              | 0.49853   |
| Perplexity           | 1.6463    |
| AveragePolicyProb[0] | 0.50026   |
| AveragePolicyProb[1] | 0.49974   |
| AverageReturn        | 199.09    |
| MinReturn            | 157       |
| MaxReturn            | 200       |
| StdReturn            | 5.6269    |
| AverageEpisodeLength | 199.09    |
| MinEpisodeLength     | 157       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 5.6269    |
| TotalNEpisodes       | 728       |
| TotalNSamples        | 98455     |
| ExplainedVariance    | 0.5253    |
------------------------------------
[2018-07-02 16:09:25.261659 UTC] Saving snapshot
[2018-07-02 16:09:25.270319 UTC] Starting iteration 50
[2018-07-02 16:09:25.270942 UTC] Start collecting samples
[2018-07-02 16:09:25.549744 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:25.582671 UTC] Performing policy update
[2018-07-02 16:09:25.583501 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:25.593772 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:25.666456 UTC] Performing line search
[2018-07-02 16:09:25.676371 UTC] Updating baseline
[2018-07-02 16:09:25.762064 UTC] Computing logging information
-------------------------------------
| Iteration            | 50         |
| ExpectedImprovement  | 0.010667   |
| ActualImprovement    | 0.0081847  |
| ImprovementRatio     | 0.76727    |
| MeanKL               | 0.0070393  |
| Entropy              | 0.50319    |
| Perplexity           | 1.654      |
| AveragePolicyProb[0] | 0.4862     |
| AveragePolicyProb[1] | 0.5138     |
| AverageReturn        | 199.09     |
| MinReturn            | 157        |
| MaxReturn            | 200        |
| StdReturn            | 5.6269     |
| AverageEpisodeLength | 199.09     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 5.6269     |
| TotalNEpisodes       | 737        |
| TotalNSamples        | 1.0026e+05 |
| ExplainedVariance    | 0.043017   |
-------------------------------------
[2018-07-02 16:09:26.555231 UTC] Saving snapshot
[2018-07-02 16:09:26.564409 UTC] Starting iteration 51
[2018-07-02 16:09:26.565092 UTC] Start collecting samples
[2018-07-02 16:09:26.822604 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:26.838962 UTC] Performing policy update
[2018-07-02 16:09:26.839798 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:26.848578 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:26.923882 UTC] Performing line search
[2018-07-02 16:09:26.930763 UTC] Updating baseline
[2018-07-02 16:09:27.014052 UTC] Computing logging information
-------------------------------------
| Iteration            | 51         |
| ExpectedImprovement  | 0.011789   |
| ActualImprovement    | 0.0077775  |
| ImprovementRatio     | 0.65971    |
| MeanKL               | 0.0067376  |
| Entropy              | 0.50531    |
| Perplexity           | 1.6575     |
| AveragePolicyProb[0] | 0.49683    |
| AveragePolicyProb[1] | 0.50317    |
| AverageReturn        | 198.68     |
| MinReturn            | 157        |
| MaxReturn            | 200        |
| StdReturn            | 6.8962     |
| AverageEpisodeLength | 198.68     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 6.8962     |
| TotalNEpisodes       | 748        |
| TotalNSamples        | 1.0241e+05 |
| ExplainedVariance    | 0.45217    |
-------------------------------------
[2018-07-02 16:09:27.769518 UTC] Saving snapshot
[2018-07-02 16:09:27.778541 UTC] Starting iteration 52
[2018-07-02 16:09:27.779162 UTC] Start collecting samples
[2018-07-02 16:09:28.038376 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:28.062724 UTC] Performing policy update
[2018-07-02 16:09:28.064161 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:28.075140 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:28.168174 UTC] Performing line search
[2018-07-02 16:09:28.174953 UTC] Updating baseline
[2018-07-02 16:09:28.258373 UTC] Computing logging information
-------------------------------------
| Iteration            | 52         |
| ExpectedImprovement  | 0.0157     |
| ActualImprovement    | 0.010429   |
| ImprovementRatio     | 0.66424    |
| MeanKL               | 0.0082846  |
| Entropy              | 0.50822    |
| Perplexity           | 1.6623     |
| AveragePolicyProb[0] | 0.50422    |
| AveragePolicyProb[1] | 0.49578    |
| AverageReturn        | 198.47     |
| MinReturn            | 157        |
| MaxReturn            | 200        |
| StdReturn            | 7.1672     |
| AverageEpisodeLength | 198.47     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 7.1672     |
| TotalNEpisodes       | 757        |
| TotalNSamples        | 1.0419e+05 |
| ExplainedVariance    | 0.32092    |
-------------------------------------
[2018-07-02 16:09:29.043610 UTC] Saving snapshot
[2018-07-02 16:09:29.053552 UTC] Starting iteration 53
[2018-07-02 16:09:29.055004 UTC] Start collecting samples
[2018-07-02 16:09:29.307404 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:29.324654 UTC] Performing policy update
[2018-07-02 16:09:29.325440 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:29.334269 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:29.407336 UTC] Performing line search
[2018-07-02 16:09:29.417552 UTC] Updating baseline
[2018-07-02 16:09:29.505118 UTC] Computing logging information
-------------------------------------
| Iteration            | 53         |
| ExpectedImprovement  | 0.0095706  |
| ActualImprovement    | 0.0083826  |
| ImprovementRatio     | 0.87587    |
| MeanKL               | 0.0061829  |
| Entropy              | 0.52727    |
| Perplexity           | 1.6943     |
| AveragePolicyProb[0] | 0.51085    |
| AveragePolicyProb[1] | 0.48915    |
| AverageReturn        | 198.24     |
| MinReturn            | 157        |
| MaxReturn            | 200        |
| StdReturn            | 7.3008     |
| AverageEpisodeLength | 198.24     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 7.3008     |
| TotalNEpisodes       | 768        |
| TotalNSamples        | 1.0637e+05 |
| ExplainedVariance    | 0.41933    |
-------------------------------------
[2018-07-02 16:09:30.264983 UTC] Saving snapshot
[2018-07-02 16:09:30.273727 UTC] Starting iteration 54
[2018-07-02 16:09:30.274413 UTC] Start collecting samples
[2018-07-02 16:09:30.538164 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:30.556737 UTC] Performing policy update
[2018-07-02 16:09:30.557684 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:30.566565 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:30.639682 UTC] Performing line search
[2018-07-02 16:09:30.648082 UTC] Updating baseline
[2018-07-02 16:09:30.733528 UTC] Computing logging information
-------------------------------------
| Iteration            | 54         |
| ExpectedImprovement  | 0.014605   |
| ActualImprovement    | 0.0047946  |
| ImprovementRatio     | 0.32828    |
| MeanKL               | 0.0091036  |
| Entropy              | 0.54621    |
| Perplexity           | 1.7267     |
| AveragePolicyProb[0] | 0.48267    |
| AveragePolicyProb[1] | 0.51733    |
| AverageReturn        | 198.24     |
| MinReturn            | 157        |
| MaxReturn            | 200        |
| StdReturn            | 7.3008     |
| AverageEpisodeLength | 198.24     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 7.3008     |
| TotalNEpisodes       | 779        |
| TotalNSamples        | 1.0857e+05 |
| ExplainedVariance    | 0.42863    |
-------------------------------------
[2018-07-02 16:09:31.503244 UTC] Saving snapshot
[2018-07-02 16:09:31.513172 UTC] Starting iteration 55
[2018-07-02 16:09:31.513960 UTC] Start collecting samples
[2018-07-02 16:09:31.761328 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:31.776668 UTC] Performing policy update
[2018-07-02 16:09:31.777600 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:31.786300 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:31.856377 UTC] Performing line search
[2018-07-02 16:09:31.862681 UTC] Updating baseline
[2018-07-02 16:09:31.941114 UTC] Computing logging information
-------------------------------------
| Iteration            | 55         |
| ExpectedImprovement  | 0.017382   |
| ActualImprovement    | 0.0098445  |
| ImprovementRatio     | 0.56636    |
| MeanKL               | 0.0093512  |
| Entropy              | 0.50636    |
| Perplexity           | 1.6592     |
| AveragePolicyProb[0] | 0.48744    |
| AveragePolicyProb[1] | 0.51256    |
| AverageReturn        | 198.15     |
| MinReturn            | 157        |
| MaxReturn            | 200        |
| StdReturn            | 7.3585     |
| AverageEpisodeLength | 198.15     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 7.3585     |
| TotalNEpisodes       | 788        |
| TotalNSamples        | 1.1036e+05 |
| ExplainedVariance    | 0.54589    |
-------------------------------------
[2018-07-02 16:09:32.709677 UTC] Saving snapshot
[2018-07-02 16:09:32.720894 UTC] Starting iteration 56
[2018-07-02 16:09:32.721474 UTC] Start collecting samples
[2018-07-02 16:09:33.028154 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:33.045903 UTC] Performing policy update
[2018-07-02 16:09:33.046943 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:33.062878 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:33.139753 UTC] Performing line search
[2018-07-02 16:09:33.150508 UTC] Updating baseline
[2018-07-02 16:09:33.245695 UTC] Computing logging information
-------------------------------------
| Iteration            | 56         |
| ExpectedImprovement  | 0.01053    |
| ActualImprovement    | 0.0097998  |
| ImprovementRatio     | 0.93068    |
| MeanKL               | 0.0068095  |
| Entropy              | 0.51163    |
| Perplexity           | 1.668      |
| AveragePolicyProb[0] | 0.50822    |
| AveragePolicyProb[1] | 0.49178    |
| AverageReturn        | 197.93     |
| MinReturn            | 157        |
| MaxReturn            | 200        |
| StdReturn            | 7.624      |
| AverageEpisodeLength | 197.93     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 7.624      |
| TotalNEpisodes       | 798        |
| TotalNSamples        | 1.1234e+05 |
| ExplainedVariance    | 0.3395     |
-------------------------------------
[2018-07-02 16:09:34.133200 UTC] Saving snapshot
[2018-07-02 16:09:34.141960 UTC] Starting iteration 57
[2018-07-02 16:09:34.142524 UTC] Start collecting samples
[2018-07-02 16:09:34.441010 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:34.464505 UTC] Performing policy update
[2018-07-02 16:09:34.465669 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:34.476442 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:34.547487 UTC] Performing line search
[2018-07-02 16:09:34.554320 UTC] Updating baseline
[2018-07-02 16:09:34.633373 UTC] Computing logging information
-------------------------------------
| Iteration            | 57         |
| ExpectedImprovement  | 0.015656   |
| ActualImprovement    | 0.0082506  |
| ImprovementRatio     | 0.52698    |
| MeanKL               | 0.0083621  |
| Entropy              | 0.50029    |
| Perplexity           | 1.6492     |
| AveragePolicyProb[0] | 0.49134    |
| AveragePolicyProb[1] | 0.50866    |
| AverageReturn        | 197.93     |
| MinReturn            | 157        |
| MaxReturn            | 200        |
| StdReturn            | 7.624      |
| AverageEpisodeLength | 197.93     |
| MinEpisodeLength     | 157        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 7.624      |
| TotalNEpisodes       | 810        |
| TotalNSamples        | 1.1474e+05 |
| ExplainedVariance    | 0.54011    |
-------------------------------------
[2018-07-02 16:09:35.411895 UTC] Saving snapshot
[2018-07-02 16:09:35.420723 UTC] Starting iteration 58
[2018-07-02 16:09:35.421425 UTC] Start collecting samples
[2018-07-02 16:09:35.661058 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:35.677845 UTC] Performing policy update
[2018-07-02 16:09:35.679038 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:35.687657 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:35.758507 UTC] Performing line search
[2018-07-02 16:09:35.764870 UTC] Updating baseline
[2018-07-02 16:09:35.843244 UTC] Computing logging information
-------------------------------------
| Iteration            | 58         |
| ExpectedImprovement  | 0.012241   |
| ActualImprovement    | 0.0083569  |
| ImprovementRatio     | 0.6827     |
| MeanKL               | 0.0065275  |
| Entropy              | 0.45274    |
| Perplexity           | 1.5726     |
| AveragePolicyProb[0] | 0.47968    |
| AveragePolicyProb[1] | 0.52032    |
| AverageReturn        | 198.36     |
| MinReturn            | 159        |
| MaxReturn            | 200        |
| StdReturn            | 6.4211     |
| AverageEpisodeLength | 198.36     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 6.4211     |
| TotalNEpisodes       | 817        |
| TotalNSamples        | 1.1614e+05 |
| ExplainedVariance    | 0.51909    |
-------------------------------------
[2018-07-02 16:09:36.657089 UTC] Saving snapshot
[2018-07-02 16:09:36.666050 UTC] Starting iteration 59
[2018-07-02 16:09:36.667458 UTC] Start collecting samples
[2018-07-02 16:09:37.038541 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:37.055267 UTC] Performing policy update
[2018-07-02 16:09:37.056155 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:37.064596 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:37.133473 UTC] Performing line search
[2018-07-02 16:09:37.139710 UTC] Updating baseline
[2018-07-02 16:09:37.219747 UTC] Computing logging information
-------------------------------------
| Iteration            | 59         |
| ExpectedImprovement  | 0.019737   |
| ActualImprovement    | 0.0088875  |
| ImprovementRatio     | 0.45031    |
| MeanKL               | 0.0056508  |
| Entropy              | 0.45303    |
| Perplexity           | 1.5731     |
| AveragePolicyProb[0] | 0.48737    |
| AveragePolicyProb[1] | 0.51263    |
| AverageReturn        | 198.28     |
| MinReturn            | 159        |
| MaxReturn            | 200        |
| StdReturn            | 5.9196     |
| AverageEpisodeLength | 198.28     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 5.9196     |
| TotalNEpisodes       | 828        |
| TotalNSamples        | 1.1828e+05 |
| ExplainedVariance    | 0.86595    |
-------------------------------------
[2018-07-02 16:09:38.351460 UTC] Saving snapshot
[2018-07-02 16:09:38.360243 UTC] Starting iteration 60
[2018-07-02 16:09:38.360842 UTC] Start collecting samples
[2018-07-02 16:09:38.686461 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:38.702460 UTC] Performing policy update
[2018-07-02 16:09:38.703324 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:38.710826 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:38.779359 UTC] Performing line search
[2018-07-02 16:09:38.784235 UTC] Updating baseline
[2018-07-02 16:09:38.864619 UTC] Computing logging information
-------------------------------------
| Iteration            | 60         |
| ExpectedImprovement  | 0.014348   |
| ActualImprovement    | 0.007259   |
| ImprovementRatio     | 0.50591    |
| MeanKL               | 0.0063199  |
| Entropy              | 0.42783    |
| Perplexity           | 1.5339     |
| AveragePolicyProb[0] | 0.5012     |
| AveragePolicyProb[1] | 0.4988     |
| AverageReturn        | 198.28     |
| MinReturn            | 159        |
| MaxReturn            | 200        |
| StdReturn            | 5.9196     |
| AverageEpisodeLength | 198.28     |
| MinEpisodeLength     | 159        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 5.9196     |
| TotalNEpisodes       | 839        |
| TotalNSamples        | 1.2048e+05 |
| ExplainedVariance    | 0.50098    |
-------------------------------------
[2018-07-02 16:09:40.165567 UTC] Saving snapshot
[2018-07-02 16:09:40.174529 UTC] Starting iteration 61
[2018-07-02 16:09:40.175189 UTC] Start collecting samples
[2018-07-02 16:09:40.469816 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:40.485380 UTC] Performing policy update
[2018-07-02 16:09:40.486384 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:40.494721 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:40.564670 UTC] Performing line search
[2018-07-02 16:09:40.569600 UTC] Updating baseline
[2018-07-02 16:09:40.644831 UTC] Computing logging information
-------------------------------------
| Iteration            | 61         |
| ExpectedImprovement  | 0.01436    |
| ActualImprovement    | 0.0098364  |
| ImprovementRatio     | 0.68499    |
| MeanKL               | 0.0078923  |
| Entropy              | 0.46569    |
| Perplexity           | 1.5931     |
| AveragePolicyProb[0] | 0.48937    |
| AveragePolicyProb[1] | 0.51063    |
| AverageReturn        | 198.69     |
| MinReturn            | 178        |
| MaxReturn            | 200        |
| StdReturn            | 4.4129     |
| AverageEpisodeLength | 198.69     |
| MinEpisodeLength     | 178        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 4.4129     |
| TotalNEpisodes       | 848        |
| TotalNSamples        | 1.2228e+05 |
| ExplainedVariance    | 0.60087    |
-------------------------------------
[2018-07-02 16:09:41.725755 UTC] Saving snapshot
[2018-07-02 16:09:41.737885 UTC] Starting iteration 62
[2018-07-02 16:09:41.738617 UTC] Start collecting samples
[2018-07-02 16:09:41.996607 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:42.011803 UTC] Performing policy update
[2018-07-02 16:09:42.012634 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:42.020285 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:42.090228 UTC] Performing line search
[2018-07-02 16:09:42.095073 UTC] Updating baseline
[2018-07-02 16:09:42.170479 UTC] Computing logging information
-------------------------------------
| Iteration            | 62         |
| ExpectedImprovement  | 0.018586   |
| ActualImprovement    | 0.019128   |
| ImprovementRatio     | 1.0291     |
| MeanKL               | 0.0099871  |
| Entropy              | 0.47477    |
| Perplexity           | 1.6076     |
| AveragePolicyProb[0] | 0.49409    |
| AveragePolicyProb[1] | 0.50591    |
| AverageReturn        | 199.03     |
| MinReturn            | 178        |
| MaxReturn            | 200        |
| StdReturn            | 3.7615     |
| AverageEpisodeLength | 199.03     |
| MinEpisodeLength     | 178        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 3.7615     |
| TotalNEpisodes       | 859        |
| TotalNSamples        | 1.2448e+05 |
| ExplainedVariance    | 0.90896    |
-------------------------------------
[2018-07-02 16:09:42.919429 UTC] Saving snapshot
[2018-07-02 16:09:42.928772 UTC] Starting iteration 63
[2018-07-02 16:09:42.929422 UTC] Start collecting samples
[2018-07-02 16:09:43.179588 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:43.194395 UTC] Performing policy update
[2018-07-02 16:09:43.195392 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:43.202971 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:43.270588 UTC] Performing line search
[2018-07-02 16:09:43.286141 UTC] Updating baseline
[2018-07-02 16:09:43.369790 UTC] Computing logging information
-------------------------------------
| Iteration            | 63         |
| ExpectedImprovement  | 0.014819   |
| ActualImprovement    | 0.0026297  |
| ImprovementRatio     | 0.17745    |
| MeanKL               | 0.0027075  |
| Entropy              | 0.48884    |
| Perplexity           | 1.6304     |
| AveragePolicyProb[0] | 0.48726    |
| AveragePolicyProb[1] | 0.51275    |
| AverageReturn        | 197.55     |
| MinReturn            | 52         |
| MaxReturn            | 200        |
| StdReturn            | 15.104     |
| AverageEpisodeLength | 197.55     |
| MinEpisodeLength     | 52         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.104     |
| TotalNEpisodes       | 869        |
| TotalNSamples        | 1.2632e+05 |
| ExplainedVariance    | 0.44232    |
-------------------------------------
[2018-07-02 16:09:44.163171 UTC] Saving snapshot
[2018-07-02 16:09:44.171248 UTC] Starting iteration 64
[2018-07-02 16:09:44.171816 UTC] Start collecting samples
[2018-07-02 16:09:44.437612 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:44.459600 UTC] Performing policy update
[2018-07-02 16:09:44.460658 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:44.468092 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:44.541582 UTC] Performing line search
[2018-07-02 16:09:44.548012 UTC] Updating baseline
[2018-07-02 16:09:44.629556 UTC] Computing logging information
-------------------------------------
| Iteration            | 64         |
| ExpectedImprovement  | 0.019024   |
| ActualImprovement    | 0.011818   |
| ImprovementRatio     | 0.62123    |
| MeanKL               | 0.0065241  |
| Entropy              | 0.51819    |
| Perplexity           | 1.679      |
| AveragePolicyProb[0] | 0.52186    |
| AveragePolicyProb[1] | 0.47814    |
| AverageReturn        | 197.55     |
| MinReturn            | 52         |
| MaxReturn            | 200        |
| StdReturn            | 15.104     |
| AverageEpisodeLength | 197.55     |
| MinEpisodeLength     | 52         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.104     |
| TotalNEpisodes       | 878        |
| TotalNSamples        | 1.2812e+05 |
| ExplainedVariance    | 0.47782    |
-------------------------------------
[2018-07-02 16:09:45.415651 UTC] Saving snapshot
[2018-07-02 16:09:45.424438 UTC] Starting iteration 65
[2018-07-02 16:09:45.425035 UTC] Start collecting samples
[2018-07-02 16:09:45.738514 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:45.757477 UTC] Performing policy update
[2018-07-02 16:09:45.758321 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:45.765850 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:45.835214 UTC] Performing line search
[2018-07-02 16:09:45.844022 UTC] Updating baseline
[2018-07-02 16:09:45.932584 UTC] Computing logging information
-------------------------------------
| Iteration            | 65         |
| ExpectedImprovement  | 0.010199   |
| ActualImprovement    | 0.0050654  |
| ImprovementRatio     | 0.49666    |
| MeanKL               | 0.0087058  |
| Entropy              | 0.52841    |
| Perplexity           | 1.6962     |
| AveragePolicyProb[0] | 0.50992    |
| AveragePolicyProb[1] | 0.49008    |
| AverageReturn        | 197.88     |
| MinReturn            | 52         |
| MaxReturn            | 200        |
| StdReturn            | 14.953     |
| AverageEpisodeLength | 197.88     |
| MinEpisodeLength     | 52         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 14.953     |
| TotalNEpisodes       | 892        |
| TotalNSamples        | 1.3092e+05 |
| ExplainedVariance    | 0.25282    |
-------------------------------------
[2018-07-02 16:09:47.145545 UTC] Saving snapshot
[2018-07-02 16:09:47.154445 UTC] Starting iteration 66
[2018-07-02 16:09:47.155212 UTC] Start collecting samples
[2018-07-02 16:09:47.435087 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:47.458457 UTC] Performing policy update
[2018-07-02 16:09:47.459898 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:47.471049 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:47.547905 UTC] Performing line search
[2018-07-02 16:09:47.555256 UTC] Updating baseline
[2018-07-02 16:09:47.635938 UTC] Computing logging information
------------------------------------
| Iteration            | 66        |
| ExpectedImprovement  | 0.015208  |
| ActualImprovement    | 0.012309  |
| ImprovementRatio     | 0.80935   |
| MeanKL               | 0.0092879 |
| Entropy              | 0.51134   |
| Perplexity           | 1.6675    |
| AveragePolicyProb[0] | 0.49648   |
| AveragePolicyProb[1] | 0.50352   |
| AverageReturn        | 196.63    |
| MinReturn            | 52        |
| MaxReturn            | 200       |
| StdReturn            | 19.313    |
| AverageEpisodeLength | 196.63    |
| MinEpisodeLength     | 52        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 19.313    |
| TotalNEpisodes       | 899       |
| TotalNSamples        | 1.322e+05 |
| ExplainedVariance    | 0.33333   |
------------------------------------
[2018-07-02 16:09:48.467581 UTC] Saving snapshot
[2018-07-02 16:09:48.476414 UTC] Starting iteration 67
[2018-07-02 16:09:48.478163 UTC] Start collecting samples
[2018-07-02 16:09:48.783224 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:48.801908 UTC] Performing policy update
[2018-07-02 16:09:48.802844 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:48.812217 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:48.879049 UTC] Performing line search
[2018-07-02 16:09:48.885295 UTC] Updating baseline
[2018-07-02 16:09:48.973884 UTC] Computing logging information
-------------------------------------
| Iteration            | 67         |
| ExpectedImprovement  | 0.010707   |
| ActualImprovement    | 0.005728   |
| ImprovementRatio     | 0.535      |
| MeanKL               | 0.0054648  |
| Entropy              | 0.51455    |
| Perplexity           | 1.6729     |
| AveragePolicyProb[0] | 0.47786    |
| AveragePolicyProb[1] | 0.52214    |
| AverageReturn        | 196.4      |
| MinReturn            | 52         |
| MaxReturn            | 200        |
| StdReturn            | 19.408     |
| AverageEpisodeLength | 196.4      |
| MinEpisodeLength     | 52         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 19.408     |
| TotalNEpisodes       | 909        |
| TotalNSamples        | 1.3418e+05 |
| ExplainedVariance    | 0.71602    |
-------------------------------------
[2018-07-02 16:09:49.813429 UTC] Saving snapshot
[2018-07-02 16:09:49.823877 UTC] Starting iteration 68
[2018-07-02 16:09:49.824425 UTC] Start collecting samples
[2018-07-02 16:09:50.141081 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:50.159527 UTC] Performing policy update
[2018-07-02 16:09:50.160363 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:50.166775 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:50.236028 UTC] Performing line search
[2018-07-02 16:09:50.245983 UTC] Updating baseline
[2018-07-02 16:09:50.333509 UTC] Computing logging information
-------------------------------------
| Iteration            | 68         |
| ExpectedImprovement  | 0.015043   |
| ActualImprovement    | 0.0096425  |
| ImprovementRatio     | 0.64098    |
| MeanKL               | 0.0098078  |
| Entropy              | 0.52202    |
| Perplexity           | 1.6854     |
| AveragePolicyProb[0] | 0.5016     |
| AveragePolicyProb[1] | 0.4984     |
| AverageReturn        | 196.4      |
| MinReturn            | 52         |
| MaxReturn            | 200        |
| StdReturn            | 19.408     |
| AverageEpisodeLength | 196.4      |
| MinEpisodeLength     | 52         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 19.408     |
| TotalNEpisodes       | 920        |
| TotalNSamples        | 1.3638e+05 |
| ExplainedVariance    | 0.36192    |
-------------------------------------
[2018-07-02 16:09:51.064804 UTC] Saving snapshot
[2018-07-02 16:09:51.072512 UTC] Starting iteration 69
[2018-07-02 16:09:51.073331 UTC] Start collecting samples
[2018-07-02 16:09:51.373876 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:51.390841 UTC] Performing policy update
[2018-07-02 16:09:51.391819 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:51.399268 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:51.468728 UTC] Performing line search
[2018-07-02 16:09:51.473380 UTC] Updating baseline
[2018-07-02 16:09:51.581116 UTC] Computing logging information
-------------------------------------
| Iteration            | 69         |
| ExpectedImprovement  | 0.0088807  |
| ActualImprovement    | 0.0082894  |
| ImprovementRatio     | 0.93341    |
| MeanKL               | 0.0090464  |
| Entropy              | 0.51377    |
| Perplexity           | 1.6716     |
| AveragePolicyProb[0] | 0.49992    |
| AveragePolicyProb[1] | 0.50008    |
| AverageReturn        | 196.94     |
| MinReturn            | 52         |
| MaxReturn            | 200        |
| StdReturn            | 19.293     |
| AverageEpisodeLength | 196.94     |
| MinEpisodeLength     | 52         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 19.293     |
| TotalNEpisodes       | 930        |
| TotalNSamples        | 1.3838e+05 |
| ExplainedVariance    | 0.29477    |
-------------------------------------
[2018-07-02 16:09:52.720186 UTC] Saving snapshot
[2018-07-02 16:09:52.729048 UTC] Starting iteration 70
[2018-07-02 16:09:52.729860 UTC] Start collecting samples
[2018-07-02 16:09:53.049451 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:53.066972 UTC] Performing policy update
[2018-07-02 16:09:53.067880 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:53.075050 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:53.142022 UTC] Performing line search
[2018-07-02 16:09:53.146714 UTC] Updating baseline
[2018-07-02 16:09:53.222640 UTC] Computing logging information
-------------------------------------
| Iteration            | 70         |
| ExpectedImprovement  | 0.011856   |
| ActualImprovement    | 0.0082272  |
| ImprovementRatio     | 0.69392    |
| MeanKL               | 0.0099541  |
| Entropy              | 0.50725    |
| Perplexity           | 1.6607     |
| AveragePolicyProb[0] | 0.50144    |
| AveragePolicyProb[1] | 0.49856    |
| AverageReturn        | 196.94     |
| MinReturn            | 52         |
| MaxReturn            | 200        |
| StdReturn            | 19.293     |
| AverageEpisodeLength | 196.94     |
| MinEpisodeLength     | 52         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 19.293     |
| TotalNEpisodes       | 941        |
| TotalNSamples        | 1.4058e+05 |
| ExplainedVariance    | 0.1663     |
-------------------------------------
[2018-07-02 16:09:54.253480 UTC] Saving snapshot
[2018-07-02 16:09:54.266179 UTC] Starting iteration 71
[2018-07-02 16:09:54.267129 UTC] Start collecting samples
[2018-07-02 16:09:54.546524 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:54.562541 UTC] Performing policy update
[2018-07-02 16:09:54.563711 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:54.571519 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:54.641651 UTC] Performing line search
[2018-07-02 16:09:54.646328 UTC] Updating baseline
[2018-07-02 16:09:54.736711 UTC] Computing logging information
-------------------------------------
| Iteration            | 71         |
| ExpectedImprovement  | 0.013519   |
| ActualImprovement    | 0.0087226  |
| ImprovementRatio     | 0.64522    |
| MeanKL               | 0.0055599  |
| Entropy              | 0.48593    |
| Perplexity           | 1.6257     |
| AveragePolicyProb[0] | 0.51119    |
| AveragePolicyProb[1] | 0.48881    |
| AverageReturn        | 196.94     |
| MinReturn            | 52         |
| MaxReturn            | 200        |
| StdReturn            | 19.293     |
| AverageEpisodeLength | 196.94     |
| MinEpisodeLength     | 52         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 19.293     |
| TotalNEpisodes       | 949        |
| TotalNSamples        | 1.4218e+05 |
| ExplainedVariance    | 0.28965    |
-------------------------------------
[2018-07-02 16:09:55.745826 UTC] Saving snapshot
[2018-07-02 16:09:55.757153 UTC] Starting iteration 72
[2018-07-02 16:09:55.758270 UTC] Start collecting samples
[2018-07-02 16:09:56.066266 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:56.111999 UTC] Performing policy update
[2018-07-02 16:09:56.113952 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:56.130536 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:56.234912 UTC] Performing line search
[2018-07-02 16:09:56.242587 UTC] Updating baseline
[2018-07-02 16:09:56.318927 UTC] Computing logging information
-------------------------------------
| Iteration            | 72         |
| ExpectedImprovement  | 0.015628   |
| ActualImprovement    | 0.0081745  |
| ImprovementRatio     | 0.52308    |
| MeanKL               | 0.0070974  |
| Entropy              | 0.49352    |
| Perplexity           | 1.6381     |
| AveragePolicyProb[0] | 0.50644    |
| AveragePolicyProb[1] | 0.49356    |
| AverageReturn        | 196.94     |
| MinReturn            | 52         |
| MaxReturn            | 200        |
| StdReturn            | 19.293     |
| AverageEpisodeLength | 196.94     |
| MinEpisodeLength     | 52         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 19.293     |
| TotalNEpisodes       | 959        |
| TotalNSamples        | 1.4418e+05 |
| ExplainedVariance    | -0.060914  |
-------------------------------------
[2018-07-02 16:09:57.212110 UTC] Saving snapshot
[2018-07-02 16:09:57.219996 UTC] Starting iteration 73
[2018-07-02 16:09:57.220964 UTC] Start collecting samples
[2018-07-02 16:09:57.580287 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:57.596280 UTC] Performing policy update
[2018-07-02 16:09:57.597145 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:57.605517 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:57.670713 UTC] Performing line search
[2018-07-02 16:09:57.677132 UTC] Updating baseline
[2018-07-02 16:09:57.752156 UTC] Computing logging information
-------------------------------------
| Iteration            | 73         |
| ExpectedImprovement  | 0.014019   |
| ActualImprovement    | 0.010191   |
| ImprovementRatio     | 0.72694    |
| MeanKL               | 0.0089698  |
| Entropy              | 0.48643    |
| Perplexity           | 1.6265     |
| AveragePolicyProb[0] | 0.49781    |
| AveragePolicyProb[1] | 0.50219    |
| AverageReturn        | 198.52     |
| MinReturn            | 75         |
| MaxReturn            | 200        |
| StdReturn            | 12.623     |
| AverageEpisodeLength | 198.52     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 12.623     |
| TotalNEpisodes       | 973        |
| TotalNSamples        | 1.4698e+05 |
| ExplainedVariance    | 0.21823    |
-------------------------------------
[2018-07-02 16:09:58.417283 UTC] Saving snapshot
[2018-07-02 16:09:58.424917 UTC] Starting iteration 74
[2018-07-02 16:09:58.425745 UTC] Start collecting samples
[2018-07-02 16:09:58.666813 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:58.680831 UTC] Performing policy update
[2018-07-02 16:09:58.681637 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:58.687964 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:58.752996 UTC] Performing line search
[2018-07-02 16:09:58.759045 UTC] Updating baseline
[2018-07-02 16:09:58.828846 UTC] Computing logging information
-------------------------------------
| Iteration            | 74         |
| ExpectedImprovement  | 0.016028   |
| ActualImprovement    | 0.010767   |
| ImprovementRatio     | 0.67177    |
| MeanKL               | 0.0086626  |
| Entropy              | 0.48955    |
| Perplexity           | 1.6316     |
| AveragePolicyProb[0] | 0.4843     |
| AveragePolicyProb[1] | 0.5157     |
| AverageReturn        | 198.52     |
| MinReturn            | 75         |
| MaxReturn            | 200        |
| StdReturn            | 12.623     |
| AverageEpisodeLength | 198.52     |
| MinEpisodeLength     | 75         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 12.623     |
| TotalNEpisodes       | 979        |
| TotalNSamples        | 1.4818e+05 |
| ExplainedVariance    | 0.37087    |
-------------------------------------
[2018-07-02 16:09:59.511585 UTC] Saving snapshot
[2018-07-02 16:09:59.518791 UTC] Starting iteration 75
[2018-07-02 16:09:59.519527 UTC] Start collecting samples
[2018-07-02 16:09:59.806077 UTC] Computing input variables for policy optimization
[2018-07-02 16:09:59.821918 UTC] Performing policy update
[2018-07-02 16:09:59.822658 UTC] Computing gradient in Euclidean space
[2018-07-02 16:09:59.828924 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:09:59.892978 UTC] Performing line search
[2018-07-02 16:09:59.902356 UTC] Updating baseline
[2018-07-02 16:09:59.985202 UTC] Computing logging information
-------------------------------------
| Iteration            | 75         |
| ExpectedImprovement  | 0.021068   |
| ActualImprovement    | 0.011841   |
| ImprovementRatio     | 0.56202    |
| MeanKL               | 0.0070499  |
| Entropy              | 0.47683    |
| Perplexity           | 1.611      |
| AveragePolicyProb[0] | 0.50174    |
| AveragePolicyProb[1] | 0.49826    |
| AverageReturn        | 197.01     |
| MinReturn            | 49         |
| MaxReturn            | 200        |
| StdReturn            | 19.509     |
| AverageEpisodeLength | 197.01     |
| MinEpisodeLength     | 49         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 19.509     |
| TotalNEpisodes       | 990        |
| TotalNSamples        | 1.5023e+05 |
| ExplainedVariance    | 0.57827    |
-------------------------------------
[2018-07-02 16:10:00.805002 UTC] Saving snapshot
[2018-07-02 16:10:00.815791 UTC] Starting iteration 76
[2018-07-02 16:10:00.816546 UTC] Start collecting samples
[2018-07-02 16:10:01.147745 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:01.164446 UTC] Performing policy update
[2018-07-02 16:10:01.165718 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:01.172322 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:01.241744 UTC] Performing line search
[2018-07-02 16:10:01.255660 UTC] Updating baseline
[2018-07-02 16:10:01.333297 UTC] Computing logging information
-------------------------------------
| Iteration            | 76         |
| ExpectedImprovement  | 0.0082915  |
| ActualImprovement    | 0.0067879  |
| ImprovementRatio     | 0.81865    |
| MeanKL               | 0.0070523  |
| Entropy              | 0.48685    |
| Perplexity           | 1.6272     |
| AveragePolicyProb[0] | 0.50589    |
| AveragePolicyProb[1] | 0.49411    |
| AverageReturn        | 198.26     |
| MinReturn            | 49         |
| MaxReturn            | 200        |
| StdReturn            | 15.175     |
| AverageEpisodeLength | 198.26     |
| MinEpisodeLength     | 49         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.175     |
| TotalNEpisodes       | 1001       |
| TotalNSamples        | 1.5243e+05 |
| ExplainedVariance    | 0.32849    |
-------------------------------------
[2018-07-02 16:10:02.360196 UTC] Saving snapshot
[2018-07-02 16:10:02.368708 UTC] Starting iteration 77
[2018-07-02 16:10:02.370236 UTC] Start collecting samples
[2018-07-02 16:10:02.657111 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:02.673663 UTC] Performing policy update
[2018-07-02 16:10:02.674679 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:02.681632 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:02.749419 UTC] Performing line search
[2018-07-02 16:10:02.755651 UTC] Updating baseline
[2018-07-02 16:10:02.834326 UTC] Computing logging information
-------------------------------------
| Iteration            | 77         |
| ExpectedImprovement  | 0.018493   |
| ActualImprovement    | 0.012636   |
| ImprovementRatio     | 0.68327    |
| MeanKL               | 0.0064393  |
| Entropy              | 0.4975     |
| Perplexity           | 1.6446     |
| AveragePolicyProb[0] | 0.50709    |
| AveragePolicyProb[1] | 0.49291    |
| AverageReturn        | 198.49     |
| MinReturn            | 49         |
| MaxReturn            | 200        |
| StdReturn            | 15.024     |
| AverageEpisodeLength | 198.49     |
| MinEpisodeLength     | 49         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.024     |
| TotalNEpisodes       | 1010       |
| TotalNSamples        | 1.5423e+05 |
| ExplainedVariance    | 0.60358    |
-------------------------------------
[2018-07-02 16:10:03.796629 UTC] Saving snapshot
[2018-07-02 16:10:03.810432 UTC] Starting iteration 78
[2018-07-02 16:10:03.811318 UTC] Start collecting samples
[2018-07-02 16:10:04.106964 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:04.124002 UTC] Performing policy update
[2018-07-02 16:10:04.125165 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:04.133766 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:04.202054 UTC] Performing line search
[2018-07-02 16:10:04.208402 UTC] Updating baseline
[2018-07-02 16:10:04.284443 UTC] Computing logging information
-------------------------------------
| Iteration            | 78         |
| ExpectedImprovement  | 0.014211   |
| ActualImprovement    | 0.0086817  |
| ImprovementRatio     | 0.61091    |
| MeanKL               | 0.0078888  |
| Entropy              | 0.48242    |
| Perplexity           | 1.62       |
| AveragePolicyProb[0] | 0.48606    |
| AveragePolicyProb[1] | 0.51394    |
| AverageReturn        | 198.49     |
| MinReturn            | 49         |
| MaxReturn            | 200        |
| StdReturn            | 15.024     |
| AverageEpisodeLength | 198.49     |
| MinEpisodeLength     | 49         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.024     |
| TotalNEpisodes       | 1022       |
| TotalNSamples        | 1.5663e+05 |
| ExplainedVariance    | 0.21511    |
-------------------------------------
[2018-07-02 16:10:05.035486 UTC] Saving snapshot
[2018-07-02 16:10:05.044796 UTC] Starting iteration 79
[2018-07-02 16:10:05.045513 UTC] Start collecting samples
[2018-07-02 16:10:05.263511 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:05.278803 UTC] Performing policy update
[2018-07-02 16:10:05.279904 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:05.286450 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:05.355798 UTC] Performing line search
[2018-07-02 16:10:05.362430 UTC] Updating baseline
[2018-07-02 16:10:05.437360 UTC] Computing logging information
-------------------------------------
| Iteration            | 79         |
| ExpectedImprovement  | 0.014431   |
| ActualImprovement    | 0.0093164  |
| ImprovementRatio     | 0.64558    |
| MeanKL               | 0.0086699  |
| Entropy              | 0.50241    |
| Perplexity           | 1.6527     |
| AveragePolicyProb[0] | 0.51188    |
| AveragePolicyProb[1] | 0.48812    |
| AverageReturn        | 198.08     |
| MinReturn            | 49         |
| MaxReturn            | 200        |
| StdReturn            | 15.303     |
| AverageEpisodeLength | 198.08     |
| MinEpisodeLength     | 49         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.303     |
| TotalNEpisodes       | 1030       |
| TotalNSamples        | 1.5818e+05 |
| ExplainedVariance    | 0.27908    |
-------------------------------------
[2018-07-02 16:10:06.276355 UTC] Saving snapshot
[2018-07-02 16:10:06.291399 UTC] Starting iteration 80
[2018-07-02 16:10:06.292528 UTC] Start collecting samples
[2018-07-02 16:10:06.547715 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:06.566629 UTC] Performing policy update
[2018-07-02 16:10:06.567801 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:06.576687 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:06.646343 UTC] Performing line search
[2018-07-02 16:10:06.652798 UTC] Updating baseline
[2018-07-02 16:10:06.737032 UTC] Computing logging information
-------------------------------------
| Iteration            | 80         |
| ExpectedImprovement  | 0.019313   |
| ActualImprovement    | 0.016624   |
| ImprovementRatio     | 0.86079    |
| MeanKL               | 0.009196   |
| Entropy              | 0.49589    |
| Perplexity           | 1.642      |
| AveragePolicyProb[0] | 0.50659    |
| AveragePolicyProb[1] | 0.49341    |
| AverageReturn        | 198.08     |
| MinReturn            | 49         |
| MaxReturn            | 200        |
| StdReturn            | 15.303     |
| AverageEpisodeLength | 198.08     |
| MinEpisodeLength     | 49         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.303     |
| TotalNEpisodes       | 1040       |
| TotalNSamples        | 1.6018e+05 |
| ExplainedVariance    | 0.32415    |
-------------------------------------
[2018-07-02 16:10:07.491863 UTC] Saving snapshot
[2018-07-02 16:10:07.503456 UTC] Starting iteration 81
[2018-07-02 16:10:07.504386 UTC] Start collecting samples
[2018-07-02 16:10:07.922124 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:07.950701 UTC] Performing policy update
[2018-07-02 16:10:07.951897 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:07.966027 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:08.060404 UTC] Performing line search
[2018-07-02 16:10:08.070466 UTC] Updating baseline
[2018-07-02 16:10:08.147380 UTC] Computing logging information
-------------------------------------
| Iteration            | 81         |
| ExpectedImprovement  | 0.010451   |
| ActualImprovement    | 0.0097995  |
| ImprovementRatio     | 0.93771    |
| MeanKL               | 0.0071372  |
| Entropy              | 0.48912    |
| Perplexity           | 1.6309     |
| AveragePolicyProb[0] | 0.50067    |
| AveragePolicyProb[1] | 0.49933    |
| AverageReturn        | 198.08     |
| MinReturn            | 49         |
| MaxReturn            | 200        |
| StdReturn            | 15.303     |
| AverageEpisodeLength | 198.08     |
| MinEpisodeLength     | 49         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.303     |
| TotalNEpisodes       | 1054       |
| TotalNSamples        | 1.6298e+05 |
| ExplainedVariance    | 0.47195    |
-------------------------------------
[2018-07-02 16:10:08.937517 UTC] Saving snapshot
[2018-07-02 16:10:08.945993 UTC] Starting iteration 82
[2018-07-02 16:10:08.946747 UTC] Start collecting samples
[2018-07-02 16:10:09.155768 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:09.168520 UTC] Performing policy update
[2018-07-02 16:10:09.169479 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:09.177908 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:09.242452 UTC] Performing line search
[2018-07-02 16:10:09.251909 UTC] Updating baseline
[2018-07-02 16:10:09.321923 UTC] Computing logging information
-------------------------------------
| Iteration            | 82         |
| ExpectedImprovement  | 0.0089457  |
| ActualImprovement    | 0.0082333  |
| ImprovementRatio     | 0.92037    |
| MeanKL               | 0.0070562  |
| Entropy              | 0.47943    |
| Perplexity           | 1.6152     |
| AveragePolicyProb[0] | 0.51122    |
| AveragePolicyProb[1] | 0.48878    |
| AverageReturn        | 197.73     |
| MinReturn            | 49         |
| MaxReturn            | 200        |
| StdReturn            | 15.651     |
| AverageEpisodeLength | 197.73     |
| MinEpisodeLength     | 49         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.651     |
| TotalNEpisodes       | 1060       |
| TotalNSamples        | 1.6415e+05 |
| ExplainedVariance    | 0.6056     |
-------------------------------------
[2018-07-02 16:10:10.070945 UTC] Saving snapshot
[2018-07-02 16:10:10.080491 UTC] Starting iteration 83
[2018-07-02 16:10:10.081765 UTC] Start collecting samples
[2018-07-02 16:10:10.358183 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:10.375075 UTC] Performing policy update
[2018-07-02 16:10:10.376137 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:10.384679 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:10.450843 UTC] Performing line search
[2018-07-02 16:10:10.459260 UTC] Updating baseline
[2018-07-02 16:10:10.531464 UTC] Computing logging information
-------------------------------------
| Iteration            | 83         |
| ExpectedImprovement  | 0.012374   |
| ActualImprovement    | 0.0098518  |
| ImprovementRatio     | 0.79618    |
| MeanKL               | 0.0073362  |
| Entropy              | 0.45216    |
| Perplexity           | 1.5717     |
| AveragePolicyProb[0] | 0.4921     |
| AveragePolicyProb[1] | 0.5079     |
| AverageReturn        | 197.56     |
| MinReturn            | 49         |
| MaxReturn            | 200        |
| StdReturn            | 15.718     |
| AverageEpisodeLength | 197.56     |
| MinEpisodeLength     | 49         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.718     |
| TotalNEpisodes       | 1072       |
| TotalNSamples        | 1.6653e+05 |
| ExplainedVariance    | 0.35625    |
-------------------------------------
[2018-07-02 16:10:11.320498 UTC] Saving snapshot
[2018-07-02 16:10:11.329552 UTC] Starting iteration 84
[2018-07-02 16:10:11.330395 UTC] Start collecting samples
[2018-07-02 16:10:11.545945 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:11.561467 UTC] Performing policy update
[2018-07-02 16:10:11.562293 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:11.568814 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:11.634829 UTC] Performing line search
[2018-07-02 16:10:11.640978 UTC] Updating baseline
[2018-07-02 16:10:11.710551 UTC] Computing logging information
-------------------------------------
| Iteration            | 84         |
| ExpectedImprovement  | 0.011101   |
| ActualImprovement    | 0.0072396  |
| ImprovementRatio     | 0.65214    |
| MeanKL               | 0.0090856  |
| Entropy              | 0.46917    |
| Perplexity           | 1.5987     |
| AveragePolicyProb[0] | 0.47635    |
| AveragePolicyProb[1] | 0.52365    |
| AverageReturn        | 197.56     |
| MinReturn            | 49         |
| MaxReturn            | 200        |
| StdReturn            | 15.718     |
| AverageEpisodeLength | 197.56     |
| MinEpisodeLength     | 49         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 15.718     |
| TotalNEpisodes       | 1081       |
| TotalNSamples        | 1.6833e+05 |
| ExplainedVariance    | 0.30625    |
-------------------------------------
[2018-07-02 16:10:12.447150 UTC] Saving snapshot
[2018-07-02 16:10:12.455329 UTC] Starting iteration 85
[2018-07-02 16:10:12.456021 UTC] Start collecting samples
[2018-07-02 16:10:12.676642 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:12.692852 UTC] Performing policy update
[2018-07-02 16:10:12.694077 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:12.702564 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:12.766533 UTC] Performing line search
[2018-07-02 16:10:12.772888 UTC] Updating baseline
[2018-07-02 16:10:12.843498 UTC] Computing logging information
-------------------------------------
| Iteration            | 85         |
| ExpectedImprovement  | 0.015478   |
| ActualImprovement    | 0.0093303  |
| ImprovementRatio     | 0.60282    |
| MeanKL               | 0.0090034  |
| Entropy              | 0.47881    |
| Perplexity           | 1.6141     |
| AveragePolicyProb[0] | 0.49879    |
| AveragePolicyProb[1] | 0.50121    |
| AverageReturn        | 199.02     |
| MinReturn            | 165        |
| MaxReturn            | 200        |
| StdReturn            | 4.9274     |
| AverageEpisodeLength | 199.02     |
| MinEpisodeLength     | 165        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 4.9274     |
| TotalNEpisodes       | 1090       |
| TotalNSamples        | 1.7013e+05 |
| ExplainedVariance    | 0.61674    |
-------------------------------------
[2018-07-02 16:10:13.616352 UTC] Saving snapshot
[2018-07-02 16:10:13.625118 UTC] Starting iteration 86
[2018-07-02 16:10:13.625770 UTC] Start collecting samples
[2018-07-02 16:10:13.936523 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:13.951678 UTC] Performing policy update
[2018-07-02 16:10:13.952560 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:13.958913 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:14.023236 UTC] Performing line search
[2018-07-02 16:10:14.027437 UTC] Updating baseline
[2018-07-02 16:10:14.120491 UTC] Computing logging information
-------------------------------------
| Iteration            | 86         |
| ExpectedImprovement  | 0.014043   |
| ActualImprovement    | 0.0082372  |
| ImprovementRatio     | 0.58656    |
| MeanKL               | 0.0069279  |
| Entropy              | 0.47199    |
| Perplexity           | 1.6032     |
| AveragePolicyProb[0] | 0.49066    |
| AveragePolicyProb[1] | 0.50934    |
| AverageReturn        | 199.02     |
| MinReturn            | 165        |
| MaxReturn            | 200        |
| StdReturn            | 4.9274     |
| AverageEpisodeLength | 199.02     |
| MinEpisodeLength     | 165        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 4.9274     |
| TotalNEpisodes       | 1103       |
| TotalNSamples        | 1.7273e+05 |
| ExplainedVariance    | 0.32667    |
-------------------------------------
[2018-07-02 16:10:14.836313 UTC] Saving snapshot
[2018-07-02 16:10:14.845131 UTC] Starting iteration 87
[2018-07-02 16:10:14.845665 UTC] Start collecting samples
[2018-07-02 16:10:15.064570 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:15.079616 UTC] Performing policy update
[2018-07-02 16:10:15.080677 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:15.088545 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:15.154190 UTC] Performing line search
[2018-07-02 16:10:15.158416 UTC] Updating baseline
[2018-07-02 16:10:15.238764 UTC] Computing logging information
-------------------------------------
| Iteration            | 87         |
| ExpectedImprovement  | 0.016002   |
| ActualImprovement    | 0.012804   |
| ImprovementRatio     | 0.80015    |
| MeanKL               | 0.0085033  |
| Entropy              | 0.47502    |
| Perplexity           | 1.608      |
| AveragePolicyProb[0] | 0.50717    |
| AveragePolicyProb[1] | 0.49283    |
| AverageReturn        | 199.02     |
| MinReturn            | 165        |
| MaxReturn            | 200        |
| StdReturn            | 4.9274     |
| AverageEpisodeLength | 199.02     |
| MinEpisodeLength     | 165        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 4.9274     |
| TotalNEpisodes       | 1110       |
| TotalNSamples        | 1.7413e+05 |
| ExplainedVariance    | 0.47287    |
-------------------------------------
[2018-07-02 16:10:16.043520 UTC] Saving snapshot
[2018-07-02 16:10:16.051497 UTC] Starting iteration 88
[2018-07-02 16:10:16.052379 UTC] Start collecting samples
[2018-07-02 16:10:16.323300 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:16.338433 UTC] Performing policy update
[2018-07-02 16:10:16.339301 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:16.345659 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:16.410426 UTC] Performing line search
[2018-07-02 16:10:16.414639 UTC] Updating baseline
[2018-07-02 16:10:16.515357 UTC] Computing logging information
-------------------------------------
| Iteration            | 88         |
| ExpectedImprovement  | 0.024059   |
| ActualImprovement    | 0.014032   |
| ImprovementRatio     | 0.58323    |
| MeanKL               | 0.0079878  |
| Entropy              | 0.47532    |
| Perplexity           | 1.6085     |
| AveragePolicyProb[0] | 0.5043     |
| AveragePolicyProb[1] | 0.4957     |
| AverageReturn        | 199.02     |
| MinReturn            | 165        |
| MaxReturn            | 200        |
| StdReturn            | 4.9274     |
| AverageEpisodeLength | 199.02     |
| MinEpisodeLength     | 165        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 4.9274     |
| TotalNEpisodes       | 1120       |
| TotalNSamples        | 1.7613e+05 |
| ExplainedVariance    | 0.19704    |
-------------------------------------
[2018-07-02 16:10:17.290137 UTC] Saving snapshot
[2018-07-02 16:10:17.297964 UTC] Starting iteration 89
[2018-07-02 16:10:17.298880 UTC] Start collecting samples
[2018-07-02 16:10:17.634718 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:17.650081 UTC] Performing policy update
[2018-07-02 16:10:17.650976 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:17.657169 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:17.721051 UTC] Performing line search
[2018-07-02 16:10:17.725223 UTC] Updating baseline
[2018-07-02 16:10:17.803423 UTC] Computing logging information
-------------------------------------
| Iteration            | 89         |
| ExpectedImprovement  | 0.01502    |
| ActualImprovement    | 0.010965   |
| ImprovementRatio     | 0.73001    |
| MeanKL               | 0.0086972  |
| Entropy              | 0.44563    |
| Perplexity           | 1.5615     |
| AveragePolicyProb[0] | 0.50533    |
| AveragePolicyProb[1] | 0.49467    |
| AverageReturn        | 199.43     |
| MinReturn            | 165        |
| MaxReturn            | 200        |
| StdReturn            | 3.8814     |
| AverageEpisodeLength | 199.43     |
| MinEpisodeLength     | 165        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 3.8814     |
| TotalNEpisodes       | 1134       |
| TotalNSamples        | 1.7893e+05 |
| ExplainedVariance    | 0.41986    |
-------------------------------------
[2018-07-02 16:10:18.725048 UTC] Saving snapshot
[2018-07-02 16:10:18.737055 UTC] Starting iteration 90
[2018-07-02 16:10:18.738005 UTC] Start collecting samples
[2018-07-02 16:10:18.953291 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:18.967292 UTC] Performing policy update
[2018-07-02 16:10:18.968181 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:18.974231 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:19.036939 UTC] Performing line search
[2018-07-02 16:10:19.043166 UTC] Updating baseline
[2018-07-02 16:10:19.122300 UTC] Computing logging information
-------------------------------------
| Iteration            | 90         |
| ExpectedImprovement  | 0.0096596  |
| ActualImprovement    | 0.0086135  |
| ImprovementRatio     | 0.8917     |
| MeanKL               | 0.0080102  |
| Entropy              | 0.43894    |
| Perplexity           | 1.5511     |
| AveragePolicyProb[0] | 0.50111    |
| AveragePolicyProb[1] | 0.49889    |
| AverageReturn        | 199.43     |
| MinReturn            | 165        |
| MaxReturn            | 200        |
| StdReturn            | 3.8814     |
| AverageEpisodeLength | 199.43     |
| MinEpisodeLength     | 165        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 3.8814     |
| TotalNEpisodes       | 1140       |
| TotalNSamples        | 1.8013e+05 |
| ExplainedVariance    | 0.39962    |
-------------------------------------
[2018-07-02 16:10:19.924399 UTC] Saving snapshot
[2018-07-02 16:10:19.933103 UTC] Starting iteration 91
[2018-07-02 16:10:19.933930 UTC] Start collecting samples
[2018-07-02 16:10:20.225559 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:20.241928 UTC] Performing policy update
[2018-07-02 16:10:20.242769 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:20.248900 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:20.320190 UTC] Performing line search
[2018-07-02 16:10:20.324295 UTC] Updating baseline
[2018-07-02 16:10:20.402519 UTC] Computing logging information
-------------------------------------
| Iteration            | 91         |
| ExpectedImprovement  | 0.0078642  |
| ActualImprovement    | 0.0044296  |
| ImprovementRatio     | 0.56326    |
| MeanKL               | 0.0057783  |
| Entropy              | 0.42965    |
| Perplexity           | 1.5367     |
| AveragePolicyProb[0] | 0.50215    |
| AveragePolicyProb[1] | 0.49785    |
| AverageReturn        | 199.43     |
| MinReturn            | 165        |
| MaxReturn            | 200        |
| StdReturn            | 3.8814     |
| AverageEpisodeLength | 199.43     |
| MinEpisodeLength     | 165        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 3.8814     |
| TotalNEpisodes       | 1152       |
| TotalNSamples        | 1.8253e+05 |
| ExplainedVariance    | 0.06548    |
-------------------------------------
[2018-07-02 16:10:21.197758 UTC] Saving snapshot
[2018-07-02 16:10:21.205890 UTC] Starting iteration 92
[2018-07-02 16:10:21.206745 UTC] Start collecting samples
[2018-07-02 16:10:21.482522 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:21.496934 UTC] Performing policy update
[2018-07-02 16:10:21.497781 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:21.503885 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:21.566286 UTC] Performing line search
[2018-07-02 16:10:21.570649 UTC] Updating baseline
[2018-07-02 16:10:21.640526 UTC] Computing logging information
-------------------------------------
| Iteration            | 92         |
| ExpectedImprovement  | 0.014193   |
| ActualImprovement    | 0.010105   |
| ImprovementRatio     | 0.71202    |
| MeanKL               | 0.0083835  |
| Entropy              | 0.43853    |
| Perplexity           | 1.5504     |
| AveragePolicyProb[0] | 0.51125    |
| AveragePolicyProb[1] | 0.48875    |
| AverageReturn        | 199.78     |
| MinReturn            | 183        |
| MaxReturn            | 200        |
| StdReturn            | 1.7583     |
| AverageEpisodeLength | 199.78     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 1.7583     |
| TotalNEpisodes       | 1161       |
| TotalNSamples        | 1.8433e+05 |
| ExplainedVariance    | 0.66494    |
-------------------------------------
[2018-07-02 16:10:22.419267 UTC] Saving snapshot
[2018-07-02 16:10:22.427838 UTC] Starting iteration 93
[2018-07-02 16:10:22.428555 UTC] Start collecting samples
[2018-07-02 16:10:22.769828 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:22.786418 UTC] Performing policy update
[2018-07-02 16:10:22.787438 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:22.795894 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:22.861020 UTC] Performing line search
[2018-07-02 16:10:22.870437 UTC] Updating baseline
[2018-07-02 16:10:22.949214 UTC] Computing logging information
-------------------------------------
| Iteration            | 93         |
| ExpectedImprovement  | 0.0076313  |
| ActualImprovement    | 0.0082243  |
| ImprovementRatio     | 1.0777     |
| MeanKL               | 0.0072663  |
| Entropy              | 0.44347    |
| Perplexity           | 1.5581     |
| AveragePolicyProb[0] | 0.5118     |
| AveragePolicyProb[1] | 0.4882     |
| AverageReturn        | 199.78     |
| MinReturn            | 183        |
| MaxReturn            | 200        |
| StdReturn            | 1.7583     |
| AverageEpisodeLength | 199.78     |
| MinEpisodeLength     | 183        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 1.7583     |
| TotalNEpisodes       | 1170       |
| TotalNSamples        | 1.8613e+05 |
| ExplainedVariance    | 0.59861    |
-------------------------------------
[2018-07-02 16:10:23.719752 UTC] Saving snapshot
[2018-07-02 16:10:23.729011 UTC] Starting iteration 94
[2018-07-02 16:10:23.729653 UTC] Start collecting samples
[2018-07-02 16:10:23.997976 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:24.013847 UTC] Performing policy update
[2018-07-02 16:10:24.014828 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:24.021320 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:24.085169 UTC] Performing line search
[2018-07-02 16:10:24.092736 UTC] Updating baseline
[2018-07-02 16:10:24.163446 UTC] Computing logging information
-------------------------------------
| Iteration            | 94         |
| ExpectedImprovement  | 0.0084272  |
| ActualImprovement    | 0.0071704  |
| ImprovementRatio     | 0.85086    |
| MeanKL               | 0.0068121  |
| Entropy              | 0.45767    |
| Perplexity           | 1.5804     |
| AveragePolicyProb[0] | 0.5086     |
| AveragePolicyProb[1] | 0.4914     |
| AverageReturn        | 199.95     |
| MinReturn            | 195        |
| MaxReturn            | 200        |
| StdReturn            | 0.49749    |
| AverageEpisodeLength | 199.95     |
| MinEpisodeLength     | 195        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0.49749    |
| TotalNEpisodes       | 1183       |
| TotalNSamples        | 1.8873e+05 |
| ExplainedVariance    | 0.5166     |
-------------------------------------
[2018-07-02 16:10:24.968009 UTC] Saving snapshot
[2018-07-02 16:10:24.977110 UTC] Starting iteration 95
[2018-07-02 16:10:24.977923 UTC] Start collecting samples
[2018-07-02 16:10:25.316927 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:25.335590 UTC] Performing policy update
[2018-07-02 16:10:25.337036 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:25.345730 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:25.418627 UTC] Performing line search
[2018-07-02 16:10:25.422733 UTC] Updating baseline
[2018-07-02 16:10:25.536490 UTC] Computing logging information
-------------------------------------
| Iteration            | 95         |
| ExpectedImprovement  | 0.011873   |
| ActualImprovement    | 0.0082768  |
| ImprovementRatio     | 0.69711    |
| MeanKL               | 0.0098219  |
| Entropy              | 0.46708    |
| Perplexity           | 1.5953     |
| AveragePolicyProb[0] | 0.50047    |
| AveragePolicyProb[1] | 0.49953    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1190       |
| TotalNSamples        | 1.9013e+05 |
| ExplainedVariance    | 0.25729    |
-------------------------------------
[2018-07-02 16:10:26.508236 UTC] Saving snapshot
[2018-07-02 16:10:26.517776 UTC] Starting iteration 96
[2018-07-02 16:10:26.518437 UTC] Start collecting samples
[2018-07-02 16:10:26.803337 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:26.817360 UTC] Performing policy update
[2018-07-02 16:10:26.818246 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:26.824744 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:26.888070 UTC] Performing line search
[2018-07-02 16:10:26.892498 UTC] Updating baseline
[2018-07-02 16:10:26.986246 UTC] Computing logging information
-------------------------------------
| Iteration            | 96         |
| ExpectedImprovement  | 0.010877   |
| ActualImprovement    | 0.0014408  |
| ImprovementRatio     | 0.13246    |
| MeanKL               | 0.0034546  |
| Entropy              | 0.46014    |
| Perplexity           | 1.5843     |
| AveragePolicyProb[0] | 0.51812    |
| AveragePolicyProb[1] | 0.48188    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1200       |
| TotalNSamples        | 1.9213e+05 |
| ExplainedVariance    | 0.34883    |
-------------------------------------
[2018-07-02 16:10:28.252519 UTC] Saving snapshot
[2018-07-02 16:10:28.261456 UTC] Starting iteration 97
[2018-07-02 16:10:28.263483 UTC] Start collecting samples
[2018-07-02 16:10:28.587597 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:28.606241 UTC] Performing policy update
[2018-07-02 16:10:28.607299 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:28.615642 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:28.683379 UTC] Performing line search
[2018-07-02 16:10:28.687766 UTC] Updating baseline
[2018-07-02 16:10:28.761837 UTC] Computing logging information
-------------------------------------
| Iteration            | 97         |
| ExpectedImprovement  | 0.0096332  |
| ActualImprovement    | 0.0069277  |
| ImprovementRatio     | 0.71914    |
| MeanKL               | 0.006942   |
| Entropy              | 0.46349    |
| Perplexity           | 1.5896     |
| AveragePolicyProb[0] | 0.51246    |
| AveragePolicyProb[1] | 0.48754    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1214       |
| TotalNSamples        | 1.9493e+05 |
| ExplainedVariance    | 0.35243    |
-------------------------------------
[2018-07-02 16:10:29.610197 UTC] Saving snapshot
[2018-07-02 16:10:29.620521 UTC] Starting iteration 98
[2018-07-02 16:10:29.621317 UTC] Start collecting samples
[2018-07-02 16:10:29.843359 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:29.857724 UTC] Performing policy update
[2018-07-02 16:10:29.859241 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:29.866148 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:29.942286 UTC] Performing line search
[2018-07-02 16:10:29.952970 UTC] Updating baseline
[2018-07-02 16:10:30.036662 UTC] Computing logging information
-------------------------------------
| Iteration            | 98         |
| ExpectedImprovement  | 0.0056015  |
| ActualImprovement    | 0.0061772  |
| ImprovementRatio     | 1.1028     |
| MeanKL               | 0.0071197  |
| Entropy              | 0.4691     |
| Perplexity           | 1.5986     |
| AveragePolicyProb[0] | 0.50278    |
| AveragePolicyProb[1] | 0.49722    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1220       |
| TotalNSamples        | 1.9613e+05 |
| ExplainedVariance    | 0.47782    |
-------------------------------------
[2018-07-02 16:10:30.867023 UTC] Saving snapshot
[2018-07-02 16:10:30.877127 UTC] Starting iteration 99
[2018-07-02 16:10:30.878650 UTC] Start collecting samples
[2018-07-02 16:10:31.157582 UTC] Computing input variables for policy optimization
[2018-07-02 16:10:31.196013 UTC] Performing policy update
[2018-07-02 16:10:31.197118 UTC] Computing gradient in Euclidean space
[2018-07-02 16:10:31.207395 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2018-07-02 16:10:31.287621 UTC] Performing line search
[2018-07-02 16:10:31.292465 UTC] Updating baseline
[2018-07-02 16:10:31.373093 UTC] Computing logging information
-------------------------------------
| Iteration            | 99         |
| ExpectedImprovement  | 0.016123   |
| ActualImprovement    | 0.010656   |
| ImprovementRatio     | 0.66091    |
| MeanKL               | 0.0097248  |
| Entropy              | 0.47911    |
| Perplexity           | 1.6146     |
| AveragePolicyProb[0] | 0.48307    |
| AveragePolicyProb[1] | 0.51693    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1232       |
| TotalNSamples        | 1.9853e+05 |
| ExplainedVariance    | 0.056437   |
-------------------------------------
[2018-07-02 16:10:32.250530 UTC] Saving snapshot
