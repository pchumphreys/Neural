{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution strategy\n",
    "\n",
    "Here for funsies I try to understand the Evolution Strategy implementation from https://arxiv.org/pdf/1703.03864.pdf\n",
    "\n",
    "I implemented a simple serial version of the code, along with a basic Adam optimizer to use the extracted gradient estimate for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "  \n",
    "from tensorflow.python.ops import variables\n",
    "from tensorflow.python.framework import ops\n",
    "from scipy.stats import rankdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Setup the gym environment!\n",
    "import gym\n",
    "env_name = 'BipedalWalker-v2'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adam Optimizer https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "# To do --> learn more\n",
    "\n",
    "# Recipe:\n",
    "# t <- t + 1\n",
    "# lr_t <- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)\n",
    "\n",
    "# m_t <- beta1 * m_{t-1} + (1 - beta1) * g\n",
    "# v_t <- beta2 * v_{t-1} + (1 - beta2) * g * g\n",
    "# variable <- variable - lr_t * m_t / (sqrt(v_t) + epsilon)\n",
    "\n",
    "class Simple_Optimizer(object):\n",
    "    def __init__(self, variables,grad_vars, alpha = 1e-3,beta1 = 1e-1):\n",
    "        self.a = tf.Variable(alpha,trainable=False)\n",
    "        self.b1 = tf.constant(beta1)\n",
    "        \n",
    "        self.vars = variables\n",
    "        self.grad_vars = grad_vars\n",
    "        \n",
    "        self.create_update_ops()\n",
    "        \n",
    "    def create_update_ops(self):\n",
    "        ops = []\n",
    "        for var,g in zip(self.vars, self.grad_vars):\n",
    "            ops.append(var.assign_add(self.a * g))\n",
    "        self.update = tf.group(*ops)\n",
    "\n",
    "    \n",
    "class Momentum_Optimizer(object):\n",
    "    def __init__(self, variables,grad_vars, alpha = 1e-3,beta1 = 0.9):\n",
    "        self.a = tf.Variable(alpha,trainable=False)\n",
    "        self.b1 = tf.constant(beta1)\n",
    "        \n",
    "        self.vars = variables\n",
    "        self.grad_vars = grad_vars\n",
    "        \n",
    "        self.m = []\n",
    "        for var in self.vars:\n",
    "            self.m.append(tf.Variable(tf.zeros(var.get_shape()),trainable=False))\n",
    "        \n",
    "        self.create_update_ops()\n",
    "        \n",
    "    def create_update_ops(self):\n",
    "        ops = []\n",
    "        for var,g,m in zip(self.vars, self.grad_vars, self.m):\n",
    "            m_op = m.assign(self.b1 * m + (1 - self.b1) * g)\n",
    "            with tf.get_default_graph().control_dependencies([m_op]): # Ensure m runs first\n",
    "                var_op = var.assign_add(self.a * m)\n",
    "            ops += [m_op,var_op]\n",
    "        self.update = tf.group(*ops)\n",
    "    \n",
    "class Adam_Optimizer(object):\n",
    "    def __init__(self, variables,grad_vars, alpha = 1e-3,beta1 = 0.9, beta2 = 0.999, eps = 1e-8):\n",
    "        self.t = tf.Variable(0.0,trainable=False)\n",
    "        self.a = tf.Variable(alpha,trainable=False)\n",
    "        self.b1 = tf.constant(beta1)\n",
    "        self.b2 = tf.constant(beta2)\n",
    "        self.eps = tf.constant(eps)\n",
    "        self.vars = variables\n",
    "        self.grad_vars = grad_vars\n",
    "        \n",
    "        self.m = []\n",
    "        self.v = []\n",
    "        for var in self.vars:\n",
    "            self.m.append(tf.Variable(tf.zeros(var.get_shape()),trainable=False))\n",
    "            self.v.append(tf.Variable(tf.zeros(var.get_shape()),trainable=False))\n",
    "        \n",
    "        self.create_update_ops()\n",
    "        \n",
    "    def create_update_ops(self):\n",
    "        t_op = self.t.assign_add(1.0)\n",
    "        with tf.get_default_graph().control_dependencies([t_op]): # Ensure t runs first\n",
    "            a_op = self.a.assign(self.a * tf.sqrt(1- tf.pow(self.b2,self.t)) / (1 - tf.pow(self.b1,self.t)))   \n",
    "        ops = [a_op,t_op]\n",
    "        \n",
    "        for var,g,m,v in zip(self.vars, self.grad_vars, self.m, self.v):\n",
    "            m_op = m.assign(self.b1 * m + (1 - self.b1) * g)\n",
    "            v_op = v.assign(self.b2 * v + (1 - self.b2) * tf.square(g))\n",
    "            with tf.get_default_graph().control_dependencies([m_op,v_op,a_op]): # Ensure m,v,a runs first\n",
    "                var_op = var.assign_add(self.a * m / (tf.sqrt(v) + self.eps))\n",
    "            ops += [m_op,v_op,var_op]\n",
    "        self.update = tf.group(*ops)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution strategy gradient estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rescale_to_normal(array):\n",
    "    # Helper function\n",
    "    return (array - np.mean(array))/ np.std(array)\n",
    "\n",
    "\n",
    "class ES_gradient_estimator(object):\n",
    "    def __init__(self, sigma = 0.01,weight_decay=0.005, mini_batch_size = 30):\n",
    "        self.sigma = tf.constant(np.float(sigma)) # Standard deviation of weight adjustments\n",
    "        self.weight_decay = tf.constant(weight_decay)\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        \n",
    "        self.current_ind = tf.placeholder(\"int32\")\n",
    "        self.update_weights = tf.placeholder(\"float32\",shape = [mini_batch_size])\n",
    "        \n",
    "        # Used for fitness shaping http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf\n",
    "        self.fitness_shape_rank_vals = [np.max([0,np.log(self.mini_batch_size/2.0+1)-np.log(ind)]) for ind in np.arange(1,(self.mini_batch_size+1))]\n",
    "        self.fitness_shape_rank_vals = self.fitness_shape_rank_vals/np.sum(self.fitness_shape_rank_vals)\n",
    "#         self.fitness_shape_rank_vals = -(1.0/self.mini_batch_size)*(np.arange(0,self.mini_batch_size)/(self.mini_batch_size-1.0) - 0.5)\n",
    "        \n",
    "        # Get the weights and biases from TF\n",
    "        self.set_links_to_vars()\n",
    "\n",
    "    def set_links_to_vars(self):\n",
    "        # Have to pull out the relevant variables from TF, make the operations necessary to modify them efficiently\n",
    "        self.noise_vars = []\n",
    "        self.grad_vars = []\n",
    "        \n",
    "        self.new_noise_vals_ops = []\n",
    "        self.set_noise_ops = []\n",
    "        self.copy_state_ops = []\n",
    "        self.reset_state_ops = []\n",
    "        self.calc_gradient_ops = []\n",
    "        \n",
    "        self.vars = variables.trainable_variables() + ops.get_collection(ops.GraphKeys.TRAINABLE_RESOURCE_VARIABLES)\n",
    "        for var in self.vars:\n",
    "            # Mirrored sampling! \n",
    "            tmp_noise_var = tf.random_normal([np.int(self.mini_batch_size/2)] + var.get_shape().as_list())\n",
    "            noise_var = tf.concat([tmp_noise_var,-tmp_noise_var],0)\n",
    "            \n",
    "            current_noise_vals = tf.Variable(tf.zeros(noise_var.get_shape()),trainable=False)\n",
    "            # Only want to make random noise variable change after each round, so have to do this fudge\n",
    "            self.new_noise_vals_ops.append(current_noise_vals.assign(noise_var))\n",
    "            self.noise_vars.append(current_noise_vals)\n",
    "            copy_var = tf.Variable(var.initialized_value(),trainable=False)\n",
    "            \n",
    "            self.set_noise_ops.append(var.assign(copy_var + self.sigma * current_noise_vals[self.current_ind]))\n",
    "            self.copy_state_ops.append(copy_var.assign(var))\n",
    "            self.reset_state_ops.append(var.assign(copy_var))\n",
    "            \n",
    "            grad_var = tf.Variable(tf.zeros(var.get_shape()),trainable=False)\n",
    "            self.grad_vars.append(grad_var)\n",
    "            self.calc_gradient_ops.append(grad_var.assign(tf.tensordot(self.update_weights, current_noise_vals,axes=[0,0]) - self.weight_decay * var)) # Note weight decay\n",
    "            \n",
    "    # Three different flavours of update mechanism. In practice only using the fitness shaping version\n",
    "    def calc_update_based_on_reward(self,rewards):\n",
    "        \n",
    "        self.calc_weights = 1.0/(self.mini_batch_size * self.sigma.eval()) * rescale_to_normal(rewards)\n",
    "    \n",
    "    def calc_update_based_on_highest_reward(self,rewards):\n",
    "        \n",
    "        self.calc_weights = self.sigma.eval()*np.array([(1.0 if val == np.max(rewards) else 0) for val in np.array(rewards)])\n",
    "    \n",
    "    def calc_update_fitness_shaping(self,rewards):\n",
    "        \n",
    "        k = self.mini_batch_size - rankdata(rewards_t, method='ordinal')\n",
    "        self.calc_weights = self.fitness_shape_rank_vals[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # THIS IS NECESSARY BEFORE MAKING NEW SESSION TO STOP IT ERRORING!!\n",
    "try:\n",
    "    sess\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    sess.close()\n",
    "    del sess\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Borrowed some bits from http://mat.univie.ac.at/~grohs/tmp/DeepLearningClass_Jun28_1.html\n",
    "n_inputs = env.observation_space.shape[0]\n",
    "n_hidden = 50  \n",
    "n_hlayers = 2\n",
    "n_outputs = env.action_space.shape[0]\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "# 2. Build the neural network\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "Y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "layer = X\n",
    "for _ in range(n_hlayers):\n",
    "    layer = tf.layers.dense(layer, n_hidden, activation=tf.nn.relu,\n",
    "                         kernel_initializer=initializer)\n",
    "\n",
    "raw_action = tf.layers.dense(layer, n_outputs,\n",
    "                          kernel_initializer=initializer)\n",
    "action = tf.clip_by_value(raw_action,tf.expand_dims(env.action_space.low,0),tf.expand_dims(env.action_space.high,0)) # Have to clip the action space. This might be a bad idea\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "es = ES_gradient_estimator(sigma = 0.05,mini_batch_size=200)\n",
    "\n",
    "# optimizer = Adam_Optimizer(es.vars,es.grad_vars,alpha=0.03,beta1=0.7,beta2=0.99)\n",
    "# optimizer = Simple_Optimizer(es.vars,es.grad_vars,alpha=0.1)\n",
    "optimizer = Momentum_Optimizer(es.vars,es.grad_vars,alpha=0.1,beta1=0.8)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.17903239 -0.44995713 -1.        ]]\n",
      "[-0.01384725 -0.01731161 -0.02824958 -0.01428074  0.46836835  0.99999988\n",
      "  0.08038187 -1.13134289  1.          0.36950374  0.8752476   0.0851922\n",
      " -0.99999396  1.          0.44615921  0.45122603  0.46701789  0.49548635\n",
      "  0.5405792   0.6097663   0.71774787  0.89667439  1.          1.        ]\n",
      "-0.207235966964312\n"
     ]
    }
   ],
   "source": [
    "# Some idiot checking here for when running new environments\n",
    "observation = env.reset()\n",
    "act = sess.run(action, feed_dict={X: np.expand_dims(observation,axis=0)})\n",
    "print(act)\n",
    "observation, reward, done, info = env.step(act[0])\n",
    "print(observation)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "mini batch results for run 1 : -97.763082\n",
      "mini batch results for run 2 : -94.768030\n",
      "mini batch results for run 3 : -95.242487\n",
      "mini batch results for run 4 : -95.028203\n",
      "mini batch results for run 5 : -91.267639\n",
      "mini batch results for run 6 : -87.946751\n",
      "mini batch results for run 7 : -80.591184\n",
      "mini batch results for run 8 : -67.084345\n",
      "mini batch results for run 9 : -67.816033\n",
      "mini batch results for run 10 : -75.109876\n",
      "mini batch results for run 11 : -86.779048\n",
      "mini batch results for run 12 : -92.803668\n",
      "mini batch results for run 13 : -84.741979\n",
      "mini batch results for run 14 : -71.285320\n",
      "mini batch results for run 15 : -55.309078\n",
      "mini batch results for run 16 : -52.575483\n",
      "mini batch results for run 17 : -56.603329\n",
      "mini batch results for run 18 : -52.853080\n",
      "mini batch results for run 19 : -52.550976\n",
      "mini batch results for run 20 : -53.789244\n",
      "mini batch results for run 21 : -51.462712\n",
      "mini batch results for run 22 : -52.156940\n",
      "mini batch results for run 23 : -59.303285\n",
      "mini batch results for run 24 : -73.420836\n",
      "mini batch results for run 25 : -86.290038\n",
      "mini batch results for run 26 : -75.515098\n",
      "mini batch results for run 27 : -54.501385\n",
      "mini batch results for run 28 : -39.928897\n",
      "mini batch results for run 29 : -37.094578\n",
      "mini batch results for run 30 : -33.478727\n",
      "mini batch results for run 31 : -37.260033\n",
      "mini batch results for run 32 : -48.243054\n",
      "mini batch results for run 33 : -48.995998\n",
      "mini batch results for run 34 : -47.985076\n",
      "mini batch results for run 35 : -37.758834\n",
      "mini batch results for run 36 : -34.012342\n",
      "mini batch results for run 37 : -28.029332\n",
      "mini batch results for run 38 : -26.846417\n",
      "mini batch results for run 39 : -28.001624\n",
      "mini batch results for run 40 : -29.293704\n",
      "mini batch results for run 41 : -29.722647\n",
      "mini batch results for run 42 : -30.647771\n",
      "mini batch results for run 43 : -33.313231\n",
      "mini batch results for run 44 : -32.285752\n",
      "mini batch results for run 45 : -35.738651\n",
      "mini batch results for run 46 : -28.071258\n",
      "mini batch results for run 47 : -26.936742\n",
      "mini batch results for run 48 : -23.972176\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "\n",
    "render = False\n",
    "\n",
    "i = 0\n",
    "max_t = 1000\n",
    "\n",
    "sess.run(optimizer.a.assign(0.15))\n",
    "\n",
    "while i < 1000:\n",
    "    sess.run(es.copy_state_ops)\n",
    "    sess.run(es.new_noise_vals_ops)\n",
    "\n",
    "    rewards_t = np.zeros([es.mini_batch_size])\n",
    "    obs_std = np.zeros([es.mini_batch_size,env.observation_space.shape[0]])\n",
    "    \n",
    "    for j in range(es.mini_batch_size):\n",
    "        observation = env.reset()\n",
    "        obs_t = np.array([observation])\n",
    "        sess.run(es.set_noise_ops, feed_dict={es.current_ind : j})\n",
    "        reward_t = 0\n",
    "        for t in range(max_t):\n",
    "            if render:\n",
    "                env.render()\n",
    "            obs_t = np.append(obs_t,[observation],axis = 0)\n",
    "            \n",
    "            act = sess.run(action, feed_dict={X: np.expand_dims(observation,axis=0)})[0]\n",
    "            observation, reward, done, info = env.step(act)\n",
    "            reward_t += reward\n",
    "            if done or (t == max_t-1):\n",
    "                rewards_t[j] = reward_t\n",
    "                break\n",
    "    print('mini batch results for run %d : %f' % (i + 1,np.mean(rewards_t)))\n",
    "    \n",
    "    es.calc_update_fitness_shaping(rewards_t)\n",
    "    sess.run(es.reset_state_ops)\n",
    "    sess.run(es.calc_gradient_ops,feed_dict={es.update_weights : es.calc_weights})\n",
    "    sess.run(optimizer.update)\n",
    "    i += 1\n",
    "                \n",
    "            \n",
    "# saver.save(sess, \"./my_policy_net_basic.ckpt\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver.save(sess, \"./my_policy_net_basic.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "-16.06925745076827\n"
     ]
    }
   ],
   "source": [
    "rewardsum = 0\n",
    "env = gym.make(env_name)\n",
    "obs = env.reset()\n",
    "for step in range(1000):\n",
    "    env.render()\n",
    "    action_val = action.eval(feed_dict={X: obs.reshape(1, n_inputs)})\n",
    "    obs, reward, done, info = env.step(action_val[0])\n",
    "    rewardsum += reward\n",
    "    if done:\n",
    "        break\n",
    "env.close()\n",
    "print(rewardsum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
