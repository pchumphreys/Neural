{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution strategy\n",
    "\n",
    "Here for funsies I try to understand the Evolution Strategy implementation from https://arxiv.org/pdf/1703.03864.pdf\n",
    "\n",
    "I implemented a simple serial version of the code, along with a basic Adam optimizer to use the extracted gradient estimate for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "  \n",
    "from tensorflow.python.ops import variables\n",
    "from tensorflow.python.framework import ops\n",
    "from scipy.stats import rankdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Setup the gym environment!\n",
    "import gym\n",
    "env_name = 'LunarLanderContinuous-v2'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adam Optimizer https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "# To do --> learn more\n",
    "\n",
    "# Recipe:\n",
    "# t <- t + 1\n",
    "# lr_t <- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)\n",
    "\n",
    "# m_t <- beta1 * m_{t-1} + (1 - beta1) * g\n",
    "# v_t <- beta2 * v_{t-1} + (1 - beta2) * g * g\n",
    "# variable <- variable - lr_t * m_t / (sqrt(v_t) + epsilon)\n",
    "\n",
    "class Simple_Optimizer(object):\n",
    "    def __init__(self, variables,grad_vars, alpha = 1e-3,beta1 = 1e-1):\n",
    "        self.a = tf.constant(alpha)\n",
    "        self.b1 = tf.constant(beta1)\n",
    "        \n",
    "        self.vars = variables\n",
    "        self.grad_vars = grad_vars\n",
    "        \n",
    "        self.create_update_ops()\n",
    "        \n",
    "    def create_update_ops(self):\n",
    "        ops = []\n",
    "        for var,g in zip(self.vars, self.grad_vars):\n",
    "            ops.append(var.assign_add(self.a * g))\n",
    "        self.update = tf.group(*ops)\n",
    "\n",
    "    \n",
    "class Momentum_Optimizer(object):\n",
    "    def __init__(self, variables,grad_vars, alpha = 1e-3,beta1 = 0.9):\n",
    "        self.a = tf.constant(alpha)\n",
    "        self.b1 = tf.constant(beta1)\n",
    "        \n",
    "        self.vars = variables\n",
    "        self.grad_vars = grad_vars\n",
    "        \n",
    "        self.m = []\n",
    "        for var in self.vars:\n",
    "            self.m.append(tf.Variable(tf.zeros(var.get_shape()),trainable=False))\n",
    "        \n",
    "        self.create_update_ops()\n",
    "        \n",
    "    def create_update_ops(self):\n",
    "        ops = []\n",
    "        for var,g,m in zip(self.vars, self.grad_vars, self.m):\n",
    "            m_op = m.assign(self.b1 * m + (1 - self.b1) * g)\n",
    "            with tf.get_default_graph().control_dependencies([m_op]): # Ensure m runs first\n",
    "                var_op = var.assign_add(self.a * m)\n",
    "            ops += [m_op,var_op]\n",
    "        self.update = tf.group(*ops)\n",
    "    \n",
    "class Adam_Optimizer(object):\n",
    "    def __init__(self, variables,grad_vars, alpha = 1e-3,beta1 = 0.9, beta2 = 0.999, eps = 1e-8):\n",
    "        self.t = tf.Variable(0.0,trainable=False)\n",
    "        self.a = tf.Variable(alpha,trainable=False)\n",
    "        self.b1 = tf.constant(beta1)\n",
    "        self.b2 = tf.constant(beta2)\n",
    "        self.eps = tf.constant(eps)\n",
    "        self.vars = variables\n",
    "        self.grad_vars = grad_vars\n",
    "        \n",
    "        self.m = []\n",
    "        self.v = []\n",
    "        for var in self.vars:\n",
    "            self.m.append(tf.Variable(tf.zeros(var.get_shape()),trainable=False))\n",
    "            self.v.append(tf.Variable(tf.zeros(var.get_shape()),trainable=False))\n",
    "        \n",
    "        self.create_update_ops()\n",
    "        \n",
    "    def create_update_ops(self):\n",
    "        t_op = self.t.assign_add(1.0)\n",
    "        with tf.get_default_graph().control_dependencies([t_op]): # Ensure t runs first\n",
    "            a_op = self.a.assign(self.a * tf.sqrt(1- tf.pow(self.b2,self.t)) / (1 - tf.pow(self.b1,self.t)))   \n",
    "        ops = [a_op,t_op]\n",
    "        \n",
    "        for var,g,m,v in zip(self.vars, self.grad_vars, self.m, self.v):\n",
    "            m_op = m.assign(self.b1 * m + (1 - self.b1) * g)\n",
    "            v_op = v.assign(self.b2 * v + (1 - self.b2) * tf.square(g))\n",
    "            with tf.get_default_graph().control_dependencies([m_op,v_op,a_op]): # Ensure m,v,a runs first\n",
    "                var_op = var.assign_add(self.a * m / (tf.sqrt(v) + self.eps))\n",
    "            ops += [m_op,v_op,var_op]\n",
    "        self.update = tf.group(*ops)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution strategy gradient estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rescale_to_normal(array):\n",
    "    # Helper function\n",
    "    return (array - np.mean(array))/ np.std(array)\n",
    "\n",
    "\n",
    "class ES_gradient_estimator(object):\n",
    "    def __init__(self, sigma = 0.01,weight_decay=0.005, mini_batch_size = 30):\n",
    "        self.sigma = tf.constant(np.float(sigma)) # Standard deviation of weight adjustments\n",
    "        self.weight_decay = tf.constant(weight_decay)\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        \n",
    "        self.current_ind = tf.placeholder(\"int32\")\n",
    "        self.update_weights = tf.placeholder(\"float32\",shape = [mini_batch_size])\n",
    "        \n",
    "        # Used for fitness shaping http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf\n",
    "        self.fitness_shape_rank_vals = [np.max([0,np.log(self.mini_batch_size/2.0+1)-np.log(ind)]) for ind in np.arange(1,(self.mini_batch_size+1))]\n",
    "        self.fitness_shape_rank_vals = self.fitness_shape_rank_vals/np.sum(self.fitness_shape_rank_vals)\n",
    "#         self.fitness_shape_rank_vals = -(1.0/self.mini_batch_size)*(np.arange(0,self.mini_batch_size)/(self.mini_batch_size-1.0) - 0.5)\n",
    "        \n",
    "        # Get the weights and biases from TF\n",
    "        self.set_links_to_vars()\n",
    "\n",
    "    def set_links_to_vars(self):\n",
    "        # Have to pull out the relevant variables from TF, make the operations necessary to modify them efficiently\n",
    "        self.noise_vars = []\n",
    "        self.grad_vars = []\n",
    "        \n",
    "        self.new_noise_vals_ops = []\n",
    "        self.set_noise_ops = []\n",
    "        self.copy_state_ops = []\n",
    "        self.reset_state_ops = []\n",
    "        self.calc_gradient_ops = []\n",
    "        \n",
    "        self.vars = variables.trainable_variables() + ops.get_collection(ops.GraphKeys.TRAINABLE_RESOURCE_VARIABLES)\n",
    "        for var in self.vars:\n",
    "            # Mirrored sampling! \n",
    "            tmp_noise_var = tf.random_normal([np.int(self.mini_batch_size/2)] + var.get_shape().as_list())\n",
    "            noise_var = tf.concat([tmp_noise_var,-tmp_noise_var],0)\n",
    "            \n",
    "            current_noise_vals = tf.Variable(tf.zeros(noise_var.get_shape()),trainable=False)\n",
    "            # Only want to make random noise variable change after each round, so have to do this fudge\n",
    "            self.new_noise_vals_ops.append(current_noise_vals.assign(noise_var))\n",
    "            self.noise_vars.append(current_noise_vals)\n",
    "            copy_var = tf.Variable(var.initialized_value(),trainable=False)\n",
    "            \n",
    "            self.set_noise_ops.append(var.assign(copy_var + self.sigma * current_noise_vals[self.current_ind]))\n",
    "            self.copy_state_ops.append(copy_var.assign(var))\n",
    "            self.reset_state_ops.append(var.assign(copy_var))\n",
    "            \n",
    "            grad_var = tf.Variable(tf.zeros(var.get_shape()),trainable=False)\n",
    "            self.grad_vars.append(grad_var)\n",
    "            self.calc_gradient_ops.append(grad_var.assign(tf.tensordot(self.update_weights, current_noise_vals,axes=[0,0]) - self.weight_decay * var)) # Note weight decay\n",
    "            \n",
    "    # Three different flavours of update mechanism. In practice only using the fitness shaping version\n",
    "    def calc_update_based_on_reward(self,rewards):\n",
    "        \n",
    "        self.calc_weights = 1.0/(self.mini_batch_size * self.sigma.eval()) * rescale_to_normal(rewards)\n",
    "    \n",
    "    def calc_update_based_on_highest_reward(self,rewards):\n",
    "        \n",
    "        self.calc_weights = self.sigma.eval()*np.array([(1.0 if val == np.max(rewards) else 0) for val in np.array(rewards)])\n",
    "    \n",
    "    def calc_update_fitness_shaping(self,rewards):\n",
    "        \n",
    "        k = self.mini_batch_size - rankdata(rewards_t, method='ordinal')\n",
    "        self.calc_weights = self.fitness_shape_rank_vals[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # THIS IS NECESSARY BEFORE MAKING NEW SESSION TO STOP IT ERRORING!!\n",
    "try:\n",
    "    sess\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    sess.close()\n",
    "    del sess\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Borrowed some bits from http://mat.univie.ac.at/~grohs/tmp/DeepLearningClass_Jun28_1.html\n",
    "n_inputs = env.observation_space.shape[0]\n",
    "n_hidden = 50  \n",
    "n_hlayers = 2\n",
    "n_outputs = env.action_space.shape[0]\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "# 2. Build the neural network\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "Y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "layer = X\n",
    "for _ in range(n_hlayers):\n",
    "    layer = tf.layers.dense(layer, n_hidden, activation=tf.nn.relu,\n",
    "                         kernel_initializer=initializer)\n",
    "\n",
    "raw_action = tf.layers.dense(layer, n_outputs,\n",
    "                          kernel_initializer=initializer)\n",
    "action = tf.clip_by_value(raw_action,tf.expand_dims(env.action_space.low,0),tf.expand_dims(env.action_space.high,0)) # Have to clip the action space. This might be a bad idea\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "es = ES_gradient_estimator(sigma = 0.05,mini_batch_size=100)\n",
    "\n",
    "# optimizer = Adam_Optimizer(es.vars,es.grad_vars,alpha=0.03,beta1=0.7,beta2=0.99)\n",
    "# optimizer = Simple_Optimizer(es.vars,es.grad_vars,alpha=0.1)\n",
    "optimizer = Momentum_Optimizer(es.vars,es.grad_vars,alpha=0.2,beta1=0.8)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7124109   0.14059368]]\n",
      "[-3.99303436e-04  9.30008799e-01 -2.01963693e-02 -3.66309420e-01\n",
      "  4.64240613e-04  4.52603661e-03  0.00000000e+00  0.00000000e+00]\n",
      "-2.032960954674053\n"
     ]
    }
   ],
   "source": [
    "# Some idiot checking here for when running new environments\n",
    "observation = env.reset()\n",
    "act = sess.run(action, feed_dict={X: np.expand_dims(observation,axis=0)})\n",
    "print(act)\n",
    "observation, reward, done, info = env.step(act[0])\n",
    "print(observation)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "mini batch results for run 1 : -263.510529\n",
      "mini batch results for run 2 : -237.803700\n",
      "mini batch results for run 3 : -227.417628\n",
      "mini batch results for run 4 : -183.300298\n",
      "mini batch results for run 5 : -190.768314\n",
      "mini batch results for run 6 : -194.500362\n",
      "mini batch results for run 7 : -230.957407\n",
      "mini batch results for run 8 : -234.918825\n",
      "mini batch results for run 9 : -175.463493\n",
      "mini batch results for run 10 : -163.511177\n",
      "mini batch results for run 11 : -136.755237\n",
      "mini batch results for run 12 : -129.672726\n",
      "mini batch results for run 13 : -97.722849\n",
      "mini batch results for run 14 : -91.986897\n",
      "mini batch results for run 15 : -76.344146\n",
      "mini batch results for run 16 : -25.674561\n",
      "mini batch results for run 17 : -4.345082\n",
      "mini batch results for run 18 : 30.213323\n",
      "mini batch results for run 19 : 42.094862\n",
      "mini batch results for run 20 : 52.546219\n",
      "mini batch results for run 21 : 70.615524\n",
      "mini batch results for run 22 : 110.244219\n",
      "mini batch results for run 23 : 105.450442\n",
      "mini batch results for run 24 : 112.641179\n",
      "mini batch results for run 25 : 116.645491\n",
      "mini batch results for run 26 : 113.284566\n",
      "mini batch results for run 27 : 118.930982\n",
      "mini batch results for run 28 : 130.032485\n",
      "mini batch results for run 29 : 137.445088\n",
      "mini batch results for run 30 : 129.220185\n",
      "mini batch results for run 31 : 150.711991\n",
      "mini batch results for run 32 : 138.436894\n",
      "mini batch results for run 33 : 165.103201\n",
      "mini batch results for run 34 : 152.105989\n",
      "mini batch results for run 35 : 152.686879\n",
      "mini batch results for run 36 : 143.038056\n",
      "mini batch results for run 37 : 176.964384\n",
      "mini batch results for run 38 : 154.661642\n",
      "mini batch results for run 39 : 160.724014\n",
      "mini batch results for run 40 : 153.829006\n",
      "mini batch results for run 41 : 157.539301\n",
      "mini batch results for run 42 : 166.447294\n",
      "mini batch results for run 43 : 130.974481\n",
      "mini batch results for run 44 : 133.518639\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-0dcdf7f9c5df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mreward_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyLinearImpulse\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mox\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0moy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpulse_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mBeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mcontactListener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mBeginContact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "\n",
    "render = False\n",
    "\n",
    "i = 0\n",
    "max_t = 1000\n",
    "\n",
    "# sess.run(.alpha.assign(0.01))\n",
    "\n",
    "while i < 1000:\n",
    "    sess.run(es.copy_state_ops)\n",
    "    sess.run(es.new_noise_vals_ops)\n",
    "\n",
    "    rewards_t = np.zeros([es.mini_batch_size])\n",
    "    obs_std = np.zeros([es.mini_batch_size,env.observation_space.shape[0]])\n",
    "    \n",
    "    for j in range(es.mini_batch_size):\n",
    "        observation = env.reset()\n",
    "        obs_t = np.array([observation])\n",
    "        sess.run(es.set_noise_ops, feed_dict={es.current_ind : j})\n",
    "        reward_t = 0\n",
    "        for t in range(max_t):\n",
    "            if render:\n",
    "                env.render()\n",
    "            obs_t = np.append(obs_t,[observation],axis = 0)\n",
    "            \n",
    "            act = sess.run(action, feed_dict={X: np.expand_dims(observation,axis=0)})[0]\n",
    "            observation, reward, done, info = env.step(act)\n",
    "            reward_t += reward\n",
    "            if done or (t == max_t-1):\n",
    "                rewards_t[j] = reward_t\n",
    "                break\n",
    "    print('mini batch results for run %d : %f' % (i + 1,np.mean(rewards_t)))\n",
    "    \n",
    "    es.calc_update_fitness_shaping(rewards_t)\n",
    "    sess.run(es.reset_state_ops)\n",
    "    sess.run(es.calc_gradient_ops,feed_dict={es.update_weights : es.calc_weights})\n",
    "    sess.run(optimizer.update)\n",
    "    i += 1\n",
    "                \n",
    "            \n",
    "# saver.save(sess, \"./my_policy_net_basic.ckpt\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_policy_net_basic.ckpt'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saver.save(sess, \"./my_policy_net_basic.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "render() got an unexpected keyword argument 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-2e4b3f78e54d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewardsum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: render() got an unexpected keyword argument 'close'"
     ]
    }
   ],
   "source": [
    "rewardsum = 0\n",
    "env = gym.make(env_name)\n",
    "obs = env.reset()\n",
    "for step in range(1000):\n",
    "    env.render()\n",
    "    action_val = action.eval(feed_dict={X: obs.reshape(1, n_inputs)})\n",
    "    obs, reward, done, info = env.step(action_val[0])\n",
    "    rewardsum += reward\n",
    "    if done:\n",
    "        break\n",
    "env.render(close=True)\n",
    "env.close()\n",
    "print(rewardsum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
