{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution strategy\n",
    "\n",
    "Here for funsies I try to understand the Evolution Strategy implementation from https://arxiv.org/pdf/1703.03864.pdf\n",
    "\n",
    "I implemented a simple serial version of the code, along with a basic Adam optimizer to use the extracted gradient estimate for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "  \n",
    "from tensorflow.python.ops import variables\n",
    "from tensorflow.python.framework import ops\n",
    "from scipy.stats import rankdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Setup the gym environment!\n",
    "import gym\n",
    "env_name = 'BipedalWalker-v2'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adam Optimizer https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "# To do --> learn more\n",
    "\n",
    "# Recipe:\n",
    "# t <- t + 1\n",
    "# lr_t <- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)\n",
    "\n",
    "# m_t <- beta1 * m_{t-1} + (1 - beta1) * g\n",
    "# v_t <- beta2 * v_{t-1} + (1 - beta2) * g * g\n",
    "# variable <- variable - lr_t * m_t / (sqrt(v_t) + epsilon)\n",
    "\n",
    "class Simple_Optimizer(object):\n",
    "    def __init__(self, variables,grad_vars, alpha = 1e-3,beta1 = 1e-1):\n",
    "        self.a = tf.Variable(alpha,trainable=False)\n",
    "        self.b1 = tf.constant(beta1)\n",
    "        \n",
    "        self.vars = variables\n",
    "        self.grad_vars = grad_vars\n",
    "        \n",
    "        self.create_update_ops()\n",
    "        \n",
    "    def create_update_ops(self):\n",
    "        ops = []\n",
    "        for var,g in zip(self.vars, self.grad_vars):\n",
    "            ops.append(var.assign_add(self.a * g))\n",
    "        self.update = tf.group(*ops)\n",
    "\n",
    "    \n",
    "class Momentum_Optimizer(object):\n",
    "    def __init__(self, variables,grad_vars, alpha = 1e-3,beta1 = 0.9):\n",
    "        self.a = tf.Variable(alpha,trainable=False)\n",
    "        self.b1 = tf.constant(beta1)\n",
    "        \n",
    "        self.vars = variables\n",
    "        self.grad_vars = grad_vars\n",
    "        \n",
    "        self.m = []\n",
    "        for var in self.vars:\n",
    "            self.m.append(tf.Variable(tf.zeros(var.get_shape()),trainable=False))\n",
    "        \n",
    "        self.create_update_ops()\n",
    "        \n",
    "    def create_update_ops(self):\n",
    "        ops = []\n",
    "        for var,g,m in zip(self.vars, self.grad_vars, self.m):\n",
    "            m_op = m.assign(self.b1 * m + (1 - self.b1) * g)\n",
    "            with tf.get_default_graph().control_dependencies([m_op]): # Ensure m runs first\n",
    "                var_op = var.assign_add(self.a * m)\n",
    "            ops += [m_op,var_op]\n",
    "        self.update = tf.group(*ops)\n",
    "    \n",
    "class Adam_Optimizer(object):\n",
    "    def __init__(self, variables,grad_vars, alpha = 1e-3,beta1 = 0.9, beta2 = 0.999, eps = 1e-8):\n",
    "        self.t = tf.Variable(0.0,trainable=False)\n",
    "        self.a = tf.Variable(alpha,trainable=False)\n",
    "        self.b1 = tf.constant(beta1)\n",
    "        self.b2 = tf.constant(beta2)\n",
    "        self.eps = tf.constant(eps)\n",
    "        self.vars = variables\n",
    "        self.grad_vars = grad_vars\n",
    "        \n",
    "        self.m = []\n",
    "        self.v = []\n",
    "        for var in self.vars:\n",
    "            self.m.append(tf.Variable(tf.zeros(var.get_shape()),trainable=False))\n",
    "            self.v.append(tf.Variable(tf.zeros(var.get_shape()),trainable=False))\n",
    "        \n",
    "        self.create_update_ops()\n",
    "        \n",
    "    def create_update_ops(self):\n",
    "        t_op = self.t.assign_add(1.0)\n",
    "        with tf.get_default_graph().control_dependencies([t_op]): # Ensure t runs first\n",
    "            a_op = self.a.assign(self.a * tf.sqrt(1- tf.pow(self.b2,self.t)) / (1 - tf.pow(self.b1,self.t)))   \n",
    "        ops = [a_op,t_op]\n",
    "        \n",
    "        for var,g,m,v in zip(self.vars, self.grad_vars, self.m, self.v):\n",
    "            m_op = m.assign(self.b1 * m + (1 - self.b1) * g)\n",
    "            v_op = v.assign(self.b2 * v + (1 - self.b2) * tf.square(g))\n",
    "            with tf.get_default_graph().control_dependencies([m_op,v_op,a_op]): # Ensure m,v,a runs first\n",
    "                var_op = var.assign_add(self.a * m / (tf.sqrt(v) + self.eps))\n",
    "            ops += [m_op,v_op,var_op]\n",
    "        self.update = tf.group(*ops)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution strategy gradient estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rescale_to_normal(array):\n",
    "    # Helper function\n",
    "    return (array - np.mean(array))/ np.std(array)\n",
    "\n",
    "\n",
    "class ES_gradient_estimator(object):\n",
    "    def __init__(self, sigma = 0.01,weight_decay=0.005, mini_batch_size = 30):\n",
    "        self.sigma = tf.constant(np.float(sigma)) # Standard deviation of weight adjustments\n",
    "        self.weight_decay = tf.constant(weight_decay)\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        \n",
    "        self.current_ind = tf.placeholder(\"int32\")\n",
    "        self.update_weights = tf.placeholder(\"float32\",shape = [mini_batch_size])\n",
    "        \n",
    "        # Used for fitness shaping http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf\n",
    "        self.fitness_shape_rank_vals = [np.max([0,np.log(self.mini_batch_size/2.0+1)-np.log(ind)]) for ind in np.arange(1,(self.mini_batch_size+1))]\n",
    "        self.fitness_shape_rank_vals = self.fitness_shape_rank_vals/np.sum(self.fitness_shape_rank_vals)\n",
    "#         self.fitness_shape_rank_vals = -(1.0/self.mini_batch_size)*(np.arange(0,self.mini_batch_size)/(self.mini_batch_size-1.0) - 0.5)\n",
    "        \n",
    "        # Get the weights and biases from TF\n",
    "        self.set_links_to_vars()\n",
    "\n",
    "    def set_links_to_vars(self):\n",
    "        # Have to pull out the relevant variables from TF, make the operations necessary to modify them efficiently\n",
    "        self.noise_vars = []\n",
    "        self.grad_vars = []\n",
    "        \n",
    "        self.new_noise_vals_ops = []\n",
    "        self.set_noise_ops = []\n",
    "        self.copy_state_ops = []\n",
    "        self.reset_state_ops = []\n",
    "        self.calc_gradient_ops = []\n",
    "        \n",
    "        self.vars = variables.trainable_variables() + ops.get_collection(ops.GraphKeys.TRAINABLE_RESOURCE_VARIABLES)\n",
    "        for var in self.vars:\n",
    "            # Mirrored sampling! \n",
    "            tmp_noise_var = tf.random_normal([np.int(self.mini_batch_size/2)] + var.get_shape().as_list())\n",
    "            noise_var = tf.concat([tmp_noise_var,-tmp_noise_var],0)\n",
    "            \n",
    "            current_noise_vals = tf.Variable(tf.zeros(noise_var.get_shape()),trainable=False)\n",
    "            # Only want to make random noise variable change after each round, so have to do this fudge\n",
    "            self.new_noise_vals_ops.append(current_noise_vals.assign(noise_var))\n",
    "            self.noise_vars.append(current_noise_vals)\n",
    "            copy_var = tf.Variable(var.initialized_value(),trainable=False)\n",
    "            \n",
    "            self.set_noise_ops.append(var.assign(copy_var + self.sigma * current_noise_vals[self.current_ind]))\n",
    "            self.copy_state_ops.append(copy_var.assign(var))\n",
    "            self.reset_state_ops.append(var.assign(copy_var))\n",
    "            \n",
    "            grad_var = tf.Variable(tf.zeros(var.get_shape()),trainable=False)\n",
    "            self.grad_vars.append(grad_var)\n",
    "            self.calc_gradient_ops.append(grad_var.assign(tf.tensordot(self.update_weights, current_noise_vals,axes=[0,0]) - self.weight_decay * var)) # Note weight decay\n",
    "            \n",
    "    # Three different flavours of update mechanism. In practice only using the fitness shaping version\n",
    "    def calc_update_based_on_reward(self,rewards):\n",
    "        \n",
    "        self.calc_weights = 1.0/(self.mini_batch_size * self.sigma.eval()) * rescale_to_normal(rewards)\n",
    "    \n",
    "    def calc_update_based_on_highest_reward(self,rewards):\n",
    "        \n",
    "        self.calc_weights = self.sigma.eval()*np.array([(1.0 if val == np.max(rewards) else 0) for val in np.array(rewards)])\n",
    "    \n",
    "    def calc_update_fitness_shaping(self,rewards):\n",
    "        \n",
    "        k = self.mini_batch_size - rankdata(rewards_t, method='ordinal')\n",
    "        self.calc_weights = self.fitness_shape_rank_vals[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # THIS IS NECESSARY BEFORE MAKING NEW SESSION TO STOP IT ERRORING!!\n",
    "try:\n",
    "    sess\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    sess.close()\n",
    "    del sess\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Borrowed some bits from http://mat.univie.ac.at/~grohs/tmp/DeepLearningClass_Jun28_1.html\n",
    "n_inputs = env.observation_space.shape[0]\n",
    "n_hidden = 100  \n",
    "n_hlayers = 2\n",
    "n_outputs = env.action_space.shape[0]\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "# 2. Build the neural network\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "Y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "layer = X\n",
    "for _ in range(n_hlayers):\n",
    "    layer = tf.layers.dense(layer, n_hidden, activation=tf.nn.relu,\n",
    "                         kernel_initializer=initializer)\n",
    "\n",
    "raw_action = tf.layers.dense(layer, n_outputs,\n",
    "                          kernel_initializer=initializer)\n",
    "action = tf.clip_by_value(raw_action,tf.expand_dims(env.action_space.low,0),tf.expand_dims(env.action_space.high,0)) # Have to clip the action space. This might be a bad idea\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "es = ES_gradient_estimator(sigma = 0.05,mini_batch_size=200)\n",
    "\n",
    "# optimizer = Adam_Optimizer(es.vars,es.grad_vars,alpha=0.03,beta1=0.7,beta2=0.99)\n",
    "# optimizer = Simple_Optimizer(es.vars,es.grad_vars,alpha=0.1)\n",
    "optimizer = Momentum_Optimizer(es.vars,es.grad_vars,alpha=0.2,beta1=0.7)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.37650865  0.69396424  0.18345767]]\n",
      "[-0.02121598 -0.03145527 -0.02868943 -0.01274514  0.47453269  1.00028253\n",
      "  0.08405423 -1.00017548  1.          0.37595776  0.90991032  0.08919561\n",
      " -0.88032214  1.          0.4465037   0.45157441  0.46737847  0.49586892\n",
      "  0.54099655  0.61023712  0.71830201  0.89736676  1.          1.        ]\n",
      "-0.23493897485857443\n"
     ]
    }
   ],
   "source": [
    "# Some idiot checking here for when running new environments\n",
    "observation = env.reset()\n",
    "act = sess.run(action, feed_dict={X: np.expand_dims(observation,axis=0)})\n",
    "print(act)\n",
    "observation, reward, done, info = env.step(act[0])\n",
    "print(observation)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "mini batch results for run 1 : -107.273033\n",
      "mini batch results for run 2 : -93.529358\n",
      "mini batch results for run 3 : -78.803114\n",
      "mini batch results for run 4 : -70.769058\n",
      "mini batch results for run 5 : -73.679605\n",
      "mini batch results for run 6 : -69.722070\n",
      "mini batch results for run 7 : -66.959893\n",
      "mini batch results for run 8 : -78.672353\n",
      "mini batch results for run 9 : -84.937350\n",
      "mini batch results for run 10 : -86.326880\n",
      "mini batch results for run 11 : -74.901914\n",
      "mini batch results for run 12 : -68.593762\n",
      "mini batch results for run 13 : -65.427668\n",
      "mini batch results for run 14 : -64.081339\n",
      "mini batch results for run 15 : -65.264838\n",
      "mini batch results for run 16 : -52.673349\n",
      "mini batch results for run 17 : -65.465931\n",
      "mini batch results for run 18 : -58.385733\n",
      "mini batch results for run 19 : -55.863099\n",
      "mini batch results for run 20 : -50.237778\n",
      "mini batch results for run 21 : -53.982290\n",
      "mini batch results for run 22 : -36.926525\n",
      "mini batch results for run 23 : -25.738794\n",
      "mini batch results for run 24 : -10.707543\n",
      "mini batch results for run 25 : -28.935639\n",
      "mini batch results for run 26 : -15.130893\n",
      "mini batch results for run 27 : -4.373729\n",
      "mini batch results for run 28 : -12.580869\n",
      "mini batch results for run 29 : 6.200798\n",
      "mini batch results for run 30 : 9.790987\n",
      "mini batch results for run 31 : 19.089110\n",
      "mini batch results for run 32 : 5.466720\n",
      "mini batch results for run 33 : 17.838152\n",
      "mini batch results for run 34 : 48.292865\n",
      "mini batch results for run 35 : 21.225223\n",
      "mini batch results for run 36 : 9.726920\n",
      "mini batch results for run 37 : 27.990994\n",
      "mini batch results for run 38 : 27.684341\n",
      "mini batch results for run 39 : 47.789134\n",
      "mini batch results for run 40 : 25.146896\n",
      "mini batch results for run 41 : 56.343384\n",
      "mini batch results for run 42 : 47.332855\n",
      "mini batch results for run 43 : 34.100050\n",
      "mini batch results for run 44 : 43.057469\n",
      "mini batch results for run 45 : 53.178239\n",
      "mini batch results for run 46 : 48.130805\n",
      "mini batch results for run 47 : 45.383515\n",
      "mini batch results for run 48 : 48.651926\n",
      "mini batch results for run 49 : 52.416839\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-44a934c2892d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mobs_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mreward_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1293\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mfeeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m       \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m       \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1293\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mfeeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m       \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m       \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "\n",
    "render = False\n",
    "\n",
    "i = 0\n",
    "max_t = 1000\n",
    "\n",
    "sess.run(optimizer.a.assign(0.15))\n",
    "\n",
    "while i < 1000:\n",
    "    sess.run(es.copy_state_ops)\n",
    "    sess.run(es.new_noise_vals_ops)\n",
    "\n",
    "    rewards_t = np.zeros([es.mini_batch_size])\n",
    "    obs_std = np.zeros([es.mini_batch_size,env.observation_space.shape[0]])\n",
    "    \n",
    "    for j in range(es.mini_batch_size):\n",
    "        observation = env.reset()\n",
    "        obs_t = np.array([observation])\n",
    "        sess.run(es.set_noise_ops, feed_dict={es.current_ind : j})\n",
    "        reward_t = 0\n",
    "        for t in range(max_t):\n",
    "            if render:\n",
    "                env.render()\n",
    "            obs_t = np.append(obs_t,[observation],axis = 0)\n",
    "            \n",
    "            act = sess.run(action, feed_dict={X: np.expand_dims(observation,axis=0)})[0]\n",
    "            observation, reward, done, info = env.step(act)\n",
    "            reward_t += reward\n",
    "            if done or (t == max_t-1):\n",
    "                rewards_t[j] = reward_t\n",
    "                break\n",
    "    print('mini batch results for run %d : %f' % (i + 1,np.mean(rewards_t)))\n",
    "    \n",
    "    es.calc_update_fitness_shaping(rewards_t)\n",
    "    sess.run(es.reset_state_ops)\n",
    "    sess.run(es.calc_gradient_ops,feed_dict={es.update_weights : es.calc_weights})\n",
    "    sess.run(optimizer.update)\n",
    "    i += 1\n",
    "                \n",
    "            \n",
    "# saver.save(sess, \"./my_policy_net_basic.ckpt\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_policy_net_basic.ckpt'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saver.save(sess, \"./my_policy_net_basic.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "130.21838680337558\n"
     ]
    }
   ],
   "source": [
    "rewardsum = 0\n",
    "env = gym.make(env_name)\n",
    "obs = env.reset()\n",
    "for step in range(1000):\n",
    "    env.render()\n",
    "    action_val = action.eval(feed_dict={X: obs.reshape(1, n_inputs)})\n",
    "    obs, reward, done, info = env.step(action_val[0])\n",
    "    rewardsum += reward\n",
    "    if done:\n",
    "        break\n",
    "env.close()\n",
    "print(rewardsum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
