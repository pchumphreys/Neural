{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "% autoreload 2\n",
    "import setup_deep_q_expm\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expm_params import expm_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "expm_name = 'CartPole-v1'\n",
    "\n",
    "log,writer,merged,sampler,algo,qnet,policy,run_params = setup_deep_q_expm.setup_expm(expm_name = expm_name,params = expm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': {'discount': 0.95, 'lr': 0.001, 'tau': 0.001},\n",
       " 'base': {'env_name': 'CartPole-v1',\n",
       "  'epoch_length': 400,\n",
       "  'grad_steps_per_t': 1,\n",
       "  'max_episode_length': -1,\n",
       "  'max_epochs': 5000,\n",
       "  'online_training': False},\n",
       " 'nnet': {'n_hidden': 24, 'n_layers': 2},\n",
       " 'policy': {'epsilon_decay': 0.99,\n",
       "  'epsilon_end': 0.01,\n",
       "  'epsilon_start': 1.0,\n",
       "  'reward_scale': 1,\n",
       "  'scheme': 'Epsilon'},\n",
       " 'replay_buffer': {'batch_size': 32,\n",
       "  'max_buffer_size': 2000,\n",
       "  'min_pool_size': 32}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expm_params[expm_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "238.99481\n",
      "[-148.90883 -104.28388]\n",
      "48.83081581771373\n",
      "Epoch 0, mean_reward 10\n",
      "-----\n",
      "2281.8364\n",
      "[-3133.4287 -2170.8362]\n",
      "1206.6281198883046\n",
      "Epoch 1, mean_reward 9\n",
      "-----\n",
      "9020.956\n",
      "[-7895.4834 -5331.7456]\n",
      "5513.362417602539\n",
      "Epoch 2, mean_reward 9\n",
      "-----\n",
      "19511.174\n",
      "[-17726.541 -11747.774]\n",
      "14236.248415527343\n",
      "Epoch 3, mean_reward 9\n",
      "-----\n",
      "36914.543\n",
      "[-54999.562 -35546.   ]\n",
      "27535.736103515625\n",
      "Epoch 4, mean_reward 9\n",
      "-----\n",
      "59962.383\n",
      "[-67423.92  -39435.918]\n",
      "46811.96973632815\n",
      "Epoch 5, mean_reward 9\n",
      "-----\n",
      "69085.8\n",
      "[-117652.33  -62713.45]\n",
      "70933.10579101561\n",
      "Epoch 6, mean_reward 9\n",
      "-----\n",
      "92643.836\n",
      "[-168463.69   -84549.875]\n",
      "101252.9239453125\n",
      "Epoch 7, mean_reward 9\n",
      "-----\n",
      "112054.82\n",
      "[-182398.31  -86550.45]\n",
      "136835.39191406255\n",
      "Epoch 8, mean_reward 9\n",
      "-----\n",
      "108720.56\n",
      "[-162322.28  -73668.98]\n",
      "177529.65681640626\n",
      "Epoch 9, mean_reward 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-9525deea0a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_ready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrun_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'online_training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrun_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'online_training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repositories/Neural/Soft Actor Critic/sampler.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscrete\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplaybuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_obs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repositories/Neural/Soft Actor Critic/replay_buffer.py\u001b[0m in \u001b[0;36madd_sample\u001b[0;34m(self, action, obs, next_obs, reward, done)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_observations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(run_params['max_epochs']):\n",
    "    sampler.reset()\n",
    "    epoch_avg_losses = 0\n",
    "\n",
    "    for t in range(run_params['epoch_length']):\n",
    "        sampler.sample()\n",
    "        if sampler.batch_ready() or run_params['online_training']:\n",
    "            if run_params['online_training']:\n",
    "                samples = sampler.get_last_sample()\n",
    "                summary,losses,qnet_o = algo.train(samples,merged,algo.Q_Loss,qnet.output) \n",
    "            else:\n",
    "                for j in range(run_params['grad_steps_per_t']):\n",
    "                    samples = sampler.get_samples()\n",
    "                    \n",
    "                    qnet_next_obs = qnet.output.eval(feed_dict = {qnet.obs : samples['next_observations']})\n",
    "                    pred = np.argmax(qnet_next_obs,axis=1)\n",
    "                    one_hot = np.eye(qnet.output_size)[pred]\n",
    "                    tqnet_output = algo.target_Q_outputs.eval(feed_dict = {algo.next_obs : samples['next_observations']})\n",
    "#                      np.sum(one_hot*tqnet_output,axis=1)\n",
    "                    doubleQ = tqnet_output[range(expm_params[expm_name]['replay_buffer']['batch_size']),pred]\n",
    "                    \n",
    "                    qt = samples['rewards'] + algo.discount * (1- samples['dones']) *doubleQ\n",
    "                    feed_dict = algo._construct_feed_dict(samples)\n",
    "                    feed_dict[algo.Q_t] = qt\n",
    "                          \n",
    "                    _,summary,losses,qnet_o = tf.get_default_session().run([algo.train_ops,merged,algo.Q_Loss,qnet.output], feed_dict = feed_dict)\n",
    "\n",
    "#                     summary,losses,qnet_o= algo.train(samples,merged,algo.Q_Loss,qnet.output) \n",
    "            epoch_avg_losses = (epoch_avg_losses*(t) + np.array(losses))/(t+1)\n",
    "    print('-----')\n",
    "    print(np.sqrt(np.mean((qnet_next_obs[:10]-tqnet_output[:10])**2)))\n",
    "                    \n",
    "    print(qnet_o[0])\n",
    "    log.record('mean_episode_reward',sampler.mean_episode_reward)\n",
    "    writer.add_summary(summary, i)\n",
    "    print(epoch_avg_losses)\n",
    "\n",
    "    writer.flush()\n",
    "\n",
    "    print('Epoch %i, mean_reward %d' % (i, sampler.mean_episode_reward))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
