{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Finished episode 100 after 50 timesteps (reward: 50.0)\n",
      "Finished episode 200 after 59 timesteps (reward: 59.0)\n",
      "Finished episode 300 after 200 timesteps (reward: 200.0)\n",
      "Finished episode 400 after 85 timesteps (reward: 85.0)\n",
      "Finished episode 500 after 26 timesteps (reward: 26.0)\n",
      "Finished episode 600 after 17 timesteps (reward: 17.0)\n",
      "Finished episode 700 after 12 timesteps (reward: 12.0)\n",
      "Finished episode 800 after 9 timesteps (reward: 9.0)\n",
      "Finished episode 900 after 10 timesteps (reward: 10.0)\n",
      "Finished episode 1000 after 9 timesteps (reward: 9.0)\n",
      "Finished episode 1100 after 12 timesteps (reward: 12.0)\n",
      "Finished episode 1200 after 10 timesteps (reward: 10.0)\n",
      "Finished episode 1300 after 10 timesteps (reward: 10.0)\n",
      "Finished episode 1400 after 9 timesteps (reward: 9.0)\n",
      "Finished episode 1500 after 11 timesteps (reward: 11.0)\n",
      "Finished episode 1600 after 9 timesteps (reward: 9.0)\n",
      "Finished episode 1700 after 12 timesteps (reward: 12.0)\n",
      "Finished episode 1800 after 9 timesteps (reward: 9.0)\n",
      "Finished episode 1900 after 9 timesteps (reward: 9.0)\n",
      "Finished episode 2000 after 12 timesteps (reward: 12.0)\n",
      "Finished episode 2100 after 10 timesteps (reward: 10.0)\n",
      "Finished episode 2200 after 10 timesteps (reward: 10.0)\n",
      "Finished episode 2300 after 9 timesteps (reward: 9.0)\n",
      "Finished episode 2400 after 11 timesteps (reward: 11.0)\n",
      "Finished episode 2500 after 8 timesteps (reward: 8.0)\n",
      "Finished episode 2600 after 10 timesteps (reward: 10.0)\n",
      "Finished episode 2700 after 9 timesteps (reward: 9.0)\n",
      "Finished episode 2800 after 11 timesteps (reward: 11.0)\n",
      "Finished episode 2900 after 12 timesteps (reward: 12.0)\n",
      "Finished episode 3000 after 9 timesteps (reward: 9.0)\n",
      "Finished episode 3100 after 10 timesteps (reward: 10.0)\n",
      "Finished episode 3200 after 9 timesteps (reward: 9.0)\n",
      "Finished episode 3300 after 9 timesteps (reward: 9.0)\n",
      "Finished episode 3400 after 10 timesteps (reward: 10.0)\n",
      "Finished episode 3500 after 11 timesteps (reward: 11.0)\n",
      "Finished episode 3600 after 10 timesteps (reward: 10.0)\n",
      "Finished episode 3700 after 11 timesteps (reward: 11.0)\n",
      "Finished episode 3800 after 10 timesteps (reward: 10.0)\n",
      "Finished episode 3900 after 11 timesteps (reward: 11.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorforce.agents import *\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "\n",
    "# Create an OpenAIgym environment.\n",
    "environment = OpenAIGym('CartPole-v0', visualize=False)\n",
    "\n",
    "# Network as list of layers\n",
    "# - Embedding layer:\n",
    "#   - For Gym environments utilizing a discrete observation space, an\n",
    "#     \"embedding\" layer should be inserted at the head of the network spec.\n",
    "#     Such environments are usually identified by either:\n",
    "#     - class ...Env(discrete.DiscreteEnv):\n",
    "#     - self.observation_space = spaces.Discrete(...)\n",
    "\n",
    "# Note that depending on the following layers used, the embedding layer *may* need a\n",
    "# flattening layer\n",
    "\n",
    "network_spec = [\n",
    "    # dict(type='embedding', indices=100, size=32),\n",
    "    # dict(type'flatten'),\n",
    "    dict(type='dense', size=24),\n",
    "    dict(type='dense', size=24)\n",
    "]\n",
    "\n",
    "actions_exploration_spec=dict(\n",
    "            type=\"epsilon_decay\",\n",
    "            initial_epsilon=1.0,\n",
    "            final_epsilon=0.1,\n",
    "            timesteps=1000\n",
    "        )\n",
    "        \n",
    "\n",
    "memory=dict(\n",
    "            type='replay',\n",
    "            include_next_states=True,\n",
    "            capacity=32\n",
    "        )\n",
    "\n",
    "optimizer=dict(\n",
    "            type='adam',\n",
    "            learning_rate=1e-3\n",
    "        )\n",
    "\n",
    "agent = DQNAgent(\n",
    "    states=environment.states,\n",
    "    actions=environment.actions,\n",
    "    network=network_spec,\n",
    "    memory = memory,\n",
    "    optimizer = optimizer,\n",
    "    actions_exploration = actions_exploration_spec)\n",
    "\n",
    "\n",
    "# Create the runner\n",
    "runner = Runner(agent=agent, environment=environment)\n",
    "\n",
    "\n",
    "# Callback function printing episode statistics\n",
    "def episode_finished(r):\n",
    "    if r.episode%100 == 0:\n",
    "        print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "                                                                                 reward=r.episode_rewards[-1]))\n",
    "    return True\n",
    "\n",
    "\n",
    "# Start learning\n",
    "runner.run(episodes=30000, max_episode_timesteps=200, episode_finished=episode_finished)\n",
    "runner.close()\n",
    "\n",
    "# Print statistics\n",
    "print(\"Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.\".format(\n",
    "    ep=runner.episode,\n",
    "    ar=np.mean(runner.episode_rewards[-100:]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
