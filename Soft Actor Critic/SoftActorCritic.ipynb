{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Model - this makes the network (ie the policy)\n",
    "\n",
    "#we have the soft Q function, the parameterised value function V, and the parameterised tractable policy pi\n",
    "\n",
    "# TODOs\n",
    "# Make comments properly\n",
    "\n",
    "# Make run time routine\n",
    "# Whitening of inputs\n",
    "\n",
    "# Need batching\n",
    "\n",
    "# Need recall\n",
    "\n",
    "# Train, test routines\n",
    "\n",
    "\n",
    "# Here is a helper class to make a simple neural network. Importantly, it allows us to easily get the parameters, and hopefully to link the inputs to other variables\n",
    "# The get_output functionality is borrowed from the SAC reference code.\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self,name,inputs,output_size,n_hidden,n_layers):\n",
    "        self._name = name\n",
    "        self.inputs = inputs\n",
    "        self.output_size = output_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.output = self.make_network(reuse = False)\n",
    "        \n",
    "    def make_network(self,inputs = False,reuse = tf.AUTO_REUSE):\n",
    "        if inputs is False :\n",
    "            inputs = self.inputs\n",
    "            \n",
    "        with tf.variable_scope(self._name,reuse = reuse):\n",
    "            if not(isinstance(inputs,tf.Tensor)):  \n",
    "                inputs = tf.concat(inputs,axis=1)\n",
    "\n",
    "            # To do understand weight initialization!   \n",
    "            self.hidden = slim.stack(inputs, slim.fully_connected, [self.n_hidden]*self.n_layers, scope='fc',activation_fn=tf.nn.relu)\n",
    "            outputs = slim.fully_connected(self.hidden,self.output_size)\n",
    "        return outputs\n",
    "\n",
    "    def get_params_internal(self):\n",
    "\n",
    "        scope = tf.get_variable_scope().name\n",
    "        scope += '/' + self._name + '/' if len(scope) else self._name + '/'\n",
    "\n",
    "        return tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope\n",
    "        )\n",
    "\n",
    "class Qnet(MLP):\n",
    "    def __init__(self,actions,obs,n_hidden,n_layers):\n",
    "        # Ok so Q function takes s,a, gives Q   \n",
    "        self.actions = actions\n",
    "        self.obs = obs\n",
    "        \n",
    "        # Super is used to call the init method of the parent class\n",
    "        super(Qnet,self).__init__('qNet',(self.actions,self.obs),1,n_hidden,n_layers)\n",
    "\n",
    "\n",
    "class Vnet(MLP):\n",
    "    def __init__(self,obs,n_hidden,n_layers):\n",
    "        # Ok so V function takes s, gives V\n",
    "        self.obs = obs\n",
    "        super(Vnet,self).__init__('vNet',(self.obs),1,n_hidden,n_layers)\n",
    "\n",
    "class Policy_Discrete(MLP):\n",
    "    # P function takes s, needs to be able to give actions.\n",
    "    # For now assume that discrete action space, such that tractable - obviously this slightly defeats the point of SAC implementation since all about how hard to compute the partition function\n",
    "    def __init__(self,action_size,obs,n_hidden,n_layers):\n",
    "\n",
    "        self.obs = obs\n",
    "        \n",
    "        super(Policy_Discrete,self).__init__('policy',(self.obs),action_size,n_hidden,n_layers)\n",
    "        self.make_policy_outputs(reuse=False)\n",
    "        \n",
    "    def make_policy_outputs(self, reuse = tf.AUTO_REUSE):\n",
    "       \n",
    "        with tf.variable_scope(self._name + '_outs',reuse = reuse):\n",
    "            self.policy_output = tf.nn.softmax(self.output) # Automatically sum to one.\n",
    "            self.log_policy_output = tf.log(self.policy_output)\n",
    "            self.action = tf.multinomial(self.log_policy_output, num_samples=1)[0] # Will generate an action\n",
    "\n",
    "\n",
    "    def get_action(self,obs):\n",
    "        return self.action.eval(feed_dict = {self.obs : [obs]})[0]\n",
    "    \n",
    "\n",
    "#Ok, so pass the Pnet, Qnet, Vnet\n",
    "\n",
    "class Soft_Actor_Critic():\n",
    "    def __init__(self,Qnet,Vnet,Policy,actions,obs,next_obs,rewards,dones,lr=1e-3,discount = 0.99, tau=0.99):\n",
    "        self.lr = lr\n",
    "        self.discount = discount\n",
    "        self.tau = tau \n",
    "        \n",
    "        # Maybe would be nicer to not pass these but define here, but this seems to be messy\n",
    "        self.actions = actions\n",
    "        self.obs = obs\n",
    "        self.next_obs = next_obs\n",
    "        self.rewards = rewards\n",
    "        self.dones = dones\n",
    "        \n",
    "        self.Qnet = Qnet\n",
    "        self.Vnet = Vnet\n",
    "        self.Policy = Policy\n",
    "        \n",
    "        self.Qs = self.Qnet.output\n",
    "        self.Vs = self.Vnet.output\n",
    "        self.policy_log_a = self.Policy.log_policy_output\n",
    "        \n",
    "        # Duplicate v network for target. Need to add training for this\n",
    "        with tf.variable_scope('vNet_T'):\n",
    "            self.target_Vs = self.Vnet.make_network(inputs = self.next_obs,reuse=False)\n",
    "            self.target_V_params = self.Vnet.get_params_internal()\n",
    "            \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.lr)\n",
    "        \n",
    "        self.init_Q_net_training()\n",
    "        self.init_V_net_training()\n",
    "        self.init_Policy_training()\n",
    "        self.init_target_v_update()\n",
    "        \n",
    "        \n",
    "        self.train_ops = tf.group(self.train_Q,self.train_V,self.train_P,self.tvnet_update)\n",
    "        \n",
    "        \n",
    "    def init_Q_net_training(self):\n",
    "        training_variables = self.Qnet.get_params_internal()\n",
    "        with tf.variable_scope('Q_loss'):\n",
    "            Q_t = tf.stop_gradient(self.rewards + self.discount * (1-self.dones) * self.target_Vs)\n",
    "            self.Q_Loss = 0.5*tf.reduce_mean(tf.square(self.Qs - Q_t))\n",
    "            \n",
    "        self.train_Q = self.optimizer.minimize(self.Q_Loss,var_list = training_variables)\n",
    "\n",
    "    def init_V_net_training(self):\n",
    "        training_variables = self.Vnet.get_params_internal()\n",
    "\n",
    "        with tf.variable_scope('V_loss'):\n",
    "            V_t = tf.stop_gradient(self.Qs - self.policy_log_a) \n",
    "            self.V_Loss = 0.5*tf.reduce_mean(tf.square(self.Vs - V_t))\n",
    "\n",
    "        self.train_V = self.optimizer.minimize(self.V_Loss,var_list = training_variables)\n",
    "\n",
    "\n",
    "    def init_Policy_training(self):\n",
    "        training_variables = self.Policy.get_params_internal()\n",
    "\n",
    "        with tf.variable_scope('P_loss'):\n",
    "            P_t = tf.stop_gradient(self.Qs - self.Vs) \n",
    "            self.P_Loss = 0.5*tf.reduce_mean(tf.square(self.policy_log_a - P_t))\n",
    "            \n",
    "        self.train_P = self.optimizer.minimize(self.P_Loss,var_list = training_variables)\n",
    "\n",
    "        \n",
    "    def init_target_v_update(self):\n",
    "        vnet_params = self.Vnet.get_params_internal()\n",
    "        \n",
    "        with tf.variable_scope('Vt_loss'):\n",
    "            tvnet_update = []\n",
    "            for tv_p in self.target_V_params:\n",
    "                v_p = [v for v in vnet_params if tv_p.name[(tv_p.name.index('/')+1):] in v.name]\n",
    "                assert(len(v_p) == 1) # Check that only found one variable\n",
    "                v_p = v_p[0]\n",
    "                with tf.control_dependencies([self.train_V]):\n",
    "                    tvnet_update.append(tv_p.assign(self.tau * tv_p + (1-self.tau)*v_p))\n",
    "            self.tvnet_update = tf.group(tvnet_update)\n",
    "\n",
    "    def _construct_feed_dict(self,samples):  \n",
    "        return {self.actions : samples['actions'],\n",
    "                    self.obs : samples['observations'],\n",
    "                    self.next_obs : samples['next_observations'],\n",
    "                    self.dones : samples['dones'],\n",
    "                    self.rewards : samples['rewards']}\n",
    "                    \n",
    "    def train(self, samples):\n",
    "        feed_dict = self._construct_feed_dict(samples)\n",
    "        return tf.get_default_session().run(self.train_ops, feed_dict = feed_dict)\n",
    "\n",
    "class replayBuffer():\n",
    "    def __init__(self,n_inputs,n_outputs,max_buffer_size = 1e4):\n",
    "        self._max_size = int(max_buffer_size)\n",
    "        self._size = 0\n",
    "        self._pos = 0\n",
    "        \n",
    "        self.actions = np.zeros([self._max_size,n_outputs])\n",
    "        self.observations = np.zeros([self._max_size,n_inputs])\n",
    "        self.next_observations = np.zeros([self._max_size,n_inputs])\n",
    "        self.rewards = np.zeros(self._max_size)\n",
    "        self.dones = np.zeros(self._max_size)\n",
    "        \n",
    "    def add_sample(self,action,obs,next_obs,reward,done):\n",
    "        self.actions[self._pos] = action\n",
    "        self.observations[self._pos] = obs\n",
    "        self.next_observations[self._pos] = next_obs\n",
    "        self.rewards[self._pos] = reward\n",
    "        self.dones[self._pos] = done\n",
    "        \n",
    "        self._advance()\n",
    "    \n",
    "    def _advance(self):\n",
    "        self._pos = (self._pos + 1) % self._max_size\n",
    "        if self._size < self._max_size:\n",
    "            self._size += 1\n",
    "            \n",
    "    def get_samples(self,n_samples):\n",
    "        inds = np.random.randint(0,self._size,n_samples)\n",
    "        return dict(actions = self.actions[inds],\n",
    "                   observations = self.observations[inds],\n",
    "                   next_observations = self.next_observations[inds],\n",
    "                   rewards = self.rewards[inds],\n",
    "                   dones = self.dones[inds])\n",
    "    \n",
    "    def ready_to_sample(self,n_samples):\n",
    "        return self._size >= n_samples\n",
    "        \n",
    "class logger():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.vars = {}\n",
    "        \n",
    "    def record(self,var_name,value):\n",
    "        if hasattr(self.vars,var_name):\n",
    "            self.vars[var_name].append(value)\n",
    "        else:\n",
    "            self.vars[var_name] = [value]\n",
    "    \n",
    "    def get(self,var_name):\n",
    "        if hasattr(self.vars,var_name):\n",
    "            return self.vars[var_name]\n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "log = logger() \n",
    "\n",
    "# action = tf.clip_by_value(logits,tf.expand_dims(env.action_space.low,0),tf.expand_dims(env.action_space.high,0)) # Have to clip the action space. This might be a bad idea\n",
    "\n",
    "\n",
    "#should make so that the pi can be easily changed\n",
    "\n",
    "#Algorithm ie Soft Actor Critic - training etc makes the ops\n",
    "\n",
    "#Env\n",
    "\n",
    "#optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "log_dir = os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
    "                               'tensorflow/logs/soft_actor_critic')\n",
    "env_name = 'LunarLander-v2'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "n_inputs = env.observation_space.shape[0]\n",
    "n_outputs = env.action_space.n\n",
    "    \n",
    "epoch_length = 1000\n",
    "max_epochs = 100\n",
    "\n",
    "samples_per_env_step = 4\n",
    "max_buffer_size = 1e4\n",
    "\n",
    "n_hidden = 10\n",
    "n_layers = 1\n",
    "# Todo make these into lists so that can define each layer separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # THIS IS NECESSARY BEFORE MAKING NEW SESSION TO STOP IT ERRORING!!\n",
    "try:\n",
    "    sess\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    sess.close()\n",
    "    del sess\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "rewards = tf.placeholder(tf.float32,shape = [None],name = 'rewards')\n",
    "actions = tf.placeholder(tf.float32,shape = [None,n_outputs],name = 'action')\n",
    "observations = tf.placeholder(tf.float32,shape = [None,n_inputs],name = 'observations')\n",
    "next_observations = tf.placeholder(tf.float32,shape = [None,n_inputs],name = 'next_observations')\n",
    "dones = tf.placeholder(tf.float32,shape = [None],name = 'dones')\n",
    "\n",
    "qnet = Qnet(actions,observations,n_hidden,n_layers)\n",
    "vnet = Vnet(observations,n_hidden,n_layers)\n",
    "pnet = Policy_Discrete(n_outputs,observations,n_hidden,n_layers)\n",
    "\n",
    "sac = Soft_Actor_Critic(qnet,vnet,pnet,actions,observations,next_observations,rewards,dones)\n",
    "\n",
    "rb = replayBuffer(n_inputs,n_outputs,max_buffer_size)\n",
    "writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "\n",
    "tf.global_variables_initializer().run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_epochs):\n",
    "    obs = env.reset()\n",
    "    episodes = 0\n",
    "    episode_steps = 0\n",
    "    episode_reward = 0\n",
    "    mean_episode_reward = 0\n",
    "    \n",
    "    for t in range(epoch_length):\n",
    "        action = pnet.get_action(obs)\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        episode_steps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        rb.add_sample(action,obs,next_obs,reward,done)\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            episodes += 1\n",
    "            \n",
    "            log.record('episode_reward',episode_reward)\n",
    "            mean_episode_reward = (mean_episode_reward * (episodes - 1) + episode_reward) / episodes\n",
    "            episode_steps = 0\n",
    "            episode_reward = 0\n",
    "\n",
    "        if rb.ready_to_sample(samples_per_env_step):\n",
    "            samples = rb.get_samples(samples_per_env_step)\n",
    "            sac.train(samples)\n",
    "            \n",
    "    log.record('mean_episode_reward',mean_episode_reward)\n",
    "    print('Epoch %i, mean_reward %d' % i, mean_episode_reward)\n",
    "    \n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
