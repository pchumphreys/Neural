{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "% autoreload 2\n",
    "import setup_deep_q_expm\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expm_params import expm_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "expm_name = 'CartPole-v1'\n",
    "\n",
    "log,writer,merged,sampler,algo,qnet,policy,run_params = setup_deep_q_expm.setup_expm(expm_name = expm_name,params = expm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': {'lr': 1e-05, 'tau': 0.001},\n",
       " 'base': {'env_name': 'CartPole-v0',\n",
       "  'epoch_length': 1000,\n",
       "  'grad_steps_per_t': 1,\n",
       "  'max_episode_length': -1,\n",
       "  'max_epochs': 100,\n",
       "  'online_training': False},\n",
       " 'nnet': {'n_hidden': 24, 'n_layers': 2},\n",
       " 'policy': {'epsilon_decay': 1000,\n",
       "  'epsilon_end': 0.01,\n",
       "  'epsilon_start': 1.0,\n",
       "  'reward_scale': 1,\n",
       "  'scheme': 'Epsilon'},\n",
       " 'replay_buffer': {'batch_size': 32,\n",
       "  'max_buffer_size': 2000,\n",
       "  'min_pool_size': 1000}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expm_params[expm_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "0.32071784\n",
      "[-0.02614429 -0.00229675]\n",
      "0.000501096248626709\n",
      "Epoch 0, mean_reward 16\n",
      "-----\n",
      "0.5350645\n",
      "[-0.69309646 -0.22661   ]\n",
      "0.6760780218541618\n",
      "Epoch 1, mean_reward 10\n",
      "-----\n",
      "0.4851698\n",
      "[-0.23485553 -0.07686629]\n",
      "0.9025746455788617\n",
      "Epoch 2, mean_reward 9\n",
      "-----\n",
      "0.5194121\n",
      "[-1.2763993  -0.28696913]\n",
      "1.2812852504253378\n",
      "Epoch 3, mean_reward 9\n",
      "-----\n",
      "0.62873954\n",
      "[-1.3950073  -0.29626906]\n",
      "1.8229931553602217\n",
      "Epoch 4, mean_reward 9\n",
      "-----\n",
      "0.74058414\n",
      "[-5.675141  -1.1711048]\n",
      "2.3691129341125508\n",
      "Epoch 5, mean_reward 9\n",
      "-----\n",
      "0.7896648\n",
      "[-2.909739   -0.54468757]\n",
      "2.9288191354274735\n",
      "Epoch 6, mean_reward 9\n",
      "-----\n",
      "0.84253985\n",
      "[-6.0663667 -1.1465884]\n",
      "3.53915899682045\n",
      "Epoch 7, mean_reward 9\n",
      "-----\n",
      "0.82190895\n",
      "[-7.8841558 -1.5252168]\n",
      "4.249324735403063\n",
      "Epoch 8, mean_reward 9\n",
      "-----\n",
      "0.84637684\n",
      "[-2.209963   -0.45972517]\n",
      "5.017993646383287\n",
      "Epoch 9, mean_reward 9\n",
      "-----\n",
      "0.808742\n",
      "[-3.4842305 -0.6880264]\n",
      "5.884240005016329\n",
      "Epoch 10, mean_reward 9\n",
      "-----\n",
      "1.0484494\n",
      "[-11.075285   -2.1446817]\n",
      "6.791585918426514\n",
      "Epoch 11, mean_reward 9\n",
      "-----\n",
      "1.2103299\n",
      "[-11.460098   -2.2237968]\n",
      "7.769816851139068\n",
      "Epoch 12, mean_reward 9\n",
      "-----\n",
      "1.1666203\n",
      "[-12.280094   -2.4509733]\n",
      "8.887498698711381\n",
      "Epoch 13, mean_reward 9\n",
      "-----\n",
      "1.4804189\n",
      "[-20.712591  -4.146989]\n",
      "10.040372665405267\n",
      "Epoch 14, mean_reward 9\n",
      "-----\n",
      "1.4449052\n",
      "[-16.76417    -3.3992622]\n",
      "11.34113788461686\n",
      "Epoch 15, mean_reward 9\n",
      "-----\n",
      "1.7678734\n",
      "[-15.974143   -3.3374476]\n",
      "12.708952015876775\n",
      "Epoch 16, mean_reward 9\n",
      "-----\n",
      "1.8056239\n",
      "[-17.573921   -3.6624305]\n",
      "14.432242004394531\n",
      "Epoch 17, mean_reward 9\n",
      "-----\n",
      "1.8017272\n",
      "[-26.028772   -5.2719917]\n",
      "16.257811128616318\n",
      "Epoch 18, mean_reward 9\n",
      "-----\n",
      "2.4175954\n",
      "[-41.442474  -8.23135 ]\n",
      "18.123742533683796\n",
      "Epoch 19, mean_reward 9\n",
      "-----\n",
      "2.4273558\n",
      "[-31.9063     -6.3427486]\n",
      "20.209184123039226\n",
      "Epoch 20, mean_reward 9\n",
      "-----\n",
      "2.6681182\n",
      "[-26.963972  -5.366663]\n",
      "22.14243872642517\n",
      "Epoch 21, mean_reward 9\n",
      "-----\n",
      "2.217065\n",
      "[-21.508759  -4.149073]\n",
      "24.502248676300056\n",
      "Epoch 22, mean_reward 9\n",
      "-----\n",
      "2.6635525\n",
      "[-28.710398   -5.4687896]\n",
      "27.30887944984437\n",
      "Epoch 23, mean_reward 9\n",
      "-----\n",
      "2.754388\n",
      "[-25.177135  -4.698243]\n",
      "30.327813821792603\n",
      "Epoch 24, mean_reward 9\n",
      "-----\n",
      "3.2122042\n",
      "[-40.930775  -7.799995]\n",
      "33.20608222198489\n",
      "Epoch 25, mean_reward 9\n",
      "-----\n",
      "3.1085837\n",
      "[-25.217356   -4.6175923]\n",
      "36.68824652481079\n",
      "Epoch 26, mean_reward 9\n",
      "-----\n",
      "3.7053874\n",
      "[-27.139229  -4.811987]\n",
      "39.79816785430911\n",
      "Epoch 27, mean_reward 9\n",
      "-----\n",
      "3.9285803\n",
      "[-42.00896    -7.5603886]\n",
      "43.74604160308839\n",
      "Epoch 28, mean_reward 9\n",
      "-----\n",
      "4.5559235\n",
      "[-106.19796   -19.426598]\n",
      "47.78282568359378\n",
      "Epoch 29, mean_reward 9\n",
      "-----\n",
      "4.395854\n",
      "[-34.479053  -6.029219]\n",
      "52.13720788192748\n",
      "Epoch 30, mean_reward 9\n",
      "-----\n",
      "5.298331\n",
      "[-112.9866    -20.342663]\n",
      "56.93672052764892\n",
      "Epoch 31, mean_reward 9\n",
      "-----\n",
      "5.808647\n",
      "[-121.32412   -21.720179]\n",
      "61.546973670959474\n",
      "Epoch 32, mean_reward 9\n",
      "-----\n",
      "6.008078\n",
      "[-40.496014  -7.020624]\n",
      "66.38826841354381\n",
      "Epoch 33, mean_reward 9\n",
      "-----\n",
      "5.6342206\n",
      "[-81.49508  -14.421396]\n",
      "71.45611026000975\n",
      "Epoch 34, mean_reward 9\n",
      "-----\n",
      "5.7139683\n",
      "[-31.71078  -5.33333]\n",
      "77.32064215469369\n",
      "Epoch 35, mean_reward 9\n",
      "-----\n",
      "5.0930696\n",
      "[-95.5724   -16.808744]\n",
      "82.90991999053958\n",
      "Epoch 36, mean_reward 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(run_params['max_epochs']):\n",
    "    sampler.reset()\n",
    "    epoch_avg_losses = 0\n",
    "\n",
    "    for t in range(run_params['epoch_length']):\n",
    "        sampler.sample()\n",
    "        if sampler.batch_ready() or run_params['online_training']:\n",
    "            if run_params['online_training']:\n",
    "                samples = sampler.get_last_sample()\n",
    "                summary,losses,qnet_o = algo.train(samples,merged,algo.Q_Loss,qnet.output) \n",
    "            else:\n",
    "                for j in range(run_params['grad_steps_per_t']):\n",
    "                    samples = sampler.get_samples()\n",
    "                    \n",
    "                    qnet_next_obs = qnet.output.eval(feed_dict = {qnet.obs : samples['next_observations']})\n",
    "                    pred = np.argmax(qnet_next_obs,axis=1)\n",
    "                    one_hot = np.eye(qnet.output_size)[pred]\n",
    "                    tqnet_output = algo.target_Q_outputs.eval(feed_dict = {algo.next_obs : samples['next_observations']})\n",
    "#                      np.sum(one_hot*tqnet_output,axis=1)\n",
    "                    doubleQ = tqnet_output[range(expm_params[expm_name]['replay_buffer']['batch_size']),pred]\n",
    "                    \n",
    "                    qt = samples['rewards'] + algo.discount * (1- samples['dones']) *doubleQ\n",
    "                    feed_dict = algo._construct_feed_dict(samples)\n",
    "                    feed_dict[algo.Q_t] = qt\n",
    "                          \n",
    "                    _,summary,losses,qnet_o = tf.get_default_session().run([algo.train_ops,merged,algo.Q_Loss,qnet.output], feed_dict = feed_dict)\n",
    "\n",
    "#                     summary,losses,qnet_o= algo.train(samples,merged,algo.Q_Loss,qnet.output) \n",
    "            epoch_avg_losses = (epoch_avg_losses*(t) + np.array(losses))/(t+1)\n",
    "    print('-----')\n",
    "    print(np.sqrt(np.mean((qnet_next_obs[:10]-tqnet_output[:10])**2)))\n",
    "                    \n",
    "    print(qnet_o[0])\n",
    "    log.record('mean_episode_reward',sampler.mean_episode_reward)\n",
    "    writer.add_summary(summary, i)\n",
    "    print(epoch_avg_losses)\n",
    "\n",
    "    writer.flush()\n",
    "\n",
    "    print('Epoch %i, mean_reward %d' % (i, sampler.mean_episode_reward))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
