{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Model - this makes the network (ie the policy)\n",
    "\n",
    "#we have the soft Q function, the parameterised value function V, and the parameterised tractable policy pi\n",
    "\n",
    "# TODOs\n",
    "# Make comments properly\n",
    "\n",
    "# Whitening of inputs\n",
    "\n",
    "# Train, test routines\n",
    "\n",
    "\n",
    "class replayBuffer():\n",
    "    def __init__(self,n_inputs,n_outputs,max_buffer_size = 1e4,min_pool_size=1000,batch_size=128):\n",
    "        self._max_size = int(max_buffer_size)\n",
    "        self._min_pool_size = int(min_pool_size)\n",
    "        self._batch_size = batch_size\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_inputs = n_inputs\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self._size = 0\n",
    "        self._pos = 0\n",
    "        \n",
    "        self.actions = np.zeros([self._max_size,self.n_outputs])\n",
    "        self.observations = np.zeros([self._max_size,self.n_inputs])\n",
    "        self.next_observations = np.zeros([self._max_size,self.n_inputs])\n",
    "        self.rewards = np.zeros(self._max_size)\n",
    "        self.dones = np.zeros(self._max_size)\n",
    "        \n",
    "    def add_sample(self,action,obs,next_obs,reward,done):\n",
    "        self.actions[self._pos] = action\n",
    "        self.observations[self._pos] = obs\n",
    "        self.next_observations[self._pos] = next_obs\n",
    "        self.rewards[self._pos] = reward\n",
    "        self.dones[self._pos] = done\n",
    "        \n",
    "        self._advance()\n",
    "    \n",
    "    def _advance(self):\n",
    "        self._pos = (self._pos + 1) % self._max_size\n",
    "        \n",
    "        if self._size < self._max_size:\n",
    "            self._size += 1\n",
    "            \n",
    "    def get_samples(self):\n",
    "        inds = np.random.randint(0,self._size,self._batch_size)\n",
    "        return dict(actions = self.actions[inds],\n",
    "                   observations = self.observations[inds],\n",
    "                   next_observations = self.next_observations[inds],\n",
    "                   rewards = self.rewards[inds],\n",
    "                   dones = self.dones[inds])\n",
    "    \n",
    "    def get_last_sample(self):\n",
    "        last_pos = [(self._pos-1) % self._max_size]\n",
    "        return dict(actions = self.actions[last_pos],\n",
    "                   observations = self.observations[last_pos],\n",
    "                   next_observations = self.next_observations[last_pos],\n",
    "                   rewards = self.rewards[last_pos],\n",
    "                   dones = self.dones[last_pos])\n",
    "    \n",
    "    def batch_ready(self):\n",
    "        return self._size >= self._min_pool_size\n",
    "        \n",
    "class logger():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.vars = {}\n",
    "        \n",
    "    def record(self,var_name,value):\n",
    "        if hasattr(self.vars,var_name):\n",
    "            self.vars[var_name].append(value)\n",
    "        else:\n",
    "            self.vars[var_name] = [value]\n",
    "    \n",
    "    def get(self,var_name):\n",
    "        if hasattr(self.vars,var_name):\n",
    "            return self.vars[var_name]\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "class Sampler():\n",
    "    def __init__(self,policy,env,replaybuffer):\n",
    "        self.policy = policy\n",
    "        self.env = env\n",
    "        self.replaybuffer = replaybuffer\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self): \n",
    "        self.current_obs = False\n",
    "        self.episodes = 0\n",
    "        self.current_episode_reward = 0\n",
    "        self.mean_episode_reward = 0\n",
    "\n",
    "    def sample(self):\n",
    "        if (self.current_obs is False):\n",
    "            self.current_obs = env.reset()\n",
    "            \n",
    "        action = self.policy.get_action(self.current_obs)\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        if self.policy.discrete == True:\n",
    "            action = np.eye(self.policy.action_size)[action]\n",
    "        rb.add_sample(action,self.current_obs,next_obs,reward,done)\n",
    "        self.current_obs = next_obs\n",
    "        \n",
    "        self.current_episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            self.current_obs = False   \n",
    "            self.episodes += 1\n",
    "\n",
    "            log.record('episode_reward',self.current_episode_reward)\n",
    "            self.mean_episode_reward = (self.mean_episode_reward * (self.episodes - 1) + self.current_episode_reward) / self.episodes\n",
    "            self.current_episode_reward = 0\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here is a helper class to make a simple neural network. Importantly, it allows us to easily get the parameters, and hopefully to link the inputs to other variables\n",
    "# The get_output functionality is borrowed from the SAC reference code.\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self,name,inputs,output_size,n_hidden,n_layers):\n",
    "        self._name = name\n",
    "        self.inputs = inputs\n",
    "        self.output_size = output_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.output = self.make_network(reuse = False)\n",
    "        \n",
    "    def make_network(self,inputs = False,reuse = tf.AUTO_REUSE):\n",
    "        # This function just makes a simple fully connected network. It is structured in a little bit of a silly way. The idea is that this lets one reuse the network weights elsewhere with different inputs. Currently not actually using this functionality \n",
    "        if inputs is False :\n",
    "            inputs = self.inputs\n",
    "            \n",
    "        with tf.variable_scope(self._name,reuse = reuse):\n",
    "            if not(isinstance(inputs,tf.Tensor)):  # Can chuck in more than one input. This just concatenates them\n",
    "                inputs = tf.concat(inputs,axis=1)\n",
    "\n",
    "            # To do: understand weight initialization!   \n",
    "            self.hidden = slim.stack(inputs, slim.fully_connected, [self.n_hidden]*self.n_layers, scope='fc',activation_fn=tf.nn.relu) #,weights_regularizer=slim.l2_regularizer(0.1)\n",
    "            outputs = slim.fully_connected(self.hidden,self.output_size,activation_fn=None)\n",
    "        return outputs\n",
    "\n",
    "    def get_params_internal(self):\n",
    "        # Useful function to get network weights\n",
    "        \n",
    "        scope = tf.get_variable_scope().name\n",
    "        scope += '/' + self._name + '/' if len(scope) else self._name + '/'\n",
    "\n",
    "        return tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(MLP):\n",
    "    # Make a simple q network\n",
    "    def __init__(self,action_size,obs,n_hidden,n_layers):\n",
    "        # Ok so Q function takes s,a, gives Q   \n",
    "        self.obs = obs\n",
    "        # Super is used to call the init method of the parent class\n",
    "        super(Qnet,self).__init__('qNet',self.obs,action_size,n_hidden,n_layers)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Policy_Discrete():\n",
    "    # Make a policy.\n",
    "    # For now assume that discrete action space, such that tractable - obviously this slightly defeats the point of SAC implementation since all about how hard to compute the partition function\n",
    "    # P function takes s, needs to be able to give actions.\n",
    "\n",
    "    def __init__(self,Qnet,scheme = 'Bltz',reward_scale = 1.0,epsilon_start = 1,epsilon_end=0.1,epsilon_decay=10000):\n",
    "\n",
    "        self.Qnet = Qnet\n",
    "        self.action_size = Qnet.output_size\n",
    "        self.reward_scale = reward_scale\n",
    "        self.scheme = scheme\n",
    "        self.e = epsilon_start\n",
    "        self.e_end = epsilon_end\n",
    "        self.e_decay_frac = (1-1/epsilon_decay)\n",
    "        \n",
    "        self.discrete = True\n",
    "        self._name = 'Policy'\n",
    "        self.make_policy_outputs(reuse=False)\n",
    "        \n",
    "    def make_policy_outputs(self, reuse = tf.AUTO_REUSE):\n",
    "       \n",
    "        with tf.variable_scope(self._name,reuse = reuse):\n",
    "            if self.scheme == 'Bltz':\n",
    "                self.policy_output = tf.nn.softmax(reward_scale*self.Qnet.output,axis=1) # Automatically sum to one.\n",
    "                self.log_policy_output = tf.log(self.policy_output)\n",
    "                self.action = tf.multinomial(self.log_policy_output, num_samples=1)[0] # Will generate an action\n",
    "            elif self.scheme == 'Epsilon':\n",
    "                self.action = tf.argmax(self.Qnet.output)\n",
    "                \n",
    "            \n",
    "    def get_action(self,obs):\n",
    "        \n",
    "        if self.scheme == 'Bltz':\n",
    "                a = self.action.eval(feed_dict = {self.Qnet.obs : [obs]})[0]\n",
    "                \n",
    "        elif self.scheme == 'Epsilon':\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < self.e:\n",
    "                a = np.random.randint(0,self.action_size)\n",
    "            else:\n",
    "                a = self.action.eval(feed_dict = {self.Qnet.obs : [obs]})[0]\n",
    "                \n",
    "            self.e = (self.e - self.e_end) * self.e_decay_frac + self.e_end\n",
    "            \n",
    "        return a\n",
    "                \n",
    "\n",
    "    \n",
    "    \n",
    "class Deep_Q_Learning():\n",
    "    # This class handles the training of the networks\n",
    "    def __init__(self,Qnet,actions,obs,next_obs,rewards,dones,lr=3e-4,discount = 0.99, tau=0.005):\n",
    "        self.lr = lr\n",
    "        self.discount = discount\n",
    "        self.tau = tau \n",
    "        \n",
    "        # Maybe would be nicer to not pass these but define here, but this seems to be messy. Once check if works, could go back to defining here\n",
    "        self.actions = actions\n",
    "        self.obs = obs\n",
    "        self.next_obs = next_obs\n",
    "        self.rewards = rewards\n",
    "        self.dones = dones\n",
    "        \n",
    "        self.Qnet = Qnet\n",
    "        self.Q_outputs = Qnet.output\n",
    "        \n",
    "        with tf.variable_scope('qNet_T'):\n",
    "            self.target_Q_outputs = Qnet.make_network(inputs = self.next_obs,reuse=False) \n",
    "            self.target_Q_params = Qnet.get_params_internal()\n",
    "        \n",
    "        self.next_obs_out = Qnet.make_network(inputs = self.next_obs)\n",
    "        self.predict = tf.one_hot(tf.argmax(Qnet.make_network(inputs = self.next_obs),axis=1),self.Qnet.output_size)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.lr)\n",
    "        \n",
    "        self.train_ops = []\n",
    "        self.init_Q_net_training()\n",
    "        self.init_target_Q_update()\n",
    "        \n",
    "        \n",
    "    def init_Q_net_training(self):\n",
    "        training_variables = self.Qnet.get_params_internal()\n",
    "        with tf.variable_scope('Q_loss'):\n",
    "            Q_t = tf.stop_gradient(self.rewards +  self.discount * (1-self.dones) * tf.reduce_sum(self.target_Q_outputs*self.predict,axis=1))\n",
    "            self.Q_Loss = 0.5*tf.reduce_mean(tf.square(tf.reduce_sum(self.Q_outputs*self.actions,axis=1) - Q_t))\n",
    "            tf.summary.scalar('Q_loss', self.Q_Loss)\n",
    "        \n",
    "\n",
    "#         Qnet_regularization_losses = tf.get_collection(\n",
    "#             tf.GraphKeys.REGULARIZATION_LOSSES,\n",
    "#             scope=self.Qnet._name)\n",
    "#         Qnet_regularization_loss = tf.reduce_sum(\n",
    "#             Qnet_regularization_losses)\n",
    "    \n",
    "#         gradients, variables = zip(*self.optimizer.compute_gradients(self.Q_Loss + Qnet_regularization_loss,var_list = training_variables))\n",
    "#         gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "#         self.train_Q = self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        self.train_Q = self.optimizer.minimize(self.Q_Loss,var_list = training_variables)\n",
    "\n",
    "        self.train_ops.append(self.train_Q)\n",
    "        \n",
    "    def init_target_Q_update(self):\n",
    "        # Pull the qnet params\n",
    "        qnet_params = self.Qnet.get_params_internal()\n",
    "        \n",
    "        with tf.variable_scope('Target_Q_update'):\n",
    "            self.tQnet_update = []\n",
    "            for tQ_p in self.target_Q_params:\n",
    "                # Match each target net param with equiv from vnet\n",
    "                Q_p = [v for v in qnet_params if tQ_p.name[(tQ_p.name.index('/')+1):] in v.name]\n",
    "                assert(len(Q_p) == 1) # Check that only found one variable\n",
    "                Q_p = Q_p[0]\n",
    "                with tf.control_dependencies([self.train_Q]):\n",
    "                    self.tQnet_update.append(tQ_p.assign(self.tau * Q_p + (1-self.tau)*tQ_p))\n",
    "            self.tQnet_update = tf.group(self.tQnet_update)\n",
    "            \n",
    "        self.train_ops.append(self.tQnet_update)\n",
    "        \n",
    "    def _construct_feed_dict(self,samples):  \n",
    "        return {self.actions : samples['actions'],\n",
    "                    self.obs : samples['observations'],\n",
    "                    self.next_obs : samples['next_observations'],\n",
    "                    self.dones : samples['dones'],\n",
    "                    self.rewards : samples['rewards']}\n",
    "                    \n",
    "    def train(self, samples, *args):\n",
    "        feed_dict = self._construct_feed_dict(samples)\n",
    "        return tf.get_default_session().run([self.train_ops] + list(args), feed_dict = feed_dict)[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \n",
    "    'CartPole-v0' : \n",
    "    {'base' : dict(\n",
    "        env_name = 'CartPole-v0',\n",
    "        epoch_length = 1000,\n",
    "        max_epochs = 100,\n",
    "        online_training = True,\n",
    "        grad_steps_per_t = 1,\n",
    "        ),\n",
    "     'replay_buffer' : dict(\n",
    "        batch_size = 100,\n",
    "        max_buffer_size = 50000,\n",
    "        min_pool_size = 500,\n",
    "     ),\n",
    "     'algorithm' : dict(\n",
    "        reward_scale = 1,\n",
    "        epsilon_start = 1.0,\n",
    "        epsilon_end = 0.1,\n",
    "        epsilon_decay = 20000,\n",
    "        scheme = 'Epsilon',\n",
    "        lr = 1e-5,\n",
    "        tau = 0.01\n",
    "     ),\n",
    "     'nnet' : dict(\n",
    "        n_hidden = 10,\n",
    "        n_layers = 1\n",
    "     )\n",
    "    },\n",
    "    \n",
    "         'LunarLander-v2' : \n",
    "    {'base' : dict(\n",
    "        env_name = 'LunarLander-v2',\n",
    "        epoch_length = 1000,\n",
    "        max_epochs = 100,\n",
    "        online_training = False,\n",
    "        grad_steps_per_t = 1,\n",
    "        ),\n",
    "     'replay_buffer' : dict(\n",
    "        batch_size = 256,\n",
    "        max_buffer_size = 1e6,\n",
    "        min_pool_size = 1000,\n",
    "     ),\n",
    "     'algorithm' : dict(\n",
    "        reward_scale = 2,\n",
    "        epsilon_start = 1.0,\n",
    "        epsilon_end = 0.1,\n",
    "        epsilon_decay = 10000,\n",
    "        scheme = 'Epsilon',\n",
    "        lr = 3e-4,\n",
    "        tau = 0.01\n",
    "     ),\n",
    "     'nnet' : dict(\n",
    "        n_hidden = 20,\n",
    "        n_layers = 2\n",
    "     )\n",
    "    }\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "log_dir = os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
    "                               'tensorflow/logs/soft_actor_critic')\n",
    "expm_name = 'CartPole-v0'\n",
    "\n",
    "base_params = params[expm_name]['base']\n",
    "env = gym.make(base_params['env_name'])\n",
    "\n",
    "n_inputs = env.observation_space.shape[0]\n",
    "n_outputs = env.action_space.n\n",
    "    \n",
    "epoch_length = base_params['epoch_length']\n",
    "max_epochs = base_params['max_epochs']\n",
    "online_training = base_params['online_training']\n",
    "grad_steps_per_t = base_params['grad_steps_per_t']\n",
    "\n",
    "lr = params[expm_name]['algorithm']['lr']\n",
    "reward_scale = params[expm_name]['algorithm']['reward_scale']\n",
    "tau = params[expm_name]['algorithm']['tau']\n",
    "epsilon_start = params[expm_name]['algorithm']['epsilon_start']\n",
    "epsilon_end = params[expm_name]['algorithm']['epsilon_end']\n",
    "epsilon_decay = params[expm_name]['algorithm']['epsilon_decay']\n",
    "\n",
    "scheme = params[expm_name]['algorithm']['scheme']\n",
    "\n",
    "n_hidden = params[expm_name]['nnet']['n_hidden']\n",
    "n_layers = params[expm_name]['nnet']['n_layers']\n",
    "\n",
    "batch_size = params[expm_name]['replay_buffer']['batch_size']\n",
    "max_buffer_size = params[expm_name]['replay_buffer']['max_buffer_size']\n",
    "min_pool_size = params[expm_name]['replay_buffer']['min_pool_size']\n",
    "# Todo make these into lists so that can define each layer separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # THIS IS NECESSARY BEFORE MAKING NEW SESSION TO STOP IT ERRORING!!\n",
    "try:\n",
    "    sess\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    sess.close()\n",
    "    del sess\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "rewards = tf.placeholder(tf.float32,shape = [None],name = 'rewards')\n",
    "actions = tf.placeholder(tf.float32,shape = [None,n_outputs],name = 'actions')\n",
    "observations = tf.placeholder(tf.float32,shape = [None,n_inputs],name = 'observations')\n",
    "next_observations = tf.placeholder(tf.float32,shape = [None,n_inputs],name = 'next_observations')\n",
    "dones = tf.placeholder(tf.float32,shape = [None],name = 'dones')\n",
    "\n",
    "qnet = Qnet(n_outputs,observations,n_hidden=n_hidden,n_layers=n_layers)\n",
    "policy = Policy_Discrete(qnet,reward_scale=reward_scale,epsilon_start = epsilon_start,epsilon_end=epsilon_end,epsilon_decay=epsilon_decay,scheme=scheme)\n",
    "\n",
    "algo = Deep_Q_Learning(qnet,actions,observations,next_observations,rewards,dones,lr=lr,tau=tau)\n",
    "\n",
    "rb = replayBuffer(n_inputs,n_outputs,max_buffer_size,min_pool_size = min_pool_size,batch_size=batch_size)\n",
    "sampler = Sampler(policy,env,rb)\n",
    "\n",
    "            \n",
    "log = logger() \n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "\n",
    "tf.global_variables_initializer().run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8856529944334177\n",
      "Epoch 0, mean_reward 22\n",
      "0.9718393453322495\n",
      "Epoch 1, mean_reward 18\n",
      "0.8851114018789813\n",
      "Epoch 2, mean_reward 21\n",
      "0.8692759827123956\n",
      "Epoch 3, mean_reward 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-bc822482af64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_ready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0monline_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-ec74ae5934f7>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscrete\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-d357bf57b1c2>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me_end\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me_decay_frac\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me_end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     \"\"\"\n\u001b[0;32m--> 710\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5178\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5179\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5180\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(max_epochs):\n",
    "    sampler.reset()\n",
    "    epoch_avg_losses = 0\n",
    "    \n",
    "    for t in range(epoch_length):\n",
    "        sampler.sample()\n",
    "        \n",
    "        if rb.batch_ready() or online_training:\n",
    "            if online_training:\n",
    "                samples = rb.get_last_sample()\n",
    "                summary,losses,qnet_o = algo.train(samples,merged,algo.Q_Loss,qnet.output) \n",
    "            else:\n",
    "                for j in range(grad_steps_per_t):\n",
    "                    samples = rb.get_samples()\n",
    "                    summary,losses= algo.train(samples,merged,algo.Q_Loss) \n",
    "            epoch_avg_losses = (epoch_avg_losses*(t) + np.array(losses))/(t+1)\n",
    "            print(algo.predict.eval(feed_dict = algo._construct_feed_dict(samples)))\n",
    "            print(algo.next_obs_out.eval(feed_dict = algo._construct_feed_dict(samples)))\n",
    "            print(algo.target_Q_outputs.eval(feed_dict = algo._construct_feed_dict(samples)))\n",
    "\n",
    "    log.record('mean_episode_reward',sampler.mean_episode_reward)\n",
    "    writer.add_summary(summary, i)\n",
    "    print(epoch_avg_losses)\n",
    "    \n",
    "    writer.flush()\n",
    "\n",
    "    print('Epoch %i, mean_reward %d' % (i, sampler.mean_episode_reward))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]]\n",
      "[[0.02752121 0.25922558]]\n",
      "[[0.02610591 0.25770772]]\n",
      "[[0.00681379 0.05556605]]\n"
     ]
    }
   ],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0556815, 0.5648127]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
