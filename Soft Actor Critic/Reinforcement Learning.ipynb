{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "% autoreload 2\n",
    "import reinforcement_expm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from agents import load_expm_params\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_case_evaluation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1ccf4514a852>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtest_case_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_case_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mruns_per_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# grid_params = dict(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_case_evaluation' is not defined"
     ]
    }
   ],
   "source": [
    "expm_name = 'LunarLander_Soft'\n",
    "algo = 'DQN'\n",
    "params = load_expm_params(algo,expm_name)\n",
    "\n",
    "test_case_params = {\n",
    "    'soft' : dict(agent = dict(soft_learning = True)),\n",
    "#     'dueling/boltz' : dict(agent = dict(soft_learning = False,dueling =True), policy = dict(action_choice = 'Boltzmann')),\n",
    "#     'boltz' : dict(agent = dict(soft_learning = False,dueling =False), policy = dict(action_choice = 'Boltzmann')),\n",
    "#     'epsilon' : dict(agent = dict(soft_learning = False,dueling =True), policy = dict(action_choice = 'Epsilon'))\n",
    "}\n",
    "\n",
    "test_case_evaluation(params,test_case_params,runs_per_point = 2)\n",
    "\n",
    "# grid_params = dict(\n",
    "# lr = [1e-3,1e-4],\n",
    "# reward_scale = [1.0,0.1,10.0]\n",
    "# )\n",
    "\n",
    "# grid_search(params,grid_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def grid_search(params,grid_params,runs_per_point = 3):\n",
    "    \n",
    "    to_test = grid_params.items()\n",
    "    to_test_keys = [val[0] for val in to_test]\n",
    "    test_pts = np.meshgrid(*[val[1] for val in to_test])\n",
    "    test_pts = np.reshape(np.transpose(test_pts),[-1,len(test_pts)])\n",
    "\n",
    "    \n",
    "    max_episodes = params['runner']['max_episodes']\n",
    "\n",
    "    rewards = np.zeros((len(test_pts),runs_per_point,max_episodes))\n",
    "    for i,tp in enumerate(test_pts):\n",
    "        test_params = copy.deepcopy(params)\n",
    "        \n",
    "        for key,val in zip(to_test_keys,tp):\n",
    "            test_params['agent'][key]= val\n",
    "            print('Testing %s: %.2e' % (key,val))\n",
    "        for j in range(runs_per_point):\n",
    "            runner = reinforcement_expm.run_expm(expm_name,algo, params = test_params)\n",
    "            rewards[i,j] = runner.episode_rewards\n",
    "\n",
    "    title = ', '.join(to_test_keys)\n",
    "    labels = [', '.join([\"%.2e\" % val for val in pt]) for pt in test_pts]\n",
    "\n",
    "    x = range(np.shape(rewards)[-1])\n",
    "    ys = np.mean(rewards,axis=1)\n",
    "    errors = np.std(rewards,axis=1)\n",
    "\n",
    "    for y,error,lr in zip(ys,errors,labels):\n",
    "        plt.plot(x, y, '-',label=lr)\n",
    "        plt.fill_between(x, y-error, y+error,alpha=0.2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case_evaluation(params,test_case_params,runs_per_point = 3):\n",
    "    \n",
    "    runs_per_point = 3\n",
    "    max_episodes = params['runner']['max_episodes']\n",
    "    rewards = np.zeros((len(test_case_params),runs_per_point,max_episodes))\n",
    "    \n",
    "    for i,(test_name,test) in enumerate(iter(test_case_params.items())):\n",
    "        test_params = copy.deepcopy(params)\n",
    "        for cat, items in iter(test.items()):\n",
    "            for item, values in iter(items.items()):\n",
    "                test_params[cat][item] = values\n",
    "\n",
    "        print('Testing %s' % (test_name))\n",
    "        print(test_params)\n",
    "        \n",
    "        for j in range(runs_per_point):\n",
    "            runner = reinforcement_expm.run_expm(expm_name,algo, params = test_params)\n",
    "            rewards[i,j] = runner.episode_rewards\n",
    "\n",
    "    labels = test_case_params.keys()\n",
    "\n",
    "    x = range(np.shape(rewards)[-1])\n",
    "    ys = np.mean(rewards,axis=1)\n",
    "    errors = np.std(rewards,axis=1)\n",
    "\n",
    "    for y,error,lr in zip(ys,errors,labels):\n",
    "        plt.plot(x, y, '-',label=lr)\n",
    "        plt.fill_between(x, y-error, y+error,alpha=0.2)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
