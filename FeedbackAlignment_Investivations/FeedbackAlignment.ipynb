{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeedbackAlignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pchumphreys/Neural/blob/master/FeedbackAlignment_Investivations/FeedbackAlignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XxFrnTSng6k0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing feedback alignment approaches\n",
        "\n",
        "Ok, so what is the plan?\n",
        "\n",
        "1.) Load the CIFAR10 dataset\n",
        "2.) Get standard BP working, with a standard FF net\n",
        "3.) We need a duplicate set of weights for feedback. This could be straighforward if just duplicate the graph. For FA, basically just swap out the gradient rule so that using these weights as opposed to other weights. Also, need to work out how to apply gradients to other weights. Maybe instead of having a duplicate net, just duplicate the weight variables? Might be more elegant.\n",
        "4.) I think that this should be sufficient for the first experiments. What do we want to do?\n",
        "\n",
        "  a) Test FA vs BP, check matches\n",
        "  b) Maybe look at the versions from the Ashok paper\n",
        "  c) Learning FB weights as well\n",
        "  d) Think carefully about the initialisation issue"
      ]
    },
    {
      "metadata": {
        "id": "RmJPh67oOfCE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Util code"
      ]
    },
    {
      "metadata": {
        "id": "Y-PRMA8QOjY0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "class Batcher():\n",
        "    def __init__(self,x_data,y_data,mini_batch_size):\n",
        "        self.epoch = 1\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.nbatches = int(len(x_data)/mini_batch_size)\n",
        "        if len(x_data)%self.nbatches==0:\n",
        "            self.perfect_sample = 1\n",
        "        else:\n",
        "            self.perfect_sample = 0\n",
        "        self.samples = copy.deepcopy(x_data)\n",
        "        self.targets = copy.deepcopy(y_data)\n",
        "        self.samples, self.targets = shuffle(self.samples,self.targets)\n",
        "        self.batch=0\n",
        "    def get_mini_batch(self):\n",
        "        if self.batch < (self.nbatches-self.perfect_sample):\n",
        "            samples= self.samples[self.batch*self.mini_batch_size:((self.batch+1)*self.mini_batch_size)]\n",
        "            targets= self.targets[self.batch*self.mini_batch_size:((self.batch+1)*self.mini_batch_size)]\n",
        "            last_batch = False\n",
        "        else:\n",
        "            samples = self.samples[self.batch*self.mini_batch_size:]\n",
        "            targets = self.targets[self.batch*self.mini_batch_size:]\n",
        "            self.samples, self.targets = shuffle(self.samples,self.targets)\n",
        "            self.epoch += 1\n",
        "            self.batch = 0\n",
        "            last_batch = True\n",
        "        self.batch += 1\n",
        "        return samples,targets,last_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-IwQo9dg4vPQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Custom gradient propagation"
      ]
    },
    {
      "metadata": {
        "id": "XP4HXDG3kJok",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We need to be able to inject our own gradients!\n",
        "\n",
        "Here is a code snippet for overriding gradient op:\n",
        "```\n",
        "@tf.custom_gradient\n",
        "def clip_grad_layer(x):\n",
        "  def grad(dy):\n",
        "    return tf.clip_by_value(dy, -0.1, 0.1)\n",
        "  return tf.identity(x), grad\n",
        "```\n",
        "So I guess this means we have to make a custom wrapper to define a layer, create the variables etc that will incorporate a custom matmul function.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QSqq9pcozika",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "from tensorflow.contrib.layers.python.layers import initializers\n",
        "from tensorflow.python.ops import init_ops\n",
        "from tensorflow.python.ops import nn\n",
        "import numpy as np\n",
        "\n",
        "def feedback_matmul(x,weights,weights_fb,update_fb_weights = False, is_sparse = False):\n",
        "  '''\n",
        "  We want to use a custom matrix multiplication function that uses a separate feedback network for gradient calculation\n",
        "  Note that cannot handle transpose due to limitations of custom_gradient code\n",
        "  '''\n",
        "  \n",
        "  def _fb_helper(weight_grad):\n",
        "    if update_fb_weights:\n",
        "      return weight_grad\n",
        "    else: \n",
        "      return None\n",
        "  \n",
        "  def _forward_helper(x,weights):\n",
        "    if is_sparse:\n",
        "      return sparse_matmul(x, weights, b_is_sparse=True)\n",
        "    else:\n",
        "      return tf.matmul(x,weights)\n",
        "    \n",
        "  @tf.custom_gradient\n",
        "  def matmul_function(x,weights,weights_fb):\n",
        "    def grad(dzdy):\n",
        "      weight_grad = tf.matmul(dzdy,x,transpose_a=True)\n",
        "      weight_grad_fb = _fb_helper(weight_grad)\n",
        "      return tf.matmul(dzdy,weights_fb),weight_grad,weight_grad_fb # Since matrix multiplication has two args, need two gradients, but we don't care about propagating through the weight\n",
        "    return _forward_helper(x,weights), grad\n",
        "  \n",
        "  return matmul_function(x,weights,weights_fb)\n",
        "\n",
        "\n",
        "# Here is our fully connected layer!\n",
        "def feedback_fc(input_tensor,output_size,mode = 'BP',scope=None,reuse = None,weights_initializer = tf.initializers.glorot_normal(),weights_fb_initializer = tf.initializers.glorot_normal(),biases_initializer=init_ops.zeros_initializer(),activation_fn=nn.relu):\n",
        "  if activation_fn == None:\n",
        "    activation_fn = tf.identity\n",
        "  with tf.variable_scope(scope,reuse=reuse):\n",
        "    weights = slim.model_variable('weights',shape = [input_tensor.shape[1],output_size],initializer=weights_initializer)\n",
        "    weights_fb = slim.model_variable('weights_fb',shape = [input_tensor.shape[1],output_size],initializer=weights_fb_initializer)\n",
        "    biases = slim.model_variable('biases',shape = [output_size],initializer=biases_initializer)\n",
        "    if mode == 'BP':\n",
        "      return activation_fn(tf.matmul(input_tensor,weights) + biases)\n",
        "    elif mode == 'FA':\n",
        "      return activation_fn(feedback_matmul(input_tensor,weights,weights_fb) + biases)\n",
        "    elif mode == 'FA_update_both':\n",
        "      return activation_fn(feedback_matmul(input_tensor,weights,weights_fb,update_fb_weights = True) + biases)\n",
        "    else:\n",
        "      raise ValueError('Undefined mode {}'.format(mode))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HcrK2AMs5RzX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run some tests to check makes sense"
      ]
    },
    {
      "metadata": {
        "id": "5KiZEa_Azf3c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph() # THIS IS NECESSARY BEFORE MAKING NEW SESSION TO STOP IT ERRORING!!\n",
        "\n",
        "if not(tf.get_default_session() is None):\n",
        "    tf.get_default_session().close()\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "inputs = tf.placeholder(tf.float32,shape=[None, 1],name='inputs')\n",
        "x = feedback_fc(inputs,1,scope='fc',mode='FA',activation_fn=None)\n",
        "\n",
        "tf.global_variables_initializer().run()\n",
        "\n",
        "test_inputs = [[1.0],[2.0]]\n",
        "forward = sess.run(x,feed_dict = {inputs:test_inputs})\n",
        "\n",
        "weight = [var for var in tf.global_variables() if var.name == 'fc/weights:0'][0].eval()[[0]]\n",
        "\n",
        "assert (forward/test_inputs==weight).all()\n",
        "\n",
        "grad = tf.gradients(x,inputs)[0].eval(feed_dict = {inputs:test_inputs})\n",
        "fb_weight = [var for var in tf.global_variables() if var.name == 'fc/weights_fb:0'][0].eval()[[0]]\n",
        "assert (grad == fb_weight).all()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fBbdhqw-a4X_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ]
    },
    {
      "metadata": {
        "id": "LH8uvsx0OZV7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Start with MNIST, check that works"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2uSQlPhLDzhz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32')/255\n",
        "x_test = x_test.astype('float32')/255\n",
        "y_train = y_train.astype('int32')\n",
        "y_test = y_test.astype('int32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RiK-kFlSDzh8"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the network to replicate DFA paper - arxiv 1609.01596"
      ]
    },
    {
      "metadata": {
        "id": "XHb_-L_yfrTP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Test BP with RELU"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "E3bb1BrMDzh9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph() # THIS IS NECESSARY BEFORE MAKING NEW SESSION TO STOP IT ERRORING!!\n",
        "\n",
        "if not(tf.get_default_session() is None):\n",
        "    tf.get_default_session().close()\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "network_mode = 'BP'\n",
        "\n",
        "inputs = tf.placeholder(tf.float32,shape=[None, 28,28],name='inputs')\n",
        "reshaped_inputs = tf.reshape(inputs,[-1,28*28])\n",
        "training = tf.placeholder(tf.bool)\n",
        "\n",
        "n_categories = 10 \n",
        "\n",
        "dropout_rate = 0.0\n",
        "layers = [800,800]\n",
        "activation = tf.nn.relu\n",
        "\n",
        "x = reshaped_inputs\n",
        "n_prev = x.shape[1].value\n",
        "\n",
        "for i in range(len(layers)):\n",
        "  if i == len(layers)-1:\n",
        "    n_next = n_categories\n",
        "  else:\n",
        "    n_next = layers[i+1]\n",
        "    \n",
        "  weights_initializer = tf.initializers.random_uniform(minval = -1/np.sqrt(n_prev),maxval = 1/np.sqrt(n_prev))\n",
        "  weights_fb_initializer = tf.initializers.random_uniform(minval = -1/np.sqrt(n_next),maxval = 1/np.sqrt(n_next))\n",
        "  biases_initializer = tf.initializers.random_uniform(minval = -1/np.sqrt(n_prev),maxval = 1/np.sqrt(n_prev))\n",
        "\n",
        "  x = feedback_fc(x,layers[i],scope='fc_' + str(i),mode=network_mode,activation_fn=activation,\n",
        "                 weights_initializer = weights_initializer, biases_initializer = biases_initializer)\n",
        "  x = tf.layers.dropout(x,rate = dropout_rate, training = training)\n",
        "  \n",
        "  n_prev = x.shape[1].value\n",
        "  \n",
        "logits = feedback_fc(x,n_categories,scope='soft',mode=network_mode,activation_fn=None)\n",
        "\n",
        "targets = tf.placeholder(tf.int64,shape=[None],name='targets')\n",
        "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.squeeze(targets),logits = logits),axis=-1)\n",
        "error = tf.reduce_mean(tf.cast(tf.logical_not(tf.equal(tf.argmax(logits,axis=-1),tf.squeeze(targets))),'float'))\n",
        "train_op = tf.train.RMSPropOptimizer(learning_rate = 1e-3).minimize(loss)\n",
        "\n",
        "tf.summary.scalar('loss',loss)\n",
        "tf.summary.scalar('error',error)\n",
        "\n",
        "merged = tf.summary.merge_all()\n",
        "tf.global_variables_initializer().run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9GQykFxqDzh-"
      },
      "cell_type": "markdown",
      "source": [
        "Run"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "233f4e15-f0ba-4d18-ff94-aae24dd84959",
        "id": "Q7NtwcUIDzh_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1082
        }
      },
      "cell_type": "code",
      "source": [
        "mini_batch_size = 64\n",
        "\n",
        "b = Batcher(x_train,y_train,mini_batch_size)\n",
        "max_epochs = 100\n",
        "test_epoch_int = 10\n",
        "\n",
        "print(\"Batches per epoch %d\" %b.nbatches)\n",
        "while b.epoch <= max_epochs:\n",
        "    batch_x,batch_y, new_epoch = b.get_mini_batch()\n",
        "    sess.run([train_op],feed_dict = {inputs : batch_x, targets:batch_y, training : True})\n",
        "    if new_epoch:\n",
        "        batch_err,summary = sess.run([error,merged],feed_dict = {inputs : batch_x, targets:batch_y, training : False})\n",
        "#         train_writer.add_summary(summary)\n",
        "        if (b.epoch-1) % test_epoch_int == 0:\n",
        "          print(\"Train error is {}% at epoch {}\".format(batch_err*100,b.epoch-1))\n",
        "          test_err,summary = sess.run([error,merged],feed_dict = {inputs : x_test, targets : y_test, training : False})\n",
        "          print(\"Test error is {}% at epoch {}\".format(test_err*100,b.epoch-1))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batches per epoch 937\n",
            "Train error is 0.0% at epoch 10\n",
            "Test error is 1.8400000408291817% at epoch 10\n",
            "Train error is 0.0% at epoch 20\n",
            "Test error is 1.9200000911951065% at epoch 20\n",
            "Train error is 0.0% at epoch 30\n",
            "Test error is 1.8799999728798866% at epoch 30\n",
            "Train error is 0.0% at epoch 40\n",
            "Test error is 1.8400000408291817% at epoch 40\n",
            "Train error is 0.0% at epoch 50\n",
            "Test error is 1.5799999237060547% at epoch 50\n",
            "Train error is 0.0% at epoch 60\n",
            "Test error is 1.6200000420212746% at epoch 60\n",
            "Train error is 0.0% at epoch 70\n",
            "Test error is 1.489999983459711% at epoch 70\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1722cd49189a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batches per epoch %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-4686c6c71b91>\u001b[0m in \u001b[0;36mget_mini_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \"\"\"\n\u001b[1;32m    402\u001b[0m     \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;31m# convert sparse matrices to CSR for row-based indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0mresampled_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msafe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled_arrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# syntactic sugar for the unit argument case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;31m# convert sparse matrices to CSR for row-based indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0mresampled_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msafe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled_arrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# syntactic sugar for the unit argument case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[0;34m(X, indices)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w7tUm2nwEPxD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Batches per epoch 937\n",
        "Train error is 0.0% at epoch 10\n",
        "Test error is 2.15000007301569% at epoch 10\n",
        "Train error is 0.0% at epoch 20\n",
        "Test error is 1.7100000753998756% at epoch 20\n",
        "Train error is 0.0% at epoch 30\n",
        "Test error is 1.7999999225139618% at epoch 30\n",
        "Train error is 0.0% at epoch 40\n",
        "Test error is 1.510000042617321% at epoch 40\n",
        "Train error is 0.0% at epoch 50\n",
        "Test error is 1.5300000086426735% at epoch 50\n",
        "Train error is 0.0% at epoch 60\n",
        "Test error is 1.489999983459711% at epoch 60\n",
        "Train error is 0.0% at epoch 70\n",
        "Test error is 1.4800000004470348% at epoch 70\n",
        "Train error is 0.0% at epoch 80\n",
        "Test error is 1.4800000004470348% at epoch 80\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "-60lLsK2fk2h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Test feedback alignment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uRhxsv7pflGt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph() # THIS IS NECESSARY BEFORE MAKING NEW SESSION TO STOP IT ERRORING!!\n",
        "\n",
        "if not(tf.get_default_session() is None):\n",
        "    tf.get_default_session().close()\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "network_mode = 'FA'\n",
        "\n",
        "inputs = tf.placeholder(tf.float32,shape=[None, 28,28],name='inputs')\n",
        "reshaped_inputs = tf.reshape(inputs,[-1,28*28])\n",
        "training = tf.placeholder(tf.bool)\n",
        "\n",
        "n_categories = 10 \n",
        "\n",
        "dropout_rate = 0.0\n",
        "layers = [800,800]\n",
        "activation = tf.nn.relu\n",
        "\n",
        "x = reshaped_inputs\n",
        "n_prev = x.shape[1].value\n",
        "\n",
        "for i in range(len(layers)):\n",
        "  if i == len(layers)-1:\n",
        "    n_next = n_categories\n",
        "  else:\n",
        "    n_next = layers[i+1]\n",
        "    \n",
        "  weights_initializer = tf.initializers.random_uniform(minval = -1/np.sqrt(n_prev),maxval = 1/np.sqrt(n_prev))\n",
        "  weights_fb_initializer = tf.initializers.random_uniform(minval = -1/np.sqrt(n_next),maxval = 1/np.sqrt(n_next))\n",
        "  biases_initializer = tf.initializers.random_uniform(minval = -1/np.sqrt(n_prev),maxval = 1/np.sqrt(n_prev))\n",
        "\n",
        "  x = feedback_fc(x,layers[i],scope='fc_' + str(i),mode=network_mode,activation_fn=activation,\n",
        "                 weights_initializer = weights_initializer, biases_initializer = biases_initializer)\n",
        "  x = tf.layers.dropout(x,rate = dropout_rate, training = training)\n",
        "  \n",
        "  n_prev = x.shape[1].value\n",
        "  \n",
        "logits = feedback_fc(x,n_categories,scope='soft',mode=network_mode,activation_fn=None)\n",
        "\n",
        "targets = tf.placeholder(tf.int64,shape=[None],name='targets')\n",
        "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.squeeze(targets),logits = logits),axis=-1)\n",
        "error = tf.reduce_mean(tf.cast(tf.logical_not(tf.equal(tf.argmax(logits,axis=-1),tf.squeeze(targets))),'float'))\n",
        "train_op = tf.train.RMSPropOptimizer(learning_rate = 1e-4).minimize(loss)\n",
        "\n",
        "tf.summary.scalar('loss',loss)\n",
        "tf.summary.scalar('error',error)\n",
        "\n",
        "merged = tf.summary.merge_all()\n",
        "tf.global_variables_initializer().run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5l13Ryj9flGx"
      },
      "cell_type": "markdown",
      "source": [
        "Run"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "d91b7e2d-b096-45bd-8fb3-fdfc27509ae1",
        "id": "sUcWkqN-flGy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        }
      },
      "cell_type": "code",
      "source": [
        "mini_batch_size = 64\n",
        "\n",
        "b = Batcher(x_train,y_train,mini_batch_size)\n",
        "max_epochs = 100\n",
        "test_epoch_int = 10\n",
        "\n",
        "print(\"Batches per epoch %d\" %b.nbatches)\n",
        "while b.epoch <= max_epochs:\n",
        "    batch_x,batch_y, new_epoch = b.get_mini_batch()\n",
        "    sess.run([train_op],feed_dict = {inputs : batch_x, targets:batch_y, training : True})\n",
        "    if new_epoch:\n",
        "        batch_err,summary = sess.run([error,merged],feed_dict = {inputs : batch_x, targets:batch_y, training : False})\n",
        "#         train_writer.add_summary(summary)\n",
        "        if (b.epoch-1) % test_epoch_int == 0:\n",
        "          print(\"Train error is {}% at epoch {}\".format(batch_err*100,b.epoch-1))\n",
        "          test_err,summary = sess.run([error,merged],feed_dict = {inputs : x_test, targets : y_test, training : False})\n",
        "          print(\"Test error is {}% at epoch {}\".format(test_err*100,b.epoch-1))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batches per epoch 937\n",
            "Train error is 3.125% at epoch 10\n",
            "Test error is 2.4399999529123306% at epoch 10\n",
            "Train error is 0.0% at epoch 20\n",
            "Test error is 1.9200000911951065% at epoch 20\n",
            "Train error is 0.0% at epoch 30\n",
            "Test error is 1.7999999225139618% at epoch 30\n",
            "Train error is 0.0% at epoch 40\n",
            "Test error is 1.9600000232458115% at epoch 40\n",
            "Train error is 0.0% at epoch 50\n",
            "Test error is 1.7999999225139618% at epoch 50\n",
            "Train error is 0.0% at epoch 60\n",
            "Test error is 1.7799999564886093% at epoch 60\n",
            "Train error is 0.0% at epoch 70\n",
            "Test error is 1.7400000244379044% at epoch 70\n",
            "Train error is 0.0% at epoch 80\n",
            "Test error is 1.8200000748038292% at epoch 80\n",
            "Train error is 0.0% at epoch 90\n",
            "Test error is 1.7599999904632568% at epoch 90\n",
            "Train error is 0.0% at epoch 100\n",
            "Test error is 1.8400000408291817% at epoch 100\n",
            "Train error is 0.0% at epoch 110\n",
            "Test error is 1.769999973475933% at epoch 110\n",
            "Train error is 0.0% at epoch 120\n",
            "Test error is 1.7799999564886093% at epoch 120\n",
            "Train error is 0.0% at epoch 130\n",
            "Test error is 1.7899999395012856% at epoch 130\n",
            "Train error is 0.0% at epoch 140\n",
            "Test error is 1.810000091791153% at epoch 140\n",
            "Train error is 0.0% at epoch 150\n",
            "Test error is 1.769999973475933% at epoch 150\n",
            "Train error is 0.0% at epoch 160\n",
            "Test error is 1.7899999395012856% at epoch 160\n",
            "Train error is 0.0% at epoch 170\n",
            "Test error is 1.7799999564886093% at epoch 170\n",
            "Train error is 0.0% at epoch 180\n",
            "Test error is 1.7599999904632568% at epoch 180\n",
            "Train error is 0.0% at epoch 190\n",
            "Test error is 1.810000091791153% at epoch 190\n",
            "Train error is 0.0% at epoch 200\n",
            "Test error is 1.7899999395012856% at epoch 200\n",
            "Train error is 0.0% at epoch 210\n",
            "Test error is 1.7899999395012856% at epoch 210\n",
            "Train error is 0.0% at epoch 220\n",
            "Test error is 1.7899999395012856% at epoch 220\n",
            "Train error is 0.0% at epoch 230\n",
            "Test error is 1.7799999564886093% at epoch 230\n",
            "Train error is 0.0% at epoch 240\n",
            "Test error is 1.7599999904632568% at epoch 240\n",
            "Train error is 0.0% at epoch 250\n",
            "Test error is 1.7799999564886093% at epoch 250\n",
            "Train error is 0.0% at epoch 260\n",
            "Test error is 1.7799999564886093% at epoch 260\n",
            "Train error is 0.0% at epoch 270\n",
            "Test error is 1.7999999225139618% at epoch 270\n",
            "Train error is 0.0% at epoch 280\n",
            "Test error is 1.810000091791153% at epoch 280\n",
            "Train error is 0.0% at epoch 290\n",
            "Test error is 1.7899999395012856% at epoch 290\n",
            "Train error is 0.0% at epoch 300\n",
            "Test error is 1.7999999225139618% at epoch 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xhvMW9xXm0Vy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test FA, both learn"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9Q5WOVm7m4l-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph() # THIS IS NECESSARY BEFORE MAKING NEW SESSION TO STOP IT ERRORING!!\n",
        "\n",
        "if not(tf.get_default_session() is None):\n",
        "    tf.get_default_session().close()\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "network_mode = 'FA_update_both'\n",
        "\n",
        "inputs = tf.placeholder(tf.float32,shape=[None, 28,28],name='inputs')\n",
        "reshaped_inputs = tf.reshape(inputs,[-1,28*28])\n",
        "training = tf.placeholder(tf.bool)\n",
        "\n",
        "n_categories = 10 \n",
        "\n",
        "dropout_rate = 0.0\n",
        "layers = [800,800]\n",
        "activation = tf.nn.relu\n",
        "\n",
        "x = reshaped_inputs\n",
        "n_prev = x.shape[1].value\n",
        "\n",
        "for i in range(len(layers)):\n",
        "  if i == len(layers)-1:\n",
        "    n_next = n_categories\n",
        "  else:\n",
        "    n_next = layers[i+1]\n",
        "    \n",
        "  weights_initializer = tf.initializers.random_uniform(minval = -1/np.sqrt(n_prev),maxval = 1/np.sqrt(n_prev))\n",
        "  weights_fb_initializer = tf.initializers.random_uniform(minval = -1/np.sqrt(n_next),maxval = 1/np.sqrt(n_next))\n",
        "  biases_initializer = tf.initializers.random_uniform(minval = -1/np.sqrt(n_prev),maxval = 1/np.sqrt(n_prev))\n",
        "\n",
        "  x = feedback_fc(x,layers[i],scope='fc_' + str(i),mode=network_mode,activation_fn=activation,\n",
        "                 weights_initializer = weights_initializer, biases_initializer = biases_initializer)\n",
        "  x = tf.layers.dropout(x,rate = dropout_rate, training = training)\n",
        "  \n",
        "  n_prev = x.shape[1].value\n",
        "  \n",
        "logits = feedback_fc(x,n_categories,scope='soft',mode=network_mode,activation_fn=None)\n",
        "\n",
        "targets = tf.placeholder(tf.int64,shape=[None],name='targets')\n",
        "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.squeeze(targets),logits = logits),axis=-1)\n",
        "error = tf.reduce_mean(tf.cast(tf.logical_not(tf.equal(tf.argmax(logits,axis=-1),tf.squeeze(targets))),'float'))\n",
        "train_op = tf.train.RMSPropOptimizer(learning_rate = 1e-4).minimize(loss)\n",
        "\n",
        "tf.summary.scalar('loss',loss)\n",
        "tf.summary.scalar('error',error)\n",
        "\n",
        "merged = tf.summary.merge_all()\n",
        "tf.global_variables_initializer().run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uYS0tqQDm4mF"
      },
      "cell_type": "markdown",
      "source": [
        "Run"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "2db5717d-bba9-42ae-a60f-38b2d5114617",
        "id": "Ly2FmqpIm4mF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1187
        }
      },
      "cell_type": "code",
      "source": [
        "mini_batch_size = 64\n",
        "\n",
        "b = Batcher(x_train,y_train,mini_batch_size)\n",
        "max_epochs = 100\n",
        "test_epoch_int = 10\n",
        "\n",
        "print(\"Batches per epoch %d\" %b.nbatches)\n",
        "while b.epoch <= max_epochs:\n",
        "    batch_x,batch_y, new_epoch = b.get_mini_batch()\n",
        "    sess.run([train_op],feed_dict = {inputs : batch_x, targets:batch_y, training : True})\n",
        "    if new_epoch:\n",
        "        batch_err,summary = sess.run([error,merged],feed_dict = {inputs : batch_x, targets:batch_y, training : False})\n",
        "#         train_writer.add_summary(summary)\n",
        "        if (b.epoch-1) % test_epoch_int == 0:\n",
        "          print(\"Train error is {}% at epoch {}\".format(batch_err*100,b.epoch-1))\n",
        "          test_err,summary = sess.run([error,merged],feed_dict = {inputs : x_test, targets : y_test, training : False})\n",
        "          print(\"Test error is {}% at epoch {}\".format(test_err*100,b.epoch-1))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batches per epoch 937\n",
            "Train error is 0.0% at epoch 10\n",
            "Test error is 2.319999970495701% at epoch 10\n",
            "Train error is 0.0% at epoch 20\n",
            "Test error is 1.8899999558925629% at epoch 20\n",
            "Train error is 0.0% at epoch 30\n",
            "Test error is 1.9099999219179153% at epoch 30\n",
            "Train error is 0.0% at epoch 40\n",
            "Test error is 1.8400000408291817% at epoch 40\n",
            "Train error is 0.0% at epoch 50\n",
            "Test error is 1.8600000068545341% at epoch 50\n",
            "Train error is 0.0% at epoch 60\n",
            "Test error is 1.8400000408291817% at epoch 60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-1722cd49189a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbatch_err\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "jOp4wOzG5b1C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CIFAR10"
      ]
    },
    {
      "metadata": {
        "id": "WTqjeCyRgIqE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load in the data"
      ]
    },
    {
      "metadata": {
        "id": "Fd1yLKRuhyrh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')/255\n",
        "x_test = x_test.astype('float32')/255\n",
        "y_train = y_train.astype('int32')\n",
        "y_test = y_test.astype('int32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fuqwEIKqgUn2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Vanilla BP"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aCSD9g6YgU5Z",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph() # THIS IS NECESSARY BEFORE MAKING NEW SESSION TO STOP IT ERRORING!!\n",
        "\n",
        "if not(tf.get_default_session() is None):\n",
        "    tf.get_default_session().close()\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "network_mode = 'BP'\n",
        "\n",
        "inputs = tf.placeholder(tf.float32,shape=[None, 32,32,3],name='inputs')\n",
        "reshaped_inputs = tf.reshape(inputs,[-1,32*32*3])\n",
        "training = tf.placeholder(tf.bool)\n",
        "\n",
        "n_categories = 10 \n",
        "\n",
        "dropout_rate = 0.1\n",
        "layers = [1024,1024,1024]\n",
        "activation = tf.nn.tanh\n",
        "\n",
        "x = reshaped_inputs\n",
        "\n",
        "for i in range(len(layers)):\n",
        "#   weights_fb_initializer = tf.initializers.random_uniform(minval = -1/np.sqrt(n_next),maxval = 1/np.sqrt(n_next))\n",
        " \n",
        "  x = feedback_fc(x,layers[i],scope='fc_' + str(i),mode=network_mode,activation_fn=activation)\n",
        "  x = tf.layers.dropout(x,rate = dropout_rate, training = training)\n",
        "  \n",
        "logits = feedback_fc(x,n_categories,scope='soft',mode=network_mode,activation_fn=None)\n",
        "\n",
        "targets = tf.placeholder(tf.int64,shape=[None,1],name='targets')\n",
        "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.squeeze(targets),logits = logits),axis=-1)\n",
        "error = tf.reduce_mean(tf.cast(tf.logical_not(tf.equal(tf.argmax(logits,axis=-1),tf.squeeze(targets))),'float'))\n",
        "train_op = tf.train.AdamOptimizer(learning_rate = 2e-5).minimize(loss)\n",
        "\n",
        "tf.summary.scalar('loss',loss)\n",
        "tf.summary.scalar('error',error)\n",
        "\n",
        "merged = tf.summary.merge_all()\n",
        "tf.global_variables_initializer().run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8rlSih4cgU5e"
      },
      "cell_type": "markdown",
      "source": [
        "Run"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "e61613b7-5d25-43e5-f0d8-2db9ad3a4f20",
        "id": "lXfcuHOrgU5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1391
        }
      },
      "cell_type": "code",
      "source": [
        "mini_batch_size = 64\n",
        "\n",
        "b = Batcher(x_train,y_train,mini_batch_size)\n",
        "max_epochs = 300\n",
        "test_epoch_int = 10\n",
        "\n",
        "print(\"Batches per epoch %d\" %b.nbatches)\n",
        "while b.epoch <= max_epochs:\n",
        "    batch_x,batch_y, new_epoch = b.get_mini_batch()\n",
        "    sess.run([train_op],feed_dict = {inputs : batch_x, targets:batch_y, training : True})\n",
        "    if new_epoch:\n",
        "        batch_err,summary = sess.run([error,merged],feed_dict = {inputs : batch_x, targets:batch_y, training : False})\n",
        "#         train_writer.add_summary(summary)\n",
        "        if (b.epoch-1) % test_epoch_int == 0:\n",
        "          print(\"Train error is {}% at epoch {}\".format(batch_err*100,b.epoch-1))\n",
        "          test_err,summary = sess.run([error,merged],feed_dict = {inputs : x_test, targets : y_test, training : False})\n",
        "          print(\"Test error is {}% at epoch {}\".format(test_err*100,b.epoch-1))"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batches per epoch 781\n",
            "Train error is 50.0% at epoch 10\n",
            "Test error is 53.039997816085815% at epoch 10\n",
            "Train error is 37.5% at epoch 20\n",
            "Test error is 50.24999976158142% at epoch 20\n",
            "Train error is 31.25% at epoch 30\n",
            "Test error is 47.65999913215637% at epoch 30\n",
            "Train error is 50.0% at epoch 40\n",
            "Test error is 46.93000018596649% at epoch 40\n",
            "Train error is 25.0% at epoch 50\n",
            "Test error is 45.100000500679016% at epoch 50\n",
            "Train error is 25.0% at epoch 60\n",
            "Test error is 44.65000033378601% at epoch 60\n",
            "Train error is 31.25% at epoch 70\n",
            "Test error is 43.84999871253967% at epoch 70\n",
            "Train error is 31.25% at epoch 80\n",
            "Test error is 44.200000166893005% at epoch 80\n",
            "Train error is 25.0% at epoch 90\n",
            "Test error is 43.25999915599823% at epoch 90\n",
            "Train error is 31.25% at epoch 100\n",
            "Test error is 42.71000027656555% at epoch 100\n",
            "Train error is 12.5% at epoch 110\n",
            "Test error is 42.809998989105225% at epoch 110\n",
            "Train error is 12.5% at epoch 120\n",
            "Test error is 42.64000058174133% at epoch 120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-1722cd49189a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbatch_err\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "2zD8GLI7vlI0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Batches per epoch 781\n",
        "Train error is 50.0% at epoch 10\n",
        "Test error is 53.039997816085815% at epoch 10\n",
        "Train error is 37.5% at epoch 20\n",
        "Test error is 50.24999976158142% at epoch 20\n",
        "Train error is 31.25% at epoch 30\n",
        "Test error is 47.65999913215637% at epoch 30\n",
        "Train error is 50.0% at epoch 40\n",
        "Test error is 46.93000018596649% at epoch 40\n",
        "Train error is 25.0% at epoch 50\n",
        "Test error is 45.100000500679016% at epoch 50\n",
        "Train error is 25.0% at epoch 60\n",
        "Test error is 44.65000033378601% at epoch 60\n",
        "Train error is 31.25% at epoch 70\n",
        "Test error is 43.84999871253967% at epoch 70\n",
        "Train error is 31.25% at epoch 80\n",
        "Test error is 44.200000166893005% at epoch 80\n",
        "Train error is 25.0% at epoch 90\n",
        "Test error is 43.25999915599823% at epoch 90\n",
        "Train error is 31.25% at epoch 100\n",
        "Test error is 42.71000027656555% at epoch 100\n",
        "Train error is 12.5% at epoch 110\n",
        "Test error is 42.809998989105225% at epoch 110\n",
        "Train error is 12.5% at epoch 120\n",
        "Test error is 42.64000058174133% at epoch 120```"
      ]
    },
    {
      "metadata": {
        "id": "0NTewXY4mnt1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Locally connected.."
      ]
    },
    {
      "metadata": {
        "id": "tTlgsCXz24Ph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1812b67-20a2-4508-c525-b16bb35c7b0b"
      },
      "cell_type": "code",
      "source": [
        "tf.keras.layers.LocallyConnected2D"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.keras.layers.local.LocallyConnected2D"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "c0UEs1NRmmvY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LocallyConnected2D(tf.keras.layers.LocallyConnected2D):\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    tf.keras.layers.LocallyConnected2D.build(self,input_shape)\n",
        "    \n",
        "    self.kernel_fb = self.add_weight(shape=self.kernel_shape,\n",
        "                                        initializer=self.kernel_initializer,\n",
        "                                        name='kernel',\n",
        "                                        regularizer=self.kernel_regularizer,\n",
        "                                        constraint=self.kernel_constraint)\n",
        "  def call(self, inputs):\n",
        "    if self.implementation == 2:\n",
        "      output = local_conv_matmul_fb(inputs, self.kernel, self.kernel_fb,\n",
        "                                 self.kernel_mask,\n",
        "                                 self.compute_output_shape(inputs.shape))\n",
        "\n",
        "    else:\n",
        "      raise ValueError('Unsupported implementation mode: %d.'\n",
        "                       % self.implementation)\n",
        "\n",
        "    if self.use_bias:\n",
        "      output = K.bias_add(output, self.bias, data_format=self.data_format)\n",
        "\n",
        "    output = self.activation(output)\n",
        "    return output\n",
        "  \n",
        "def local_conv_matmul_fb(inputs, kernel, kernel_fb, kernel_mask, output_shape):\n",
        "  \n",
        "  inputs_flat = K.reshape(inputs, (K.shape(inputs)[0], -1))\n",
        "\n",
        "  kernel = kernel_mask * kernel\n",
        "  kernel = make_2d(kernel, split_dim=K.ndim(kernel) // 2)\n",
        "\n",
        "  output_flat = feedback_matmul(inputs_flat,kernel,kernel_fb,update_fb_weights = False, is_sparse = True)\n",
        "  \n",
        "  output = K.reshape(output_flat,\n",
        "                     [K.shape(output_flat)[0],] + output_shape.as_list()[1:])\n",
        "  return output\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}